{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was created by Franck Iutzeler, Jerome Malick and Yann Vernaz (2016).</i></small>\n",
    "<!-- Credit (images) Jeffrey Keating Thompson. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)</h3></center>\n",
    "<hr>\n",
    "<center><h1>Convex and distributed optimization</h1></center>\n",
    "<center><h2>Part II - Classification (3h + 3h home work)</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "In this Lab, we will investigate some gradient-based and proximal algorithms on the binary classification problems with logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classification and Logistic Regression\n",
    "\n",
    "### Machine Learning as an Optimization problem\n",
    "\n",
    "We have some *data*  $\\mathcal{D}$ consisting of $m$ *examples* $\\{d_i\\}$; each example consisting of a *feature* vector $a_i\\in\\mathbb{R}^d$ and an *observation* $b_i\\in \\mathcal{O}$: $\\mathcal{D} = \\{[a_i,b_i]\\}_{i=1..m}$ .\n",
    "\n",
    "\n",
    "The goal of *supervised learning* is to construct a predictor for the observations when given feature vectors.\n",
    "\n",
    "\n",
    "A popular approach is based on *linear models* which are based on finding a *parameter* $x$ such that the real number $\\langle a_i , x \\rangle$ is used to predict the value of the observation through a *predictor function* $g:\\mathbb{R}\\to \\mathcal{O}$: $g(\\langle a_i , x \\rangle)$ is the predicted value from $a_i$.\n",
    "\n",
    "\n",
    "In order to find such a parameter, we use the available data and a *loss* $\\ell$ that penalizes the error made between the predicted $g(\\langle a_i , x \\rangle)$ and observed $b_i$ values. For each example $i$, the corresponding error function for a parameter $x$ is $f_i(x) =   \\ell( g(\\langle a_i , x \\rangle) ; b_i )$. Using the whole data, the parameter that minimizes the total error is the solution of the minimization problem\n",
    "$$ \\min_{x\\in\\mathbb{R}^d} \\frac{1}{m} \\sum_{i=1}^m f_i(x) = \\frac{1}{m} \\sum_{i=1}^m  \\ell( g(\\langle a_i , x \\rangle) ; b_i ). $$\n",
    "\n",
    "\n",
    "### Binary Classification with Logisitic Regression\n",
    "\n",
    "In our setup, the observations are binary: $\\mathcal{O} = \\{-1 , +1 \\}$, and the *Logistic loss* is used to form the following optimization problem\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } f(x) := \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ).\n",
    "\\end{align*}\n",
    "\n",
    "Under some statistical hypotheses, $x^\\star = \\arg\\min f(x)$ maximizes the likelihood of the labels knowing the features vector. Then, for a new point $d$ with features vector $a$, \n",
    "$$ p_1(a) = \\mathbb{P}[d\\in \\text{ class }  +1] = \\frac{1}{1+\\exp(-\\langle a;x^\\star \\rangle)} $$\n",
    "Thus, from $a$, if $p_1(a)$ is close to $1$, one can decide that $d$ belongs to class $1$; and the opposite decision if $p(a)$ is close to $0$. Between the two, the appreciation is left to the data scientist depending on the application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised classification datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset\n",
    "\n",
    "We will use LibSVM formatted data, meaning that each line of the file (i.e. each example) will have the form\n",
    "\n",
    "<tt>class feature_number1:feature_value1 feature_number2:feature_value2 ... feature_number$n_i$:feature_value$n_i$ </tt>\n",
    "\n",
    "You may read such a file using MLUtils's <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.util.MLUtils.loadLibSVMFile\">`loadLibSVMFile`</a> routine on the supervised classification datasets below.\n",
    "\n",
    "The elements of the produced RDD have the form of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint\">`LabeledPoints`</a> composed of a label `example.label` corresponding to the class (+1 or -1) and a feature vector `example.features` generally encoded as a <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector\">`SparseVector`</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up spark environment (using Spark local mode set to # cores on your machine)\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().set(\"spark.executor.heartbeatInterval\", \"100000s\")\n",
    "# conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\")\n",
    "conf.setAppName(\"MSIAM part II - Logistic Regression\")\n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remind you that you can access this interface (Spark UI) by simply opening http://localhost:4040 in a web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to LibSVM Datasets\n",
    "LibSVMHomeDir=\"../data/LibSVM/\"\n",
    "# LibSVMHomeDir=\"\"\n",
    "LibName=\"ionosphere.txt\"             # a small dataset to begin with\n",
    "#LibName=\"rcv1_train.binary\"          # a bigger one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 1__\n",
    "> Form an RDD from the selected dataset.\n",
    "\n",
    "> Count the number of examples, features, the number of examples of class '+1' and the density of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples (N) = 351\n",
      "number of features (D) = 34\n",
      "number of examples of class +1 = 225\n",
      "number of examples of class -1 = 126\n",
      "density = 0.884113\n",
      "10551\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "data = MLUtils.loadLibSVMFile(sc, LibSVMHomeDir + LibName).setName(\"LibSVM\")\n",
    "N = data.count() # number of examples\n",
    "D = len(data.first().features) # number of features\n",
    "nb_pos_samples = data.filter(lambda x: x.label == 1).count()\n",
    "nb_neg_samples = data.filter(lambda x: x.label == -1).count()\n",
    "nb_nonzero_vals = data.map(lambda x: x.features.numNonzeros()).reduce(lambda x, y: x + y)\n",
    "density = 1. * nb_nonzero_vals / (N * D)\n",
    "\n",
    "print(\"number of examples (N) = %d\" % N)\n",
    "print(\"number of features (D) = %d\" % D)\n",
    "print(\"number of examples of class +1 = %d\" % nb_pos_samples)\n",
    "print(\"number of examples of class -1 = %d\" % nb_neg_samples)\n",
    "print(\"density = %f\" % density)\n",
    "print(nb_nonzero_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "An important first step for learning by regression is to preprocess the dataset. This processing usually consists in:\n",
    "* Adding an intercept, that is an additional feature equal to one for all examples (statistically, this accounts for the fact that the two classes may be imbalanced).\n",
    "* For the dense datasets:\n",
    "    *  normalize to have zero-mean and unit variance for every feature (except the interecept for instance.\n",
    "* For sparse datasets:\n",
    "    * normalize so that the feature vector has unit $\\ell_2$ norm for each example.\n",
    "\n",
    "This does not really change the problem but it will ease the convergence of the applied optimization algorithms.\n",
    "\n",
    "__Question 2__\n",
    "> Form a new RDD with the scaled version of the dataset.\n",
    "\n",
    "> Check that the number of examples, features, and the density is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from operator import add\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# as variance can be zero for some features, we will remove those column and insert them back\n",
    "def normalize_sample(x):\n",
    "    new_features = (x.features.toArray() - means) / np.sqrt(variance)\n",
    "    new_features = np.append(new_features, 1)\n",
    "    features_sparse_vector = SparseVector(np.shape(means)[0] + 1,\n",
    "                                          np.nonzero(new_features)[0],\n",
    "                                          new_features[np.nonzero(new_features)])\n",
    "    return LabeledPoint(x.label, features_sparse_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0,(34,[0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33],[1.0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1.0,0.0376,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.3409,0.42267,-0.54487,0.18641,-0.453]))\n",
      "(34,[0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33],[1.0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1.0,0.0376,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.3409,0.42267,-0.54487,0.18641,-0.453])\n",
      "[ 1.       0.       0.99539 -0.05889  0.85243  0.02306  0.83398 -0.37708\n",
      "  1.       0.0376   0.85243 -0.17755  0.59755 -0.44945  0.60536 -0.38223\n",
      "  0.84356 -0.38542  0.58212 -0.32192  0.56971 -0.29674  0.36946 -0.47357\n",
      "  0.56811 -0.51171  0.41078 -0.46168  0.21266 -0.3409   0.42267 -0.54487\n",
      "  0.18641 -0.453  ]\n"
     ]
    }
   ],
   "source": [
    "first = data.first()\n",
    "print(first)\n",
    "\n",
    "features = first.features\n",
    "print(features)\n",
    "\n",
    "np_features = features.toArray()\n",
    "print(np_features)\n",
    "# data_mapped = data.map(lambda x: x.features)\n",
    "# first_mapped = data_mapped.first()\n",
    "# print(first_mapped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means:\n",
      "[ 0.78347578  0.          0.64134185  0.04437188  0.60106789  0.115889\n",
      "  0.55009507  0.11936037  0.51184809  0.18134538  0.47618265  0.15504046\n",
      "  0.4008012   0.09341368  0.34415915  0.07113234  0.381949   -0.00361681\n",
      "  0.3593896  -0.0240247   0.33669547  0.0082959   0.3624755  -0.05740575\n",
      "  0.39613467 -0.07118687  0.5416408  -0.06953761  0.37844519 -0.02790709\n",
      "  0.35251373 -0.00379376  0.34936365  0.01448011]\n",
      "variance:\n",
      "[ 0.3861657   0.          0.24700772  0.19430949  0.26948603  0.211741\n",
      "  0.24201626  0.27040786  0.25638293  0.2334447   0.31662351  0.24414675\n",
      "  0.38601268  0.24420121  0.42496998  0.20950509  0.38086098  0.24606941\n",
      "  0.39109271  0.26867235  0.37083107  0.26773094  0.36349662  0.27741755\n",
      "  0.33365214  0.25783     0.26570809  0.30166587  0.33069932  0.25730253\n",
      "  0.32566278  0.26300717  0.27239872  0.21871485]\n",
      "(1.0,(35,[0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34],[0.348433280269,0.712372367309,-0.234257237563,0.484207686793,-0.201734977754,0.577058790699,-0.954679144632,0.964074158868,-0.297510229259,0.668654651086,-0.673107318134,0.316673202835,-1.0985425274,0.400677973145,-0.990485565808,0.747985026589,-0.769680949556,0.356155483134,-0.574714507232,0.38264403556,-0.589524243017,0.0115847114771,-0.790128635368,0.29772766393,-0.867564946095,-0.253867539692,-0.713971226166,-0.288289660581,-0.617038783086,0.122936944467,-1.05505394246,-0.312220599512,-0.999594828772,1.0]))\n",
      "means in the normalized data:\n",
      "[  1.56886217e-16   0.00000000e+00  -7.08518397e-17  -6.07301484e-17\n",
      "   5.06084570e-18  -6.07301484e-17   2.83407359e-16   1.41703679e-16\n",
      "  -3.34015816e-16  -4.04867656e-17   1.06277760e-16  -1.21460297e-16\n",
      "   1.01216914e-17   5.56693027e-17  -1.67007908e-16   1.31581988e-16\n",
      "  -2.83407359e-16  -3.03650742e-17  -2.63163976e-16  -5.06084570e-17\n",
      "  -1.77129599e-16   2.53042285e-17  -2.43236896e-16  -5.06084570e-18\n",
      "   3.98541599e-17  -1.32847200e-17   0.00000000e+00   1.01216914e-17\n",
      "   2.53042285e-17   3.92215541e-17   2.02433828e-16  -3.54259199e-17\n",
      "   3.54259199e-17  -7.59126854e-17   1.00000000e+00]\n",
      "variance in the normalized data:\n",
      "[ 1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.]\n",
      "number of examples (N) in the normalized data = 351\n",
      "new number of features (D) in the normalized data = 35\n",
      "density in the normalized data = 0.971429\n"
     ]
    }
   ],
   "source": [
    "means = data.map(lambda x: x.features.toArray()).reduce(add) / N\n",
    "print('means:')\n",
    "print(means)\n",
    "\n",
    "variance = data.map(lambda x: (x.features.toArray() - means) ** 2).reduce(add) / N\n",
    "print('variance:')\n",
    "print(variance)\n",
    "\n",
    "# as we can not divide by zero, we fix the values of variance where it is 0 (the second column which is empty)\n",
    "variance[np.argwhere(variance == 0)] = 1\n",
    "\n",
    "data_normalized = data.map(normalize_sample)\n",
    "\n",
    "print(data_normalized.first())\n",
    "      \n",
    "new_means = data_normalized.map(lambda x: x.features.toArray()).reduce(add) / N\n",
    "print('means in the normalized data:')\n",
    "print(new_means)\n",
    "\n",
    "new_variance = data_normalized.map(lambda x: (x.features.toArray() - new_means) ** 2).reduce(add) / N\n",
    "print('variance in the normalized data:')\n",
    "print(new_variance)\n",
    "\n",
    "\n",
    "new_N = data_normalized.count() # number of examples\n",
    "new_D = len(data_normalized.first().features.toArray()) # number of features\n",
    "new_nb_nonzero_vals = data_normalized.map(lambda x: x.features.numNonzeros()).reduce(add)\n",
    "new_density = 1. * new_nb_nonzero_vals / (new_N * new_D)\n",
    "\n",
    "print(\"number of examples (N) in the normalized data = %d\" % new_N)\n",
    "print(\"new number of features (D) in the normalized data = %d\" % new_D)\n",
    "print(\"density in the normalized data = %f\" % new_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Initialization\n",
    "\n",
    "We will set up here the variables, and the training versus testing dataset. Indeed, we will take a portion of the dataset to learn called the `learning set`, say $95$%, and we will test our predictions on the rest, the `testing set`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 3__\n",
    "\n",
    ">  Split the scaled dataset into a training and a testing set. For instance, you may use the function <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit\">`randomSplit`</a>.\n",
    "\n",
    "> Count the number of examples, and subjects in class '+1' in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_learn = 330, N_test = 21\n",
      "number of examples of class +1 in learn = %d 208\n",
      "number of examples of class +1 in test = %d 17\n"
     ]
    }
   ],
   "source": [
    "learn, test = data_normalized.randomSplit([0.95, 0.05])\n",
    "\n",
    "N_learn = learn.count()\n",
    "N_test = test.count()\n",
    "\n",
    "nb_pos_samples_learn = learn.filter(lambda x: x.label == 1).count()\n",
    "nb_pos_samples_test = test.filter(lambda x: x.label == 1).count()\n",
    "\n",
    "print('N_learn = %d, N_test = %d' %(N_learn, N_test))\n",
    "print('number of examples of class +1 in learn = %d', nb_pos_samples_learn)\n",
    "print('number of examples of class +1 in test = %d', nb_pos_samples_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Minimization of the logistic loss with the Gradient algorithm\n",
    "\n",
    "The goal of this section is to: \n",
    "1. Compute gradients of the loss functions.\n",
    "2. Implement a Gradient algorithm.\n",
    "3. Observe the prediction accuracy of the developed methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 4__\n",
    ">Define a routine computing functional loss and gradient from one example \n",
    "\n",
    "For a Labeled point <tt>example</tt> (`LabeledPoint(example.label,example.features)`) that we denoted $(b_i,a_i)$ and a regressor <tt>x</tt>, compute $f_i(x) = \\log(1+\\exp(-b_i \\langle a_i,x\\rangle) )$ and $\\nabla f_i(x)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_loss_per_example(example,x):\n",
    "    \"\"\" Computes the logistic loss for a Labeled point\n",
    "    Args:\n",
    "        example: a labeled point\n",
    "        x: regressor\n",
    "    Returns:\n",
    "        real value: l \n",
    "    \"\"\"\n",
    "    res = np.log(1 + np.exp(- example.label * np.dot(example.features.toArray(), x)))\n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_grad_per_example(example,x):\n",
    "    \"\"\" Computes the logistic gradient for a Labeled point\n",
    "    Args:\n",
    "        example: a labeled point\n",
    "        x: regressor\n",
    "    Returns:\n",
    "        numpy array: g \n",
    "    \"\"\"\n",
    "    denom = (1 + np.exp(example.label * np.dot(example.features.toArray(), x)))\n",
    "    res = - example.label * example.features.toArray() / denom\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 5__\n",
    ">Implement a gradient descent algorithm to minimize\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } f(x) := \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) = \\frac{1}{m}  \\sum_{i=1}^m f_i(x).\n",
    "\\end{align*}\n",
    ">by \n",
    "* defining a function taking a stepsize and a maximal number of iterations and returning the final point as well as the value of $f(x)$ at each iteration. \n",
    "* running `x, f_tab = grad_algo(gamma,MAX_ITE)`\n",
    "\n",
    "\n",
    "For the choice of the stepsize, we help you by provinding you an upper bound on the Lipschitz constant $L$ of $\\nabla f$:\n",
    "\n",
    "$ L \\leq L_b = \\max_i 0.25 \\|a_i\\|_2^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad_algo(trainRDD, gamma, max_iter, verbose = True):\n",
    "    if verbose:\n",
    "        print('start grad_algo with gamma = %f, max_iter = %d' % (gamma, max_iter))\n",
    "    f_tab = [1.]\n",
    "    N = trainRDD.count()\n",
    "    x = np.zeros(len(trainRDD.first().features.toArray())) # init values = 0\n",
    "    for i in range(max_iter): # maybe change to convergence criterion\n",
    "        sg = trainRDD.map(lambda ex: logistic_grad_per_example(ex, x)).reduce(add) / N\n",
    "        x -= gamma * sg\n",
    "        ll = trainRDD.map(lambda ex: logistic_loss_per_example(ex, x)).reduce(add) / N\n",
    "        f_tab.append(ll)\n",
    "        if verbose and (i == 0 or i == (max_iter - 1)):\n",
    "            print('[iter %d] f(x) = %f' %(i, ll))\n",
    "    if verbose:\n",
    "        print('done')\n",
    "    return x, f_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start grad_algo with gamma = 0.691580, max_iter = 100\n",
      "[iter 0] f(x) = 0.506834\n",
      "[iter 99] f(x) = 0.213508\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "max_example_norm = learn.map(lambda x: np.sqrt(x.features.dot(x.features))).reduce(lambda x,y: x if x > y else y)\n",
    "L_b = 0.25 * max_example_norm # we take the upperbound\n",
    "gamma = 2. / L_b # works better with e.g. 8. / L_b\n",
    "max_iter = 100 # first guess\n",
    "\n",
    "(x_opt, f_tab) = grad_algo(learn, gamma, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 6__\n",
    "\n",
    "> Plot the functional value versus the iterations.\n",
    "\n",
    "> Investigate if the computations are distributed over different threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAF5CAYAAADQ2iM1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a2dc7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(max_iter+1), f_tab, color=\"black\", linewidth=1.0, linestyle=\"-\")\n",
    "plt.xlim(0, max_iter+1)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Functional value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized logisitic regression\n",
    "\n",
    "In addition to the loss, it is usual to add a regularization term of the form\n",
    "$$ r(x) = \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2 $$\n",
    "\n",
    "The first part promotes sparsity of the iterates while the second part prevents over-fitting. \n",
    "This kind of regularization is often called:\n",
    "- *elastic-net* when $ \\lambda_1$ and $ \\lambda_2$ are non-null\n",
    "- $\\ell_1$ when $\\lambda_2 = 0$\n",
    "- *Tikhonov* when $\\lambda_1 = 0$\n",
    "\n",
    "The full optimization problems now writes\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } g(x) =  \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) +  \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "__Question 7__\n",
    "\n",
    "> Which part of $g$ is smooth, which part is not? Write $g$ as \n",
    "$$ g(x) =  \\frac{1}{m}  \\sum_{i=1}^m s_i(x) + n(x)  $$\n",
    "where the $(s_i)$ are smooth function and $n$ is non smooth. \n",
    "\n",
    "> Define a function `regularized_logistic_grad_per_example(examples,x)` returning the gradient of the smooth part per example (i.e. $\\nabla s_i(x)$)\n",
    "\n",
    "> Define a function `n_prox(x,gamma)` returning the proximal operator of the non-smooth part (i.e. $\\mathbf{prox}_{\\gamma n}(y)$)\n",
    "\n",
    "we recall that\n",
    "$$ \\mathbf{prox}_{\\gamma n}(y) = \\arg\\min_x\\left\\{ n(x) + \\frac{1}{2\\gamma} \\|x-y\\|_2^2 \\right\\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def regularized_logistic_loss_per_example(example, x, lambda_1, lambda_2):\n",
    "    res = np.log(1 + np.exp(- example.label * np.dot(example.features.toArray(), x))) + lambda_2*np.dot(x,x) +lambda_1*sum(abs(x))\n",
    "    return res\n",
    "\n",
    "\n",
    "def regularized_logistic_grad_per_example(example, x, lambda_2):\n",
    "    denom = (1 + np.exp(example.label * np.dot(example.features.toArray(), x)))\n",
    "    res = - example.label * example.features.toArray() / denom  + 2 * lambda_2 * x\n",
    "    return res\n",
    "\n",
    "def n_prox(y,gamma,lambda_1):\n",
    "    x = y.copy()\n",
    "    for i in range(np.size(y)):\n",
    "        if y[i] > gamma*lambda_1: \n",
    "            x[i] = y[i] - gamma*lambda_1\n",
    "        elif y[i] < -gamma*lambda_1:\n",
    "            x[i] = y[i] + gamma*lambda_1\n",
    "        else:\n",
    "            x[i] = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 8__\n",
    "\n",
    "> Compute a proximal gradient algorithm for computing a solution of\n",
    "$$ \\min_x  f(x) + r(x) = \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) + \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2 $$\n",
    "\n",
    "\n",
    "Hint: An admissible stepsize can be found by taking $\\gamma = 1/L_{b2}$ with  $ L_b = \\max_i 0.25 \\|a_i\\|_2^2 + 2\\lambda_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def proximal_gradient_algorithm(trainRDD, gamma, max_iter,lambda_1,lambda_2, verbose = True):\n",
    "    if verbose:\n",
    "        print('start prox_grad_algo with gamma = %f, max_iter = %d' % (gamma, max_iter))\n",
    "    f_tab = [1.]\n",
    "    N = trainRDD.count()\n",
    "    x = np.zeros(len(trainRDD.first().features.toArray())) # init values = 0\n",
    "    for i in range(max_iter): # maybe change to convergence criterion\n",
    "        sg = trainRDD.map(lambda ex: regularized_logistic_grad_per_example(ex, x,lambda_2)).reduce(add) / N\n",
    "        x -= gamma * sg\n",
    "        x = n_prox(x,gamma,lambda_1)\n",
    "        ll = trainRDD.map(lambda ex: regularized_logistic_loss_per_example(ex, x,lambda_1,lambda_2)).reduce(add) / N\n",
    "        f_tab.append(ll)\n",
    "        if verbose and (i == 0 or i == (max_iter - 1)):\n",
    "            print('[iter %d] f(x) = %f' %(i, ll))\n",
    "    if verbose:\n",
    "        print('done')\n",
    "    return x, f_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start prox_grad_algo with gamma = 0.343415, max_iter = 100\n",
      "[iter 0] f(x) = 0.590289\n",
      "[iter 99] f(x) = 0.376759\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "lambda_1 = 0.01\n",
    "lambda_2 = 0.01\n",
    "max_example_norm = learn.map(lambda x: np.sqrt(x.features.dot(x.features.toArray()))).reduce(lambda x,y: x if x > y else y)\n",
    "L_b = 0.25 * max_example_norm + 2*lambda_2 # we take the upperbound\n",
    "gamma = 1./ L_b # works better with e.g. 8. / L_b\n",
    "max_iter = 100 # first guess\n",
    "\n",
    "(x_opt, f_tab) =  proximal_gradient_algorithm(learn, gamma, max_iter,lambda_1,lambda_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 9__\n",
    "\n",
    "> Examine the behavior and output of your proximal gradient algorithm with different values of $\\lambda_1$, $\\lambda_2$. What do you observe in terms of sparsity of the solution and convergence rate of the algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_reg_vs_nonreg(learn, test, max_iter, lambda_1, lambda_2, verbose = False):\n",
    "    N_test = test.count()\n",
    "    L_b_nonreg = 0.25 * max_example_norm # we take the upperbound\n",
    "    L_b_reg = 0.25 * max_example_norm + 2*lambda_2 # we take the upperbound\n",
    "    gamma_nonreg = 2. / L_b_nonreg\n",
    "    gamma_reg = 2. / L_b_reg\n",
    "    (x_opt_nonreg, f_tab) = grad_algo(learn, gamma_nonreg, max_iter, verbose)\n",
    "    (x_opt_reg, f_tab) = proximal_gradient_algorithm(learn, gamma_reg, max_iter, lambda_1, lambda_2, verbose)\n",
    "    loss_nonreg = test.map(lambda ex: logistic_loss_per_example(ex, x_opt_nonreg)).reduce(add) / N_test\n",
    "    loss_reg = test.map(lambda ex: logistic_loss_per_example(ex, x_opt_reg)).reduce(add) / N_test\n",
    "#     res = p.map(lambda ex: np.abs(np.sign(ex - 0.5) - test.labels()).reduce(add)\n",
    "    if verbose:\n",
    "        print('loss_nonreg = %f, loss_reg = %f' % (loss_nonreg, loss_reg))\n",
    "    return (loss_nonreg, loss_reg)\n",
    "\n",
    "def accuracy_reg(learn, test, max_iter, lambda_1, lambda_2, verbose = False):\n",
    "    N_test = test.count()\n",
    "    L_b_reg = 0.25 * max_example_norm + 2*lambda_2 # we take the upperbound\n",
    "    gamma_reg = 2. / L_b_reg\n",
    "    (x_opt_reg, f_tab) = proximal_gradient_algorithm(learn, gamma_reg, max_iter, lambda_1, lambda_2, verbose)\n",
    "    loss_reg = test.map(lambda ex: logistic_loss_per_example(ex, x_opt_reg)).reduce(add) / N_test\n",
    "    if verbose:\n",
    "        print('loss on test = %f' % (loss_reg))\n",
    "    return loss_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.098295 (lambda1 = 0.000000, lambda2 = 0.000000)\n",
      "loss = 0.135086 (lambda1 = 0.000000, lambda2 = 0.010000)\n",
      "loss = 0.166345 (lambda1 = 0.000000, lambda2 = 0.020000)\n",
      "loss = 0.191560 (lambda1 = 0.000000, lambda2 = 0.030000)\n",
      "loss = 0.212723 (lambda1 = 0.000000, lambda2 = 0.040000)\n",
      "loss = 0.231124 (lambda1 = 0.000000, lambda2 = 0.050000)\n",
      "loss = 0.247504 (lambda1 = 0.000000, lambda2 = 0.060000)\n",
      "loss = 0.262320 (lambda1 = 0.000000, lambda2 = 0.070000)\n",
      "loss = 0.275870 (lambda1 = 0.000000, lambda2 = 0.080000)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2439.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2439.0 (TID 4862, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 179772 ms\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1873)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1886)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1899)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1913)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor37.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-bf9e25dd5544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mL_b\u001b[0m \u001b[0;31m# works better with e.g. 8. / L_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;31m# first guess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmbd_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmbd_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mlambda_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlmbd_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmbd_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-1fc07cb3659d>\u001b[0m in \u001b[0;36maccuracy_reg\u001b[0;34m(learn, test, max_iter, lambda_1, lambda_2, verbose)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mL_b_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_example_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlambda_2\u001b[0m \u001b[0;31m# we take the upperbound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mgamma_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mL_b_reg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mx_opt_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_tab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproximal_gradient_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mloss_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlogistic_loss_per_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_opt_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-1c3354ade256>\u001b[0m in \u001b[0;36mproximal_gradient_algorithm\u001b[0;34m(trainRDD, gamma, max_iter, lambda_1, lambda_2, verbose)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# init values = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# maybe change to convergence criterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mregularized_logistic_grad_per_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_prox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alexpashevich/MoSIG/CDO/spark-2.0.2-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alexpashevich/MoSIG/CDO/spark-2.0.2-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \"\"\"\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alexpashevich/MoSIG/CDO/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alexpashevich/MoSIG/CDO/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2439.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2439.0 (TID 4862, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 179772 ms\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1873)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1886)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1899)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1913)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor37.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "max_example_norm = learn.map(lambda x: np.sqrt(x.features.dot(x.features))).reduce(lambda x,y: x if x > y else y)\n",
    "loss_array = []\n",
    "lambda_array = []\n",
    "for lmbd_1 in np.arange(0, 0.1, 0.01):\n",
    "    for lmbd_2 in np.arange(0, 0.1, 0.01):\n",
    "        L_b = 0.25 * max_example_norm + 2*lmbd_2 # we take the upperbound\n",
    "        gamma = 2. / L_b # works better with e.g. 8. / L_b\n",
    "        max_iter = 100 # first guess\n",
    "        ll = accuracy_reg(learn, test, max_iter, lmbd_1, lmbd_2)\n",
    "        lambda_array.append([lmbd_1, lmbd_2])\n",
    "        loss_array.append(ll)\n",
    "        print('loss = %f (lambda1 = %f, lambda2 = %f)' % (ll, lmbd_1, lmbd_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(lambda_array)\n",
    "# print(len(lambda_array))\n",
    "# zip_la = zip(*lambda_array)\n",
    "# print(type(zip_la))\n",
    "\n",
    "plot_loss_vs_lambdas(lambda_array, loss_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Question 10__\n",
    "\n",
    "> Write a function that evaluates the accuracy of the classification on the training dataset.\n",
    "\n",
    "> Investigate how this accuracy change when playing with the regularization terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "def plot_loss_vs_lambdas(lambda_array, loss_array):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    lambda_array_np = np.array(lambda_array)\n",
    "    ax.scatter(lambda_array_np[:,0], lambda_array_np[:,1], loss_array) #, rstride=10, cstride=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To go further\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerations\n",
    "\n",
    "A popular acceleration method to improve the convergence rate of proximal gradient algorithm is the addition of inertia. That is, contructing the next gradient input by a combination of the last two outputs.\n",
    "\n",
    "\n",
    "In particular, Nesterov's acceleration is the most popular form of inertia. It writes\n",
    "$$ \\left\\{ \\begin{array}{l}   y_{k+1} = \\mathbf{prox\\_grad}(x_k) \\\\ x_{k+1} = y_{k+1} + \\alpha_{k+1} (y_{k+1} - y_k)  \\end{array} \\right. $$ \n",
    "with\n",
    "* $\\mathbf{prox\\_grad}$ the proximal gradient operation\n",
    "* $(\\alpha_{k})$ the inertial sequence defined as $\\alpha_k = \\frac{t_k-1}{t_{k+1}}$ and $t_0 = 0$ and $t_{k+1} = \\frac{1+\\sqrt{1+4t_k^2}}{2}$\n",
    "\n",
    "__Question 11__\n",
    "\n",
    "> Implement a fast proximal gradient with this kind of inertia (This algorithm is often nicknamed FISTA).\n",
    "\n",
    "> Compare the convergence speed with the vanilla proximal gradient algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fast_proximal_gradient_algorithm(trainRDD, gamma, max_iter,lambda_1,lambda_2):\n",
    "    print('start grad_algo with gamma = %f, max_iter = %d' % (gamma, max_iter))\n",
    "    f_tab = [1.]\n",
    "    N = trainRDD.count()\n",
    "    x = np.zeros(len(trainRDD.first().features.toArray())) # init values = 0\n",
    "    y = x.copy()\n",
    "    y_last = x.copy()\n",
    "    t = 0\n",
    "    t_last = 0\n",
    "    for i in range(max_iter): # maybe change to convergence criterion\n",
    "        t = (1. + np.sqrt(1 + 4*t_last**2))/2\n",
    "        alpha = (t_last - 1)/t\n",
    "        sg = trainRDD.map(lambda ex: regularized_logistic_grad_per_example(ex, x,lambda_2)).reduce(add) / N\n",
    "        x -= gamma * sg\n",
    "        y = n_prox(x,gamma,lambda_1)\n",
    "        x = y + alpha*(y - y_last)\n",
    "        ll = trainRDD.map(lambda ex: regularized_logistic_loss_per_example(ex, x,lambda_1,lambda_2)).reduce(add) / N\n",
    "        f_tab.append(ll)\n",
    "        y_last = y\n",
    "        t_last = t\n",
    "        print('[iter %d] f(x) = %f' %(i, ll))\n",
    "    print('done')\n",
    "    return x, f_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start grad_algo with gamma = 0.343415, max_iter = 20\n",
      "[iter 0] f(x) = 0.693147\n",
      "[iter 1] f(x) = 0.589594\n",
      "[iter 2] f(x) = 0.524838\n",
      "[iter 3] f(x) = 0.479633\n",
      "[iter 4] f(x) = 0.448187\n",
      "[iter 5] f(x) = 0.426056\n",
      "[iter 6] f(x) = 0.410231\n",
      "[iter 7] f(x) = 0.398894\n",
      "[iter 8] f(x) = 0.390705\n",
      "[iter 9] f(x) = 0.384736\n",
      "[iter 10] f(x) = 0.380300\n",
      "[iter 11] f(x) = 0.377025\n",
      "[iter 12] f(x) = 0.374724\n",
      "[iter 13] f(x) = 0.373131\n",
      "[iter 14] f(x) = 0.372114\n",
      "[iter 15] f(x) = 0.371224\n",
      "[iter 16] f(x) = 0.370463\n",
      "[iter 17] f(x) = 0.369867\n",
      "[iter 18] f(x) = 0.369638\n",
      "[iter 19] f(x) = 0.369317\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "lambda_1 = 0.01\n",
    "lambda_2 = 0.01\n",
    "max_example_norm = learn.map(lambda x: np.sqrt(x.features.dot(x.features.toArray()))).reduce(lambda x,y: x if x > y else y)\n",
    "L_b = 0.25 * max_example_norm + 2*lambda_2 # we take the upperbound\n",
    "gamma = 1./ L_b # works better with e.g. 8. / L_b\n",
    "max_iter = 20 # first guess\n",
    "\n",
    "(x_opt, f_tab) =  fast_proximal_gradient_algorithm(learn, gamma, max_iter,lambda_1,lambda_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental methods\n",
    "\n",
    "When dimension increases, incremental algorithms are often priviledged. \n",
    "\n",
    "A possible incremental algorithm for a problem such as regularized logistic regression is MISO (see *J Mairal. Incremental Majorization-Minimization Optimization with Application to Large-Scale Machine Learning. SIAM Journal on Optimization,2015 and ICML 2014.*):\n",
    "\n",
    "* Draw randomly a sample $n$\n",
    "* Compute $x^n_{k+1} = \\mathbf{prox}_{\\gamma g} (\\bar{x}_k) - \\gamma \\nabla f_n(\\mathbf{prox}_{\\gamma g} (\\bar{x}_k) )$\n",
    "* For all $i\\neq n$, $x^i_{k+1}=x^i_k$ \n",
    "* Compute new $\\bar{x}_{k+1} = \\frac{1}{m} \\sum_{j=1}^m x^j_{k+1}$\n",
    " \n",
    "\n",
    "__Question 12__\n",
    "\n",
    "> Implement this incremental algorithm and compare with the previous algorithms in terms of convergence time and functional value versus number of passes over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  \n",
    "def MISO_proximal_gradient_algorithm(trainRDD, gamma, max_iter,lambda_1,lambda_2):\n",
    "    print('start grad_algo with gamma = %f, max_iter = %d' % (gamma, max_iter))\n",
    "    f_tab = [1.]\n",
    "    N = trainRDD.count()\n",
    "    x = np.zeros(len(trainRDD.first().features.toArray())) # init values = 0\n",
    "    y = x.copy()\n",
    "    y_last = x.copy()\n",
    "    t = 0\n",
    "    t_last = 0\n",
    "    for i in range(max_iter): # maybe change to convergence criterion\n",
    "        n = np.rand(np.size(x))\n",
    "        x = n_prox(x,gamma,lambda_1)\n",
    "        sg_n = trainRDD.map(lambda ex: regularized_logistic_grad_per_example(ex, x,lambda_2)).reduce(add) / N\n",
    "        y = n_prox(x,gamma,lambda_1) - gamma*sg_n\n",
    "        \n",
    "        ll = trainRDD.map(lambda ex: regularized_logistic_loss_per_example(ex, x,lambda_1,lambda_2)).reduce(add) / N\n",
    "        f_tab.append(ll)\n",
    "        y_last = y\n",
    "        t_last = t\n",
    "        print('[iter %d] f(x) = %f' %(i, ll))\n",
    "    print('done')\n",
    "    return x, f_tab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
