{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was created by Franck Iutzeler, Jerome Malick and Yann Vernaz (2016).</i></small>\n",
    "<!-- Credit (images) Jeffrey Keating Thompson. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)</h3></center>\n",
    "<hr>\n",
    "<center><h1>Convex and distributed optimization</h1></center>\n",
    "<center><h2>Part II - Classification (3h + 3h home work)</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "In this Lab, we will investigate some gradient-based and proximal algorithms on the binary classification problems with logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classification and Logistic Regression\n",
    "\n",
    "### Machine Learning as an Optimization problem\n",
    "\n",
    "We have some *data*  $\\mathcal{D}$ consisting of $m$ *examples* $\\{d_i\\}$; each example consisting of a *feature* vector $a_i\\in\\mathbb{R}^d$ and an *observation* $b_i\\in \\mathcal{O}$: $\\mathcal{D} = \\{[a_i,b_i]\\}_{i=1..m}$ .\n",
    "\n",
    "\n",
    "The goal of *supervised learning* is to construct a predictor for the observations when given feature vectors.\n",
    "\n",
    "\n",
    "A popular approach is based on *linear models* which are based on finding a *parameter* $x$ such that the real number $\\langle a_i , x \\rangle$ is used to predict the value of the observation through a *predictor function* $g:\\mathbb{R}\\to \\mathcal{O}$: $g(\\langle a_i , x \\rangle)$ is the predicted value from $a_i$.\n",
    "\n",
    "\n",
    "In order to find such a parameter, we use the available data and a *loss* $\\ell$ that penalizes the error made between the predicted $g(\\langle a_i , x \\rangle)$ and observed $b_i$ values. For each example $i$, the corresponding error function for a parameter $x$ is $f_i(x) =   \\ell( g(\\langle a_i , x \\rangle) ; b_i )$. Using the whole data, the parameter that minimizes the total error is the solution of the minimization problem\n",
    "$$ \\min_{x\\in\\mathbb{R}^d} \\frac{1}{m} \\sum_{i=1}^m f_i(x) = \\frac{1}{m} \\sum_{i=1}^m  \\ell( g(\\langle a_i , x \\rangle) ; b_i ). $$\n",
    "\n",
    "\n",
    "### Binary Classification with Logisitic Regression\n",
    "\n",
    "In our setup, the observations are binary: $\\mathcal{O} = \\{-1 , +1 \\}$, and the *Logistic loss* is used to form the following optimization problem\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } f(x) := \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ).\n",
    "\\end{align*}\n",
    "\n",
    "Under some statistical hypotheses, $x^\\star = \\arg\\min f(x)$ maximizes the likelihood of the labels knowing the features vector. Then, for a new point $d$ with features vector $a$, \n",
    "$$ p_1(a) = \\mathbb{P}[d\\in \\text{ class }  +1] = \\frac{1}{1+\\exp(-\\langle a;x^\\star \\rangle)} $$\n",
    "Thus, from $a$, if $p_1(a)$ is close to $1$, one can decide that $d$ belongs to class $1$; and the opposite decision if $p(a)$ is close to $0$. Between the two, the appreciation is left to the data scientist depending on the application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised classification datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset\n",
    "\n",
    "We will use LibSVM formatted data, meaning that each line of the file (i.e. each example) will have the form\n",
    "\n",
    "<tt>class feature_number1:feature_value1 feature_number2:feature_value2 ... feature_number$n_i$:feature_value$n_i$ </tt>\n",
    "\n",
    "You may read such a file using MLUtils's <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.util.MLUtils.loadLibSVMFile\">`loadLibSVMFile`</a> routine on the supervised classification datasets below.\n",
    "\n",
    "The elements of the produced RDD have the form of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint\">`LabeledPoints`</a> composed of a label `example.label` corresponding to the class (+1 or -1) and a feature vector `example.features` generally encoded as a <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector\">`SparseVector`</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=MSIAM part II - Logistic Regression, master=local[*]) created by __init__ at <ipython-input-1-1674fdc40741>:11 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1674fdc40741>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSIAM part II - Logistic Regression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    257\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 259\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    260\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=MSIAM part II - Logistic Regression, master=local[*]) created by __init__ at <ipython-input-1-1674fdc40741>:11 "
     ]
    }
   ],
   "source": [
    "# set up spark environment (using Spark local mode set to # cores on your machine)\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\")\n",
    "conf.setAppName(\"MSIAM part II - Logistic Regression\")\n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remind you that you can access this interface (Spark UI) by simply opening http://localhost:4040 in a web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to LibSVM Datasets\n",
    "#LibSVMHomeDir=\"data/LibSVM/\"\n",
    "LibName=\"ionosphere.txt\"             # a small dataset to begin with\n",
    "#LibName=\"rcv1_train.binary\"          # a bigger one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 1__\n",
    "> Form an RDD from the selected dataset.\n",
    "\n",
    "> Count the number of examples, features, the number of examples of class '+1' and the density of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples (N) = 351\n",
      "number of features (D) = 34\n",
      "number of examples of class +1 = 225\n",
      "number of examples of class -1 = 126\n",
      "density = 0.884113\n",
      "10551\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "data = MLUtils.loadLibSVMFile(sc, LibName).setName(\"LibSVM\")\n",
    "#data = sc.textFile(\"ionosphere.txt\")#.setName(\"LibSVM\")\n",
    "N = data.count() # number of examples\n",
    "D = len(data.first().features) # number of features\n",
    "nb_pos_samples = data.filter(lambda x: x.label == 1).count()\n",
    "nb_neg_samples = data.filter(lambda x: x.label == -1).count()\n",
    "nb_nonzero_vals = data.map(lambda x: x.features.numNonzeros()).reduce(lambda x, y: x + y)\n",
    "density = 1. * nb_nonzero_vals / (N * D)\n",
    "\n",
    "print(\"number of examples (N) = %d\" % N)\n",
    "print(\"number of features (D) = %d\" % D)\n",
    "print(\"number of examples of class +1 = %d\" % nb_pos_samples)\n",
    "print(\"number of examples of class -1 = %d\" % nb_neg_samples)\n",
    "print(\"density = %f\" % density)\n",
    "print(nb_nonzero_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "An important first step for learning by regression is to preprocess the dataset. This processing usually consists in:\n",
    "* Adding an intercept, that is an additional feature equal to one for all examples (statistically, this accounts for the fact that the two classes may be imbalanced).\n",
    "* For the dense datasets:\n",
    "    *  normalize to have zero-mean and unit variance for every feature (except the interecept for instance.\n",
    "* For sparse datasets:\n",
    "    * normalize so that the feature vector has unit $\\ell_2$ norm for each example.\n",
    "\n",
    "This does not really change the problem but it will ease the convergence of the applied optimization algorithms.\n",
    "\n",
    "__Question 2__\n",
    "> Form a new RDD with the scaled version of the dataset.\n",
    "\n",
    "> Check that the number of examples, features, and the density is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from operator import add\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# as variance can be zero for some features, we will remove those column and insert them back\n",
    "def normalize_sample(x):\n",
    "    new_features = (x.features.toArray() - means) / np.sqrt(variance)\n",
    "    new_features = np.append(new_features, 1)\n",
    "    features_sparse_vector = SparseVector(np.shape(means)[0] + 1,\n",
    "                                          np.nonzero(new_features)[0],\n",
    "                                          new_features[np.nonzero(new_features)])\n",
    "    return LabeledPoint(x.label, features_sparse_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.4\n",
      "(1.0,(34,[0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33],[1.0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1.0,0.0376,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.3409,0.42267,-0.54487,0.18641,-0.453]))\n",
      "(34,[0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33],[1.0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1.0,0.0376,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.3409,0.42267,-0.54487,0.18641,-0.453])\n",
      "[ 1.       0.       0.99539 -0.05889  0.85243  0.02306  0.83398 -0.37708\n",
      "  1.       0.0376   0.85243 -0.17755  0.59755 -0.44945  0.60536 -0.38223\n",
      "  0.84356 -0.38542  0.58212 -0.32192  0.56971 -0.29674  0.36946 -0.47357\n",
      "  0.56811 -0.51171  0.41078 -0.46168  0.21266 -0.3409   0.42267 -0.54487\n",
      "  0.18641 -0.453  ]\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "\n",
    "first = data.first()\n",
    "print(first)\n",
    "\n",
    "features = first.features\n",
    "print(features)\n",
    "\n",
    "np_features = features.toArray()\n",
    "print(np_features)\n",
    "# data_mapped = data.map(lambda x: x.features)\n",
    "# first_mapped = data_mapped.first()\n",
    "# print(first_mapped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means:\n",
      "[ 0.78347578  0.          0.64134185  0.04437188  0.60106789  0.115889\n",
      "  0.55009507  0.11936037  0.51184809  0.18134538  0.47618265  0.15504046\n",
      "  0.4008012   0.09341368  0.34415915  0.07113234  0.381949   -0.00361681\n",
      "  0.3593896  -0.0240247   0.33669547  0.0082959   0.3624755  -0.05740575\n",
      "  0.39613467 -0.07118687  0.5416408  -0.06953761  0.37844519 -0.02790709\n",
      "  0.35251373 -0.00379376  0.34936365  0.01448011]\n",
      "variance:\n",
      "[ 0.3861657   0.          0.24700772  0.19430949  0.26948603  0.211741\n",
      "  0.24201626  0.27040786  0.25638293  0.2334447   0.31662351  0.24414675\n",
      "  0.38601268  0.24420121  0.42496998  0.20950509  0.38086098  0.24606941\n",
      "  0.39109271  0.26867235  0.37083107  0.26773094  0.36349662  0.27741755\n",
      "  0.33365214  0.25783     0.26570809  0.30166587  0.33069932  0.25730253\n",
      "  0.32566278  0.26300717  0.27239872  0.21871485]\n",
      "(1.0,(35,[0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34],[0.348433280269,0.712372367309,-0.234257237563,0.484207686793,-0.201734977754,0.577058790699,-0.954679144632,0.964074158868,-0.297510229259,0.668654651086,-0.673107318134,0.316673202835,-1.0985425274,0.400677973145,-0.990485565808,0.747985026589,-0.769680949556,0.356155483134,-0.574714507232,0.38264403556,-0.589524243017,0.0115847114771,-0.790128635368,0.29772766393,-0.867564946095,-0.253867539692,-0.713971226166,-0.288289660581,-0.617038783086,0.122936944467,-1.05505394246,-0.312220599512,-0.999594828772,1.0]))\n",
      "means in the normalized data:\n",
      "[  1.56886217e-16   0.00000000e+00  -7.08518397e-17  -6.07301484e-17\n",
      "   5.06084570e-18  -6.07301484e-17   2.83407359e-16   1.41703679e-16\n",
      "  -3.34015816e-16  -4.04867656e-17   1.06277760e-16  -1.21460297e-16\n",
      "   1.01216914e-17   5.56693027e-17  -1.67007908e-16   1.31581988e-16\n",
      "  -2.83407359e-16  -3.03650742e-17  -2.63163976e-16  -5.06084570e-17\n",
      "  -1.77129599e-16   2.53042285e-17  -2.43236896e-16  -5.06084570e-18\n",
      "   3.98541599e-17  -1.32847200e-17   0.00000000e+00   1.01216914e-17\n",
      "   2.53042285e-17   3.92215541e-17   2.02433828e-16  -3.54259199e-17\n",
      "   3.54259199e-17  -7.59126854e-17   1.00000000e+00]\n",
      "variance in the normalized data:\n",
      "[ 1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.]\n",
      "number of examples (N) in the normalized data = 351\n",
      "new number of features (D) in the normalized data = 35\n",
      "density in the normalized data = 0.971429\n"
     ]
    }
   ],
   "source": [
    "means = data.map(lambda x: x.features.toArray()).reduce(add) / N\n",
    "print('means:')\n",
    "print(means)\n",
    "\n",
    "variance = data.map(lambda x: (x.features.toArray() - means) ** 2).reduce(add) / N\n",
    "print('variance:')\n",
    "print(variance)\n",
    "\n",
    "# as we can not divide by zero, we fix the values of variance where it is 0 (the second column which is empty)\n",
    "variance[np.argwhere(variance == 0)] = 1\n",
    "\n",
    "data_normalized = data.map(normalize_sample)\n",
    "\n",
    "print(data_normalized.first())\n",
    "      \n",
    "new_means = data_normalized.map(lambda x: x.features.toArray()).reduce(add) / N\n",
    "print('means in the normalized data:')\n",
    "print(new_means)\n",
    "\n",
    "new_variance = data_normalized.map(lambda x: (x.features.toArray() - new_means) ** 2).reduce(add) / N\n",
    "print('variance in the normalized data:')\n",
    "print(new_variance)\n",
    "\n",
    "\n",
    "new_N = data_normalized.count() # number of examples\n",
    "new_D = len(data_normalized.first().features.toArray()) # number of features\n",
    "new_nb_nonzero_vals = data_normalized.map(lambda x: x.features.numNonzeros()).reduce(add)\n",
    "new_density = 1. * new_nb_nonzero_vals / (new_N * new_D)\n",
    "\n",
    "print(\"number of examples (N) in the normalized data = %d\" % new_N)\n",
    "print(\"new number of features (D) in the normalized data = %d\" % new_D)\n",
    "print(\"density in the normalized data = %f\" % new_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Initialization\n",
    "\n",
    "We will set up here the variables, and the training versus testing dataset. Indeed, we will take a portion of the dataset to learn called the `learning set`, say $95$%, and we will test our predictions on the rest, the `testing set`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 3__\n",
    "\n",
    ">  Split the scaled dataset into a training and a testing set. For instance, you may use the function <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit\">`randomSplit`</a>.\n",
    "\n",
    "> Count the number of examples, and subjects in class '+1' in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_learn = 337, N_test = 14\n",
      "number of examples of class +1 in learn = %d 218\n",
      "number of examples of class +1 in test = %d 7\n"
     ]
    }
   ],
   "source": [
    "learn, test = data_normalized.randomSplit([0.95, 0.05])\n",
    "\n",
    "N_learn = learn.count()\n",
    "N_test = test.count()\n",
    "\n",
    "nb_pos_samples_learn = learn.filter(lambda x: x.label == 1).count()\n",
    "nb_pos_samples_test = test.filter(lambda x: x.label == 1).count()\n",
    "\n",
    "print('N_learn = %d, N_test = %d' %(N_learn, N_test))\n",
    "print('number of examples of class +1 in learn = %d', nb_pos_samples_learn)\n",
    "print('number of examples of class +1 in test = %d', nb_pos_samples_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_learn = 332, N_test = 19\n",
      "number of examples of class +1 in learn = %d 214\n",
      "number of examples of class +1 in test = %d 11\n"
     ]
    }
   ],
   "source": [
    "learn, test = data.randomSplit([0.95, 0.05])\n",
    "N_learn = learn.count()\n",
    "N_test = test.count()\n",
    "\n",
    "nb_pos_samples_learn = learn.filter(lambda x: x.label == 1).count()\n",
    "nb_pos_samples_test = test.filter(lambda x: x.label == 1).count()\n",
    "\n",
    "print('N_learn = %d, N_test = %d' %(N_learn, N_test))\n",
    "print('number of examples of class +1 in learn = %d', nb_pos_samples_learn)\n",
    "print('number of examples of class +1 in test = %d', nb_pos_samples_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Minimization of the logistic loss with the Gradient algorithm\n",
    "\n",
    "The goal of this section is to: \n",
    "1. Compute gradients of the loss functions.\n",
    "2. Implement a Gradient algorithm.\n",
    "3. Observe the prediction accuracy of the developed methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 4__\n",
    ">Define a routine computing functional loss and gradient from one example \n",
    "\n",
    "For a Labeled point <tt>example</tt> (`LabeledPoint(example.label,example.features)`) that we denoted $(b_i,a_i)$ and a regressor <tt>x</tt>, compute $f_i(x) = \\log(1+\\exp(-b_i \\langle a_i,x\\rangle) )$ and $\\nabla f_i(x)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_loss_per_example(example,x):\n",
    "    \"\"\" Computes the logistic loss for a Labeled point\n",
    "    Args:\n",
    "        example: a labeled point\n",
    "        x: regressor\n",
    "    Returns:\n",
    "        real value: l \n",
    "    \"\"\"\n",
    "    res = np.log(1 + np.exp(- example.label * np.dot(example.features.toArray(), x)))\n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_grad_per_example(example,x):\n",
    "    \"\"\" Computes the logistic gradient for a Labeled point\n",
    "    Args:\n",
    "        example: a labeled point\n",
    "        x: regressor\n",
    "    Returns:\n",
    "        numpy array: g \n",
    "    \"\"\"\n",
    "    denom = (1 + np.exp(example.label * np.dot(example.features.toArray(), x)))\n",
    "    res = - example.label * example.features.toArray() / denom\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 5__\n",
    ">Implement a gradient descent algorithm to minimize\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } f(x) := \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) = \\frac{1}{m}  \\sum_{i=1}^m f_i(x).\n",
    "\\end{align*}\n",
    ">by \n",
    "* defining a function taking a stepsize and a maximal number of iterations and returning the final point as well as the value of $f(x)$ at each iteration. \n",
    "* running `x, f_tab = grad_algo(gamma,MAX_ITE)`\n",
    "\n",
    "\n",
    "For the choice of the stepsize, we help you by provinding you an upper bound on the Lipschitz constant $L$ of $\\nabla f$:\n",
    "\n",
    "$ L \\leq L_b = \\max_i 0.25 \\|a_i\\|_2^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad_algo(trainRDD, gamma, max_iter):\n",
    "    print('start grad_algo with gamma = %f, max_iter = %d' % (gamma, max_iter))\n",
    "    f_tab = [1.]\n",
    "    N = trainRDD.count()\n",
    "    x = np.zeros(len(trainRDD.first().features.toArray())) # init values = 0\n",
    "    for i in range(max_iter): # maybe change to convergence criterion\n",
    "        sg = trainRDD.map(lambda ex: logistic_grad_per_example(ex, x)).reduce(add) / N\n",
    "        x -= gamma * sg\n",
    "        ll = trainRDD.map(lambda ex: logistic_loss_per_example(ex, x)).reduce(add) / N\n",
    "        f_tab.append(ll)\n",
    "        print('[iter %d] f(x) = %f' %(i, ll))\n",
    "    print('done')\n",
    "    return x, f_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start grad_algo with gamma = 1.392621, max_iter = 100\n",
      "[iter 0] f(x) = 0.565507\n",
      "[iter 1] f(x) = 0.488141\n",
      "[iter 2] f(x) = 0.469332\n",
      "[iter 3] f(x) = 0.455753\n",
      "[iter 4] f(x) = 0.444950\n",
      "[iter 5] f(x) = 0.435923\n",
      "[iter 6] f(x) = 0.428154\n",
      "[iter 7] f(x) = 0.421339\n",
      "[iter 8] f(x) = 0.415276\n",
      "[iter 9] f(x) = 0.409827\n",
      "[iter 10] f(x) = 0.404890\n",
      "[iter 11] f(x) = 0.400387\n",
      "[iter 12] f(x) = 0.396257\n",
      "[iter 13] f(x) = 0.392450\n",
      "[iter 14] f(x) = 0.388927\n",
      "[iter 15] f(x) = 0.385654\n",
      "[iter 16] f(x) = 0.382602\n",
      "[iter 17] f(x) = 0.379749\n",
      "[iter 18] f(x) = 0.377073\n",
      "[iter 19] f(x) = 0.374557\n",
      "[iter 20] f(x) = 0.372186\n",
      "[iter 21] f(x) = 0.369947\n",
      "[iter 22] f(x) = 0.367827\n",
      "[iter 23] f(x) = 0.365816\n",
      "[iter 24] f(x) = 0.363906\n",
      "[iter 25] f(x) = 0.362088\n",
      "[iter 26] f(x) = 0.360355\n",
      "[iter 27] f(x) = 0.358701\n",
      "[iter 28] f(x) = 0.357119\n",
      "[iter 29] f(x) = 0.355605\n",
      "[iter 30] f(x) = 0.354154\n",
      "[iter 31] f(x) = 0.352761\n",
      "[iter 32] f(x) = 0.351423\n",
      "[iter 33] f(x) = 0.350136\n",
      "[iter 34] f(x) = 0.348897\n",
      "[iter 35] f(x) = 0.347703\n",
      "[iter 36] f(x) = 0.346551\n",
      "[iter 37] f(x) = 0.345440\n",
      "[iter 38] f(x) = 0.344366\n",
      "[iter 39] f(x) = 0.343327\n",
      "[iter 40] f(x) = 0.342322\n",
      "[iter 41] f(x) = 0.341349\n",
      "[iter 42] f(x) = 0.340405\n",
      "[iter 43] f(x) = 0.339491\n",
      "[iter 44] f(x) = 0.338603\n",
      "[iter 45] f(x) = 0.337742\n",
      "[iter 46] f(x) = 0.336905\n",
      "[iter 47] f(x) = 0.336091\n",
      "[iter 48] f(x) = 0.335300\n",
      "[iter 49] f(x) = 0.334530\n",
      "[iter 50] f(x) = 0.333781\n",
      "[iter 51] f(x) = 0.333051\n",
      "[iter 52] f(x) = 0.332339\n",
      "[iter 53] f(x) = 0.331646\n",
      "[iter 54] f(x) = 0.330969\n",
      "[iter 55] f(x) = 0.330309\n",
      "[iter 56] f(x) = 0.329665\n",
      "[iter 57] f(x) = 0.329036\n",
      "[iter 58] f(x) = 0.328422\n",
      "[iter 59] f(x) = 0.327821\n",
      "[iter 60] f(x) = 0.327234\n",
      "[iter 61] f(x) = 0.326660\n",
      "[iter 62] f(x) = 0.326098\n",
      "[iter 63] f(x) = 0.325549\n",
      "[iter 64] f(x) = 0.325011\n",
      "[iter 65] f(x) = 0.324484\n",
      "[iter 66] f(x) = 0.323968\n",
      "[iter 67] f(x) = 0.323463\n",
      "[iter 68] f(x) = 0.322967\n",
      "[iter 69] f(x) = 0.322482\n",
      "[iter 70] f(x) = 0.322005\n",
      "[iter 71] f(x) = 0.321538\n",
      "[iter 72] f(x) = 0.321080\n",
      "[iter 73] f(x) = 0.320631\n",
      "[iter 74] f(x) = 0.320189\n",
      "[iter 75] f(x) = 0.319756\n",
      "[iter 76] f(x) = 0.319331\n",
      "[iter 77] f(x) = 0.318913\n",
      "[iter 78] f(x) = 0.318502\n",
      "[iter 79] f(x) = 0.318099\n",
      "[iter 80] f(x) = 0.317703\n",
      "[iter 81] f(x) = 0.317313\n",
      "[iter 82] f(x) = 0.316930\n",
      "[iter 83] f(x) = 0.316553\n",
      "[iter 84] f(x) = 0.316182\n",
      "[iter 85] f(x) = 0.315818\n",
      "[iter 86] f(x) = 0.315459\n",
      "[iter 87] f(x) = 0.315106\n",
      "[iter 88] f(x) = 0.314759\n",
      "[iter 89] f(x) = 0.314417\n",
      "[iter 90] f(x) = 0.314080\n",
      "[iter 91] f(x) = 0.313749\n",
      "[iter 92] f(x) = 0.313422\n",
      "[iter 93] f(x) = 0.313101\n",
      "[iter 94] f(x) = 0.312784\n",
      "[iter 95] f(x) = 0.312472\n",
      "[iter 96] f(x) = 0.312164\n",
      "[iter 97] f(x) = 0.311861\n",
      "[iter 98] f(x) = 0.311562\n",
      "[iter 99] f(x) = 0.311267\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "max_example_norm = learn.map(lambda x: np.sqrt(x.features.dot(x.features))).reduce(lambda x,y: x if x > y else y)\n",
    "L_b = 0.25 * max_example_norm # we take the upperbound\n",
    "gamma = 2. / L_b # works better with e.g. 8. / L_b\n",
    "max_iter = 100 # first guess\n",
    "\n",
    "(x_opt, f_tab) = grad_algo(learn, gamma, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 6__\n",
    "\n",
    "> Plot the functional value versus the iterations.\n",
    "\n",
    "> Investigate if the computations are distributed over different threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAF5CAYAAADQ2iM1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2cc604c4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(max_iter+1), f_tab, color=\"black\", linewidth=1.0, linestyle=\"-\")\n",
    "plt.xlim(0, max_iter+1)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Functional value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized logisitic regression\n",
    "\n",
    "In addition to the loss, it is usual to add a regularization term of the form\n",
    "$$ r(x) = \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2 $$\n",
    "\n",
    "The first part promotes sparsity of the iterates while the second part prevents over-fitting. \n",
    "This kind of regularization is often called:\n",
    "- *elastic-net* when $ \\lambda_1$ and $ \\lambda_2$ are non-null\n",
    "- $\\ell_1$ when $\\lambda_2 = 0$\n",
    "- *Tikhonov* when $\\lambda_1 = 0$\n",
    "\n",
    "The full optimization problems now writes\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } g(x) =  \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) +  \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "__Question 7__\n",
    "\n",
    "> Which part of $g$ is smooth, which part is not? Write $g$ as \n",
    "$$ g(x) =  \\frac{1}{m}  \\sum_{i=1}^m s_i(x) + n(x)  $$\n",
    "where the $(s_i)$ are smooth function and $n$ is non smooth. \n",
    "\n",
    "> Define a function `regularized_logistic_grad_per_example(examples,x)` returning the gradient of the smooth part per example (i.e. $\\nabla s_i(x)$)\n",
    "\n",
    "> Define a function `n_prox(x,gamma)` returning the proximal operator of the non-smooth part (i.e. $\\mathbf{prox}_{\\gamma n}(y)$)\n",
    "\n",
    "we recall that\n",
    "$$ \\mathbf{prox}_{\\gamma n}(y) = \\arg\\min_x\\left\\{ n(x) + \\frac{1}{2\\gamma} \\|x-y\\|_2^2 \\right\\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def regularized_logistic_loss_per_example(example,x,lambda_1,lambda_2):\n",
    "    \"\"\" Computes the logistic loss for a Labeled point\n",
    "    Args:\n",
    "        example: a labeled point\n",
    "        x: regressor\n",
    "    Returns:\n",
    "        real value: l \n",
    "    \"\"\"\n",
    "    res = np.log(1 + np.exp(- example.label * np.dot(example.features.toArray(), x))) + lambda_2*np.dot(x,x) +lambda_1*sum(abs(x))\n",
    "    return res \n",
    "\n",
    "\n",
    "def regularized_logistic_grad_per_example(example, x, lambda_2):\n",
    "    \"\"\" Computes the logistic gradient for a Labeled point of smooth part of regularized logistic loss\n",
    "    Args:\n",
    "        example: a labeled point\n",
    "        x: regressor\n",
    "    Returns:\n",
    "        numpy array: g \n",
    "    \"\"\"\n",
    "    denom = (1 + np.exp(example.label * np.dot(example.features.toArray(), x)))\n",
    "    res = - example.label * example.features.toArray() / denom  + 2*lambda_2*x\n",
    "    return res\n",
    "\n",
    "def n_prox(y,gamma,lambda_1):\n",
    "    x = y.copy()\n",
    "    for i in range(np.size(y)):\n",
    "        if y[i] > gamma: \n",
    "            x[i] = y[i] - gamma*lambda_1\n",
    "        elif y[i] < -gamma:\n",
    "            x[i] = y[i] + gamma*lambda_1\n",
    "        else:\n",
    "            x[i] = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 8__\n",
    "\n",
    "> Compute a proximal gradient algorithm for computing a solution of\n",
    "$$ \\min_x  f(x) + r(x) = \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) + \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2 $$\n",
    "\n",
    "\n",
    "Hint: An admissible stepsize can be found by taking $\\gamma = 1/L_{b2}$ with  $ L_b = \\max_i 0.25 \\|a_i\\|_2^2 + 2\\lambda_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def proximal_gradient_algorithm(trainRDD, gamma, max_iter,lambda_1,lambda_2):\n",
    "    print('start grad_algo with gamma = %f, max_iter = %d' % (gamma, max_iter))\n",
    "    f_tab = [1.]\n",
    "    N = trainRDD.count()\n",
    "    x = np.zeros(len(trainRDD.first().features.toArray())) # init values = 0\n",
    "    for i in range(max_iter): # maybe change to convergence criterion\n",
    "        sg = trainRDD.map(lambda ex: regularized_logistic_grad_per_example(ex, x,lambda_2)).reduce(add) / N\n",
    "        x -= gamma * sg\n",
    "        x = n_prox(x,gamma,lambda_1)\n",
    "        ll = trainRDD.map(lambda ex: regularized_logistic_loss_per_example(ex, x,lambda_1,lambda_2)).reduce(add) / N\n",
    "        f_tab.append(ll)\n",
    "        print('[iter %d] f(x) = %f' %(i, ll))\n",
    "    print('done')\n",
    "    return x, f_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambda_1 = 2\n",
    "lambda_2 = 1\n",
    "max_example_norm = learn.map(lambda x: np.sqrt(x.features.dot(x.features.toArray()))).reduce(lambda x,y: x if x > y else y)\n",
    "L_b = 0.25 * max_example_norm + 2*lambda_2 # we take the upperbound\n",
    "gamma = 0.5 / L_b # works better with e.g. 8. / L_b\n",
    "max_iter = 10 # first guess\n",
    "\n",
    "(x_opt, f_tab) =  proximal_gradient_algorithm(learn, gamma, max_iter,lambda_1,lambda_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 9__\n",
    "\n",
    "> Examine the behavior and output of your proximal gradient algorithm with different values of $\\lambda_1$, $\\lambda_2$. What do you observe in terms of sparsity of the solution and convergence rate of the algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start grad_algo with gamma = 0.582048, max_iter = 100\n",
      "[iter 0] f(x) = 0.693147\n",
      "[iter 1] f(x) = 0.693147\n",
      "[iter 2] f(x) = 0.693147\n",
      "[iter 3] f(x) = 0.693147\n",
      "[iter 4] f(x) = 0.693147\n",
      "[iter 5] f(x) = 0.693147\n",
      "[iter 6] f(x) = 0.693147\n",
      "[iter 7] f(x) = 0.693147\n",
      "[iter 8] f(x) = 0.693147\n",
      "[iter 9] f(x) = 0.693147\n",
      "[iter 10] f(x) = 0.693147\n",
      "[iter 11] f(x) = 0.693147\n",
      "[iter 12] f(x) = 0.693147\n",
      "[iter 13] f(x) = 0.693147\n",
      "[iter 14] f(x) = 0.693147\n",
      "[iter 15] f(x) = 0.693147\n",
      "[iter 16] f(x) = 0.693147\n",
      "[iter 17] f(x) = 0.693147\n",
      "[iter 18] f(x) = 0.693147\n",
      "[iter 19] f(x) = 0.693147\n",
      "[iter 20] f(x) = 0.693147\n",
      "[iter 21] f(x) = 0.693147\n",
      "[iter 22] f(x) = 0.693147\n",
      "[iter 23] f(x) = 0.693147\n",
      "[iter 24] f(x) = 0.693147\n",
      "[iter 25] f(x) = 0.693147\n",
      "[iter 26] f(x) = 0.693147\n",
      "[iter 27] f(x) = 0.693147\n",
      "[iter 28] f(x) = 0.693147\n",
      "[iter 29] f(x) = 0.693147\n",
      "[iter 30] f(x) = 0.693147\n",
      "[iter 31] f(x) = 0.693147\n",
      "[iter 32] f(x) = 0.693147\n",
      "[iter 33] f(x) = 0.693147\n",
      "[iter 34] f(x) = 0.693147\n",
      "[iter 35] f(x) = 0.693147\n",
      "[iter 36] f(x) = 0.693147\n",
      "[iter 37] f(x) = 0.693147\n",
      "[iter 38] f(x) = 0.693147\n",
      "[iter 39] f(x) = 0.693147\n",
      "[iter 40] f(x) = 0.693147\n",
      "[iter 41] f(x) = 0.693147\n",
      "[iter 42] f(x) = 0.693147\n",
      "[iter 43] f(x) = 0.693147\n",
      "[iter 44] f(x) = 0.693147\n",
      "[iter 45] f(x) = 0.693147\n",
      "[iter 46] f(x) = 0.693147\n",
      "[iter 47] f(x) = 0.693147\n",
      "[iter 48] f(x) = 0.693147\n",
      "[iter 49] f(x) = 0.693147\n",
      "[iter 50] f(x) = 0.693147\n",
      "[iter 51] f(x) = 0.693147\n",
      "[iter 52] f(x) = 0.693147\n",
      "[iter 53] f(x) = 0.693147\n",
      "[iter 54] f(x) = 0.693147\n",
      "[iter 55] f(x) = 0.693147\n",
      "[iter 56] f(x) = 0.693147\n",
      "[iter 57] f(x) = 0.693147\n",
      "[iter 58] f(x) = 0.693147\n",
      "[iter 59] f(x) = 0.693147\n",
      "[iter 60] f(x) = 0.693147\n",
      "[iter 61] f(x) = 0.693147\n",
      "[iter 62] f(x) = 0.693147\n",
      "[iter 63] f(x) = 0.693147\n",
      "[iter 64] f(x) = 0.693147\n",
      "[iter 65] f(x) = 0.693147\n",
      "[iter 66] f(x) = 0.693147\n",
      "[iter 67] f(x) = 0.693147\n",
      "[iter 68] f(x) = 0.693147\n",
      "[iter 69] f(x) = 0.693147\n",
      "[iter 70] f(x) = 0.693147\n",
      "[iter 71] f(x) = 0.693147\n",
      "[iter 72] f(x) = 0.693147\n",
      "[iter 73] f(x) = 0.693147\n",
      "[iter 74] f(x) = 0.693147\n",
      "[iter 75] f(x) = 0.693147\n",
      "[iter 76] f(x) = 0.693147\n",
      "[iter 77] f(x) = 0.693147\n",
      "[iter 78] f(x) = 0.693147\n",
      "[iter 79] f(x) = 0.693147\n",
      "[iter 80] f(x) = 0.693147\n",
      "[iter 81] f(x) = 0.693147\n",
      "[iter 82] f(x) = 0.693147\n",
      "[iter 83] f(x) = 0.693147\n",
      "[iter 84] f(x) = 0.693147\n",
      "[iter 85] f(x) = 0.693147\n",
      "[iter 86] f(x) = 0.693147\n",
      "[iter 87] f(x) = 0.693147\n",
      "[iter 88] f(x) = 0.693147\n",
      "[iter 89] f(x) = 0.693147\n",
      "[iter 90] f(x) = 0.693147\n",
      "[iter 91] f(x) = 0.693147\n",
      "[iter 92] f(x) = 0.693147\n",
      "[iter 93] f(x) = 0.693147\n",
      "[iter 94] f(x) = 0.693147\n",
      "[iter 95] f(x) = 0.693147\n",
      "[iter 96] f(x) = 0.693147\n",
      "[iter 97] f(x) = 0.693147\n",
      "[iter 98] f(x) = 0.693147\n",
      "[iter 99] f(x) = 0.693147\n",
      "done\n",
      "start grad_algo with gamma = 0.582048, max_iter = 100\n",
      "[iter 0] f(x) = 0.693147\n",
      "[iter 1] f(x) = 0.693147\n",
      "[iter 2] f(x) = 0.693147\n",
      "[iter 3] f(x) = 0.693147\n",
      "[iter 4] f(x) = 0.693147\n",
      "[iter 5] f(x) = 0.693147\n",
      "[iter 6] f(x) = 0.693147\n",
      "[iter 7] f(x) = 0.693147\n",
      "[iter 8] f(x) = 0.693147\n",
      "[iter 9] f(x) = 0.693147\n",
      "[iter 10] f(x) = 0.693147\n",
      "[iter 11] f(x) = 0.693147\n",
      "[iter 12] f(x) = 0.693147\n",
      "[iter 13] f(x) = 0.693147\n",
      "[iter 14] f(x) = 0.693147\n",
      "[iter 15] f(x) = 0.693147\n",
      "[iter 16] f(x) = 0.693147\n",
      "[iter 17] f(x) = 0.693147\n",
      "[iter 18] f(x) = 0.693147\n",
      "[iter 19] f(x) = 0.693147\n",
      "[iter 20] f(x) = 0.693147\n",
      "[iter 21] f(x) = 0.693147\n",
      "[iter 22] f(x) = 0.693147\n",
      "[iter 23] f(x) = 0.693147\n",
      "[iter 24] f(x) = 0.693147\n",
      "[iter 25] f(x) = 0.693147\n",
      "[iter 26] f(x) = 0.693147\n",
      "[iter 27] f(x) = 0.693147\n",
      "[iter 28] f(x) = 0.693147\n",
      "[iter 29] f(x) = 0.693147\n",
      "[iter 30] f(x) = 0.693147\n",
      "[iter 31] f(x) = 0.693147\n",
      "[iter 32] f(x) = 0.693147\n",
      "[iter 33] f(x) = 0.693147\n",
      "[iter 34] f(x) = 0.693147\n",
      "[iter 35] f(x) = 0.693147\n",
      "[iter 36] f(x) = 0.693147\n",
      "[iter 37] f(x) = 0.693147\n",
      "[iter 38] f(x) = 0.693147\n",
      "[iter 39] f(x) = 0.693147\n",
      "[iter 40] f(x) = 0.693147\n",
      "[iter 41] f(x) = 0.693147\n",
      "[iter 42] f(x) = 0.693147\n",
      "[iter 43] f(x) = 0.693147\n",
      "[iter 44] f(x) = 0.693147\n",
      "[iter 45] f(x) = 0.693147\n",
      "[iter 46] f(x) = 0.693147\n",
      "[iter 47] f(x) = 0.693147\n",
      "[iter 48] f(x) = 0.693147\n",
      "[iter 49] f(x) = 0.693147\n",
      "[iter 50] f(x) = 0.693147\n",
      "[iter 51] f(x) = 0.693147\n",
      "[iter 52] f(x) = 0.693147\n",
      "[iter 53] f(x) = 0.693147\n",
      "[iter 54] f(x) = 0.693147\n",
      "[iter 55] f(x) = 0.693147\n",
      "[iter 56] f(x) = 0.693147\n",
      "[iter 57] f(x) = 0.693147\n",
      "[iter 58] f(x) = 0.693147\n",
      "[iter 59] f(x) = 0.693147\n",
      "[iter 60] f(x) = 0.693147\n",
      "[iter 61] f(x) = 0.693147\n",
      "[iter 62] f(x) = 0.693147\n",
      "[iter 63] f(x) = 0.693147\n",
      "[iter 64] f(x) = 0.693147\n",
      "[iter 65] f(x) = 0.693147\n",
      "[iter 66] f(x) = 0.693147\n",
      "[iter 67] f(x) = 0.693147\n",
      "[iter 68] f(x) = 0.693147\n",
      "[iter 69] f(x) = 0.693147\n",
      "[iter 70] f(x) = 0.693147\n",
      "[iter 71] f(x) = 0.693147\n",
      "[iter 72] f(x) = 0.693147\n",
      "[iter 73] f(x) = 0.693147\n",
      "[iter 74] f(x) = 0.693147\n",
      "[iter 75] f(x) = 0.693147\n",
      "[iter 76] f(x) = 0.693147\n",
      "[iter 77] f(x) = 0.693147\n",
      "[iter 78] f(x) = 0.693147\n",
      "[iter 79] f(x) = 0.693147\n",
      "[iter 80] f(x) = 0.693147\n",
      "[iter 81] f(x) = 0.693147\n",
      "[iter 82] f(x) = 0.693147\n",
      "[iter 83] f(x) = 0.693147\n",
      "[iter 84] f(x) = 0.693147\n",
      "[iter 85] f(x) = 0.693147\n",
      "[iter 86] f(x) = 0.693147\n",
      "[iter 87] f(x) = 0.693147\n",
      "[iter 88] f(x) = 0.693147\n",
      "[iter 89] f(x) = 0.693147\n",
      "[iter 90] f(x) = 0.693147\n",
      "[iter 91] f(x) = 0.693147\n",
      "[iter 92] f(x) = 0.693147\n",
      "[iter 93] f(x) = 0.693147\n",
      "[iter 94] f(x) = 0.693147\n",
      "[iter 95] f(x) = 0.693147\n",
      "[iter 96] f(x) = 0.693147\n",
      "[iter 97] f(x) = 0.693147\n",
      "[iter 98] f(x) = 0.693147\n",
      "[iter 99] f(x) = 0.693147\n",
      "done\n",
      "start grad_algo with gamma = 0.582048, max_iter = 100\n",
      "[iter 0] f(x) = 0.693147\n",
      "[iter 1] f(x) = 0.693147\n",
      "[iter 2] f(x) = 0.693147\n",
      "[iter 3] f(x) = 0.693147\n",
      "[iter 4] f(x) = 0.693147\n",
      "[iter 5] f(x) = 0.693147\n",
      "[iter 6] f(x) = 0.693147\n",
      "[iter 7] f(x) = 0.693147\n",
      "[iter 8] f(x) = 0.693147\n",
      "[iter 9] f(x) = 0.693147\n",
      "[iter 10] f(x) = 0.693147\n",
      "[iter 11] f(x) = 0.693147\n",
      "[iter 12] f(x) = 0.693147\n",
      "[iter 13] f(x) = 0.693147\n",
      "[iter 14] f(x) = 0.693147\n",
      "[iter 15] f(x) = 0.693147\n",
      "[iter 16] f(x) = 0.693147\n",
      "[iter 17] f(x) = 0.693147\n",
      "[iter 18] f(x) = 0.693147\n",
      "[iter 19] f(x) = 0.693147\n",
      "[iter 20] f(x) = 0.693147\n",
      "[iter 21] f(x) = 0.693147\n",
      "[iter 22] f(x) = 0.693147\n",
      "[iter 23] f(x) = 0.693147\n",
      "[iter 24] f(x) = 0.693147\n",
      "[iter 25] f(x) = 0.693147\n",
      "[iter 26] f(x) = 0.693147\n",
      "[iter 27] f(x) = 0.693147\n",
      "[iter 28] f(x) = 0.693147\n",
      "[iter 29] f(x) = 0.693147\n",
      "[iter 30] f(x) = 0.693147\n",
      "[iter 31] f(x) = 0.693147\n",
      "[iter 32] f(x) = 0.693147\n",
      "[iter 33] f(x) = 0.693147\n",
      "[iter 34] f(x) = 0.693147\n",
      "[iter 35] f(x) = 0.693147\n",
      "[iter 36] f(x) = 0.693147\n",
      "[iter 37] f(x) = 0.693147\n",
      "[iter 38] f(x) = 0.693147\n",
      "[iter 39] f(x) = 0.693147\n",
      "[iter 40] f(x) = 0.693147\n",
      "[iter 41] f(x) = 0.693147\n",
      "[iter 42] f(x) = 0.693147\n",
      "[iter 43] f(x) = 0.693147\n",
      "[iter 44] f(x) = 0.693147\n",
      "[iter 45] f(x) = 0.693147\n",
      "[iter 46] f(x) = 0.693147\n",
      "[iter 47] f(x) = 0.693147\n",
      "[iter 48] f(x) = 0.693147\n",
      "[iter 49] f(x) = 0.693147\n",
      "[iter 50] f(x) = 0.693147\n",
      "[iter 51] f(x) = 0.693147\n",
      "[iter 52] f(x) = 0.693147\n",
      "[iter 53] f(x) = 0.693147\n",
      "[iter 54] f(x) = 0.693147\n",
      "[iter 55] f(x) = 0.693147\n",
      "[iter 56] f(x) = 0.693147\n",
      "[iter 57] f(x) = 0.693147\n",
      "[iter 58] f(x) = 0.693147\n",
      "[iter 59] f(x) = 0.693147\n",
      "[iter 60] f(x) = 0.693147\n",
      "[iter 61] f(x) = 0.693147\n",
      "[iter 62] f(x) = 0.693147\n",
      "[iter 63] f(x) = 0.693147\n",
      "[iter 64] f(x) = 0.693147\n",
      "[iter 65] f(x) = 0.693147\n",
      "[iter 66] f(x) = 0.693147\n",
      "[iter 67] f(x) = 0.693147\n",
      "[iter 68] f(x) = 0.693147\n",
      "[iter 69] f(x) = 0.693147\n",
      "[iter 70] f(x) = 0.693147\n",
      "[iter 71] f(x) = 0.693147\n",
      "[iter 72] f(x) = 0.693147\n",
      "[iter 73] f(x) = 0.693147\n",
      "[iter 74] f(x) = 0.693147\n",
      "[iter 75] f(x) = 0.693147\n",
      "[iter 76] f(x) = 0.693147\n",
      "[iter 77] f(x) = 0.693147\n",
      "[iter 78] f(x) = 0.693147\n",
      "[iter 79] f(x) = 0.693147\n",
      "[iter 80] f(x) = 0.693147\n",
      "[iter 81] f(x) = 0.693147\n",
      "[iter 82] f(x) = 0.693147\n",
      "[iter 83] f(x) = 0.693147\n",
      "[iter 84] f(x) = 0.693147\n",
      "[iter 85] f(x) = 0.693147\n",
      "[iter 86] f(x) = 0.693147\n",
      "[iter 87] f(x) = 0.693147\n",
      "[iter 88] f(x) = 0.693147\n",
      "[iter 89] f(x) = 0.693147\n",
      "[iter 90] f(x) = 0.693147\n",
      "[iter 91] f(x) = 0.693147\n",
      "[iter 92] f(x) = 0.693147\n",
      "[iter 93] f(x) = 0.693147\n",
      "[iter 94] f(x) = 0.693147\n",
      "[iter 95] f(x) = 0.693147\n",
      "[iter 96] f(x) = 0.693147\n",
      "[iter 97] f(x) = 0.693147\n",
      "[iter 98] f(x) = 0.693147\n",
      "[iter 99] f(x) = 0.693147\n",
      "done\n",
      "start grad_algo with gamma = 0.582048, max_iter = 100\n",
      "[iter 0] f(x) = 0.693147\n",
      "[iter 1] f(x) = 0.693147\n",
      "[iter 2] f(x) = 0.693147\n",
      "[iter 3] f(x) = 0.693147\n",
      "[iter 4] f(x) = 0.693147\n",
      "[iter 5] f(x) = 0.693147\n",
      "[iter 6] f(x) = 0.693147\n",
      "[iter 7] f(x) = 0.693147\n",
      "[iter 8] f(x) = 0.693147\n",
      "[iter 9] f(x) = 0.693147\n",
      "[iter 10] f(x) = 0.693147\n",
      "[iter 11] f(x) = 0.693147\n",
      "[iter 12] f(x) = 0.693147\n",
      "[iter 13] f(x) = 0.693147\n",
      "[iter 14] f(x) = 0.693147\n",
      "[iter 15] f(x) = 0.693147\n",
      "[iter 16] f(x) = 0.693147\n",
      "[iter 17] f(x) = 0.693147\n",
      "[iter 18] f(x) = 0.693147\n",
      "[iter 19] f(x) = 0.693147\n",
      "[iter 20] f(x) = 0.693147\n",
      "[iter 21] f(x) = 0.693147\n",
      "[iter 22] f(x) = 0.693147\n",
      "[iter 23] f(x) = 0.693147\n",
      "[iter 24] f(x) = 0.693147\n",
      "[iter 25] f(x) = 0.693147\n",
      "[iter 26] f(x) = 0.693147\n",
      "[iter 27] f(x) = 0.693147\n",
      "[iter 28] f(x) = 0.693147\n",
      "[iter 29] f(x) = 0.693147\n",
      "[iter 30] f(x) = 0.693147\n",
      "[iter 31] f(x) = 0.693147\n",
      "[iter 32] f(x) = 0.693147\n",
      "[iter 33] f(x) = 0.693147\n",
      "[iter 34] f(x) = 0.693147\n",
      "[iter 35] f(x) = 0.693147\n",
      "[iter 36] f(x) = 0.693147\n",
      "[iter 37] f(x) = 0.693147\n",
      "[iter 38] f(x) = 0.693147\n",
      "[iter 39] f(x) = 0.693147\n",
      "[iter 40] f(x) = 0.693147\n",
      "[iter 41] f(x) = 0.693147\n",
      "[iter 42] f(x) = 0.693147\n",
      "[iter 43] f(x) = 0.693147\n",
      "[iter 44] f(x) = 0.693147\n",
      "[iter 45] f(x) = 0.693147\n",
      "[iter 46] f(x) = 0.693147\n",
      "[iter 47] f(x) = 0.693147\n",
      "[iter 48] f(x) = 0.693147\n",
      "[iter 49] f(x) = 0.693147\n",
      "[iter 50] f(x) = 0.693147\n",
      "[iter 51] f(x) = 0.693147\n",
      "[iter 52] f(x) = 0.693147\n",
      "[iter 53] f(x) = 0.693147\n",
      "[iter 54] f(x) = 0.693147\n",
      "[iter 55] f(x) = 0.693147\n",
      "[iter 56] f(x) = 0.693147\n",
      "[iter 57] f(x) = 0.693147\n",
      "[iter 58] f(x) = 0.693147\n",
      "[iter 59] f(x) = 0.693147\n",
      "[iter 60] f(x) = 0.693147\n",
      "[iter 61] f(x) = 0.693147\n",
      "[iter 62] f(x) = 0.693147\n",
      "[iter 63] f(x) = 0.693147\n",
      "[iter 64] f(x) = 0.693147\n",
      "[iter 65] f(x) = 0.693147\n",
      "[iter 66] f(x) = 0.693147\n",
      "[iter 67] f(x) = 0.693147\n",
      "[iter 68] f(x) = 0.693147\n",
      "[iter 69] f(x) = 0.693147\n",
      "[iter 70] f(x) = 0.693147\n",
      "[iter 71] f(x) = 0.693147\n",
      "[iter 72] f(x) = 0.693147\n",
      "[iter 73] f(x) = 0.693147\n",
      "[iter 74] f(x) = 0.693147\n",
      "[iter 75] f(x) = 0.693147\n",
      "[iter 76] f(x) = 0.693147\n",
      "[iter 77] f(x) = 0.693147\n",
      "[iter 78] f(x) = 0.693147\n",
      "[iter 79] f(x) = 0.693147\n",
      "[iter 80] f(x) = 0.693147\n",
      "[iter 81] f(x) = 0.693147\n",
      "[iter 82] f(x) = 0.693147\n",
      "[iter 83] f(x) = 0.693147\n",
      "[iter 84] f(x) = 0.693147\n",
      "[iter 85] f(x) = 0.693147\n",
      "[iter 86] f(x) = 0.693147\n",
      "[iter 87] f(x) = 0.693147\n",
      "[iter 88] f(x) = 0.693147\n",
      "[iter 89] f(x) = 0.693147\n",
      "[iter 90] f(x) = 0.693147\n",
      "[iter 91] f(x) = 0.693147\n",
      "[iter 92] f(x) = 0.693147\n",
      "[iter 93] f(x) = 0.693147\n",
      "[iter 94] f(x) = 0.693147\n",
      "[iter 95] f(x) = 0.693147\n",
      "[iter 96] f(x) = 0.693147\n",
      "[iter 97] f(x) = 0.693147\n",
      "[iter 98] f(x) = 0.693147\n",
      "[iter 99] f(x) = 0.693147\n",
      "done\n",
      "start grad_algo with gamma = 0.582048, max_iter = 100\n",
      "[iter 0] f(x) = 0.693147\n",
      "[iter 1] f(x) = 0.693147\n",
      "[iter 2] f(x) = 0.693147\n",
      "[iter 3] f(x) = 0.693147\n",
      "[iter 4] f(x) = 0.693147\n",
      "[iter 5] f(x) = 0.693147\n",
      "[iter 6] f(x) = 0.693147\n",
      "[iter 7] f(x) = 0.693147\n",
      "[iter 8] f(x) = 0.693147\n",
      "[iter 9] f(x) = 0.693147\n",
      "[iter 10] f(x) = 0.693147\n",
      "[iter 11] f(x) = 0.693147\n",
      "[iter 12] f(x) = 0.693147\n",
      "[iter 13] f(x) = 0.693147\n",
      "[iter 14] f(x) = 0.693147\n",
      "[iter 15] f(x) = 0.693147\n",
      "[iter 16] f(x) = 0.693147\n",
      "[iter 17] f(x) = 0.693147\n",
      "[iter 18] f(x) = 0.693147\n",
      "[iter 19] f(x) = 0.693147\n",
      "[iter 20] f(x) = 0.693147\n",
      "[iter 21] f(x) = 0.693147\n",
      "[iter 22] f(x) = 0.693147\n",
      "[iter 23] f(x) = 0.693147\n",
      "[iter 24] f(x) = 0.693147\n",
      "[iter 25] f(x) = 0.693147\n",
      "[iter 26] f(x) = 0.693147\n",
      "[iter 27] f(x) = 0.693147\n",
      "[iter 28] f(x) = 0.693147\n",
      "[iter 29] f(x) = 0.693147\n",
      "[iter 30] f(x) = 0.693147\n",
      "[iter 31] f(x) = 0.693147\n",
      "[iter 32] f(x) = 0.693147\n",
      "[iter 33] f(x) = 0.693147\n",
      "[iter 34] f(x) = 0.693147\n",
      "[iter 35] f(x) = 0.693147\n",
      "[iter 36] f(x) = 0.693147\n",
      "[iter 37] f(x) = 0.693147\n",
      "[iter 38] f(x) = 0.693147\n",
      "[iter 39] f(x) = 0.693147\n",
      "[iter 40] f(x) = 0.693147\n",
      "[iter 41] f(x) = 0.693147\n",
      "[iter 42] f(x) = 0.693147\n",
      "[iter 43] f(x) = 0.693147\n",
      "[iter 44] f(x) = 0.693147\n",
      "[iter 45] f(x) = 0.693147\n",
      "[iter 46] f(x) = 0.693147\n",
      "[iter 47] f(x) = 0.693147\n",
      "[iter 48] f(x) = 0.693147\n",
      "[iter 49] f(x) = 0.693147\n",
      "[iter 50] f(x) = 0.693147\n",
      "[iter 51] f(x) = 0.693147\n",
      "[iter 52] f(x) = 0.693147\n",
      "[iter 53] f(x) = 0.693147\n",
      "[iter 54] f(x) = 0.693147\n",
      "[iter 55] f(x) = 0.693147\n",
      "[iter 56] f(x) = 0.693147\n",
      "[iter 57] f(x) = 0.693147\n",
      "[iter 58] f(x) = 0.693147\n",
      "[iter 59] f(x) = 0.693147\n",
      "[iter 60] f(x) = 0.693147\n",
      "[iter 61] f(x) = 0.693147\n",
      "[iter 62] f(x) = 0.693147\n",
      "[iter 63] f(x) = 0.693147\n",
      "[iter 64] f(x) = 0.693147\n",
      "[iter 65] f(x) = 0.693147\n",
      "[iter 66] f(x) = 0.693147\n",
      "[iter 67] f(x) = 0.693147\n",
      "[iter 68] f(x) = 0.693147\n",
      "[iter 69] f(x) = 0.693147\n",
      "[iter 70] f(x) = 0.693147\n",
      "[iter 71] f(x) = 0.693147\n",
      "[iter 72] f(x) = 0.693147\n",
      "[iter 73] f(x) = 0.693147\n",
      "[iter 74] f(x) = 0.693147\n",
      "[iter 75] f(x) = 0.693147\n",
      "[iter 76] f(x) = 0.693147\n",
      "[iter 77] f(x) = 0.693147\n",
      "[iter 78] f(x) = 0.693147\n",
      "[iter 79] f(x) = 0.693147\n",
      "[iter 80] f(x) = 0.693147\n",
      "[iter 81] f(x) = 0.693147\n",
      "[iter 82] f(x) = 0.693147\n",
      "[iter 83] f(x) = 0.693147\n",
      "[iter 84] f(x) = 0.693147\n",
      "[iter 85] f(x) = 0.693147\n"
     ]
    }
   ],
   "source": [
    "max_example_norm = learn.map(lambda x: np.sqrt(x.features.dot(x.features))).reduce(lambda x,y: x if x > y else y)\n",
    "\n",
    "for i in range(10):\n",
    "    lambda_1 = i\n",
    "    lambda_2 = 1\n",
    "    L_b = 0.25 * max_example_norm + 2*lambda_2 # we take the upperbound\n",
    "    gamma = 2. / L_b # works better with e.g. 8. / L_b\n",
    "    max_iter = 100 # first guess\n",
    "    (x_opt, f_tab) = proximal_gradient_algorithm(learn, gamma, max_iter,lambda_1,lambda_2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Question 10__\n",
    "\n",
    "> Write a function that evaluates the accuracy of the classification on the training dataset.\n",
    "\n",
    "> Investigate how this accuracy change when playing with the regularization terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-71-fd74c5210587>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-71-fd74c5210587>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    p = test.map(lambda ex: 1./(1+exp(−np.dot(ex,x_opt)))\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "def accuracy(learn, test, lambda_1, lambda_2):\n",
    "    (x_opt, f_tab) = proximal_gradient_algorithm(learn, gamma, max_iter,lambda_1,lambda_2)\n",
    "    p = test.map(lambda ex: 1./(1+exp(−np.dot(ex,x_opt)))\n",
    "    res = p.map(lambda ex: np.abs(np.sign(ex - 0.5) - test.labels()).reduce(add)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To go further\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerations\n",
    "\n",
    "A popular acceleration method to improve the convergence rate of proximal gradient algorithm is the addition of inertia. That is, contructing the next gradient input by a combination of the last two outputs.\n",
    "\n",
    "\n",
    "In particular, Nesterov's acceleration is the most popular form of inertia. It writes\n",
    "$$ \\left\\{ \\begin{array}{l}   y_{k+1} = \\mathbf{prox\\_grad}(x_k) \\\\ x_{k+1} = y_{k+1} + \\alpha_{k+1} (y_{k+1} - y_k)  \\end{array} \\right. $$ \n",
    "with\n",
    "* $\\mathbf{prox\\_grad}$ the proximal gradient operation\n",
    "* $(\\alpha_{k})$ the inertial sequence defined as $\\alpha_k = \\frac{t_k-1}{t_{k+1}}$ and $t_0 = 0$ and $t_{k+1} = \\frac{1+\\sqrt{1+4t_k^2}}{2}$\n",
    "\n",
    "__Question 11__\n",
    "\n",
    "> Implement a fast proximal gradient with this kind of inertia (This algorithm is often nicknamed FISTA).\n",
    "\n",
    "> Compare the convergence speed with the vanilla proximal gradient algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental methods\n",
    "\n",
    "When dimension increases, incremental algorithms are often priviledged. \n",
    "\n",
    "A possible incremental algorithm for a problem such as regularized logistic regression is MISO (see *J Mairal. Incremental Majorization-Minimization Optimization with Application to Large-Scale Machine Learning. SIAM Journal on Optimization,2015 and ICML 2014.*):\n",
    "\n",
    "* Draw randomly a sample $n$\n",
    "* Compute $x^n_{k+1} = \\mathbf{prox}_{\\gamma g} (\\bar{x}_k) - \\gamma \\nabla f_n(\\mathbf{prox}_{\\gamma g} (\\bar{x}_k) )$\n",
    "* For all $i\\neq n$, $x^i_{k+1}=x^i_k$ \n",
    "* Compute new $\\bar{x}_{k+1} = \\frac{1}{m} \\sum_{j=1}^m x^j_{k+1}$\n",
    " \n",
    "\n",
    "__Question 12__\n",
    "\n",
    "> Implement this incremental algorithm and compare with the previous algorithms in terms of convergence time and functional value versus number of passes over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
