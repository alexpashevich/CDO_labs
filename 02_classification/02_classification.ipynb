{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was created by Franck Iutzeler, Jerome Malick and Yann Vernaz (2016).</i></small>\n",
    "<!-- Credit (images) Jeffrey Keating Thompson. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)</h3></center>\n",
    "<hr>\n",
    "<center><h1>Convex and distributed optimization</h1></center>\n",
    "<center><h2>Part II - Classification (3h + 3h home work)</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "In this Lab, we will investigate some gradient-based and proximal algorithms on the binary classification problems with logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classification and Logistic Regression\n",
    "\n",
    "### Machine Learning as an Optimization problem\n",
    "\n",
    "We have some *data*  $\\mathcal{D}$ consisting of $m$ *examples* $\\{d_i\\}$; each example consisting of a *feature* vector $a_i\\in\\mathbb{R}^d$ and an *observation* $b_i\\in \\mathcal{O}$: $\\mathcal{D} = \\{[a_i,b_i]\\}_{i=1..m}$ .\n",
    "\n",
    "\n",
    "The goal of *supervised learning* is to construct a predictor for the observations when given feature vectors.\n",
    "\n",
    "\n",
    "A popular approach is based on *linear models* which are based on finding a *parameter* $x$ such that the real number $\\langle a_i , x \\rangle$ is used to predict the value of the observation through a *predictor function* $g:\\mathbb{R}\\to \\mathcal{O}$: $g(\\langle a_i , x \\rangle)$ is the predicted value from $a_i$.\n",
    "\n",
    "\n",
    "In order to find such a parameter, we use the available data and a *loss* $\\ell$ that penalizes the error made between the predicted $g(\\langle a_i , x \\rangle)$ and observed $b_i$ values. For each example $i$, the corresponding error function for a parameter $x$ is $f_i(x) =   \\ell( g(\\langle a_i , x \\rangle) ; b_i )$. Using the whole data, the parameter that minimizes the total error is the solution of the minimization problem\n",
    "$$ \\min_{x\\in\\mathbb{R}^d} \\frac{1}{m} \\sum_{i=1}^m f_i(x) = \\frac{1}{m} \\sum_{i=1}^m  \\ell( g(\\langle a_i , x \\rangle) ; b_i ). $$\n",
    "\n",
    "\n",
    "### Binary Classification with Logisitic Regression\n",
    "\n",
    "In our setup, the observations are binary: $\\mathcal{O} = \\{-1 , +1 \\}$, and the *Logistic loss* is used to form the following optimization problem\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } f(x) := \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ).\n",
    "\\end{align*}\n",
    "\n",
    "Under some statistical hypotheses, $x^\\star = \\arg\\min f(x)$ maximizes the likelihood of the labels knowing the features vector. Then, for a new point $d$ with features vector $a$, \n",
    "$$ p_1(a) = \\mathbb{P}[d\\in \\text{ class }  +1] = \\frac{1}{1+\\exp(-\\langle a;x^\\star \\rangle)} $$\n",
    "Thus, from $a$, if $p_1(a)$ is close to $1$, one can decide that $d$ belongs to class $1$; and the opposite decision if $p(a)$ is close to $0$. Between the two, the appreciation is left to the data scientist depending on the application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised classification datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset\n",
    "\n",
    "We will use LibSVM formatted data, meaning that each line of the file (i.e. each example) will have the form\n",
    "\n",
    "<tt>class feature_number1:feature_value1 feature_number2:feature_value2 ... feature_number$n_i$:feature_value$n_i$ </tt>\n",
    "\n",
    "You may read such a file using MLUtils's <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.util.MLUtils.loadLibSVMFile\">`loadLibSVMFile`</a> routine on the supervised classification datasets below.\n",
    "\n",
    "The elements of the produced RDD have the form of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint\">`LabeledPoints`</a> composed of a label `example.label` corresponding to the class (+1 or -1) and a feature vector `example.features` generally encoded as a <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector\">`SparseVector`</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up spark environment (using Spark local mode set to # cores on your machine)\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\")\n",
    "conf.setAppName(\"MSIAM part II - Logistic Regression\")\n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remind you that you can access this interface (Spark UI) by simply opening http://localhost:4040 in a web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to LibSVM Datasets\n",
    "LibSVMHomeDir=\"../data/LibSVM/\"\n",
    "LibName=\"ionosphere.txt\"             # a small dataset to begin with\n",
    "#LibName=\"rcv1_train.binary\"          # a bigger one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 1__\n",
    "> Form an RDD from the selected dataset.\n",
    "\n",
    "> Count the number of examples, features, the number of examples of class '+1' and the density of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples (N) = 351\n",
      "number of features (D) = 34\n",
      "number of examples of class +1 = 225\n",
      "number of examples of class -1 = 126\n",
      "density = 0.884113\n",
      "10551\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "data = MLUtils.loadLibSVMFile(sc, LibSVMHomeDir + LibName).setName(\"LibSVM\")\n",
    "N = data.count() # number of examples\n",
    "D = len(data.first().features) # number of features\n",
    "nb_pos_samples = data.filter(lambda x: x.label == 1).count()\n",
    "nb_neg_samples = data.filter(lambda x: x.label == -1).count()\n",
    "nb_nonzero_vals = data.map(lambda x: x.features.numNonzeros()).reduce(lambda x, y: x + y)\n",
    "density = 1. * nb_nonzero_vals / (N * D)\n",
    "\n",
    "print(\"number of examples (N) = %d\" % N)\n",
    "print(\"number of features (D) = %d\" % D)\n",
    "print(\"number of examples of class +1 = %d\" % nb_pos_samples)\n",
    "print(\"number of examples of class -1 = %d\" % nb_neg_samples)\n",
    "print(\"density = %f\" % density)\n",
    "print(nb_nonzero_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "An important first step for learning by regression is to preprocess the dataset. This processing usually consists in:\n",
    "* Adding an intercept, that is an additional feature equal to one for all examples (statistically, this accounts for the fact that the two classes may be imbalanced).\n",
    "* For the dense datasets:\n",
    "    *  normalize to have zero-mean and unit variance for every feature (except the interecept for instance.\n",
    "* For sparse datasets:\n",
    "    * normalize so that the feature vector has unit $\\ell_2$ norm for each example.\n",
    "\n",
    "This does not really change the problem but it will ease the convergence of the applied optimization algorithms.\n",
    "\n",
    "__Question 2__\n",
    "> Form a new RDD with the scaled version of the dataset.\n",
    "\n",
    "> Check that the number of examples, features, and the density is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from operator import add\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# as variance can be zero for some features, we will remove those column and insert them back\n",
    "def normalize_sample(x):\n",
    "    new_features = (np.array(x.features) - means) / np.sqrt(variance)\n",
    "    new_features = np.append(new_features, 1)\n",
    "    features_sparse_vector = SparseVector(np.shape(means)[0] + 1,\n",
    "                                          np.nonzero(new_features)[0],\n",
    "                                          new_features[np.nonzero(new_features)])\n",
    "    return LabeledPoint(x.label, features_sparse_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means:\n",
      "[ 0.78347578  0.          0.64134185  0.04437188  0.60106789  0.115889\n",
      "  0.55009507  0.11936037  0.51184809  0.18134538  0.47618265  0.15504046\n",
      "  0.4008012   0.09341368  0.34415915  0.07113234  0.381949   -0.00361681\n",
      "  0.3593896  -0.0240247   0.33669547  0.0082959   0.3624755  -0.05740575\n",
      "  0.39613467 -0.07118687  0.5416408  -0.06953761  0.37844519 -0.02790709\n",
      "  0.35251373 -0.00379376  0.34936365  0.01448011]\n",
      "variance:\n",
      "[ 0.3861657   0.          0.24700772  0.19430949  0.26948603  0.211741\n",
      "  0.24201626  0.27040786  0.25638293  0.2334447   0.31662351  0.24414675\n",
      "  0.38601268  0.24420121  0.42496998  0.20950509  0.38086098  0.24606941\n",
      "  0.39109271  0.26867235  0.37083107  0.26773094  0.36349662  0.27741755\n",
      "  0.33365214  0.25783     0.26570809  0.30166587  0.33069932  0.25730253\n",
      "  0.32566278  0.26300717  0.27239872  0.21871485]\n",
      "means in the normalized data:\n",
      "[  1.56886217e-16   0.00000000e+00  -7.08518397e-17  -6.07301484e-17\n",
      "   5.06084570e-18  -6.07301484e-17   2.83407359e-16   1.41703679e-16\n",
      "  -3.34015816e-16  -4.04867656e-17   1.06277760e-16  -1.21460297e-16\n",
      "   1.01216914e-17   5.56693027e-17  -1.67007908e-16   1.31581988e-16\n",
      "  -2.83407359e-16  -3.03650742e-17  -2.63163976e-16  -5.06084570e-17\n",
      "  -1.77129599e-16   2.53042285e-17  -2.43236896e-16  -5.06084570e-18\n",
      "   3.98541599e-17  -1.32847200e-17   0.00000000e+00   1.01216914e-17\n",
      "   2.53042285e-17   3.92215541e-17   2.02433828e-16  -3.54259199e-17\n",
      "   3.54259199e-17  -7.59126854e-17   1.00000000e+00]\n",
      "variance in the normalized data:\n",
      "[ 1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.]\n",
      "number of examples (N) in the normalized data = 351\n",
      "new number of features (D) in the normalized data = 35\n",
      "density in the normalized data = 0.971429\n"
     ]
    }
   ],
   "source": [
    "means = data.map(lambda x: np.array(x.features)).reduce(add) / N\n",
    "print('means:')\n",
    "print(means)\n",
    "\n",
    "variance = data.map(lambda x: (np.array(x.features) - means) ** 2).reduce(add) / N\n",
    "print('variance:')\n",
    "print(variance)\n",
    "\n",
    "# as we can not divide by zero, we fix the values of varience where it is 0 (the second column which is empty)\n",
    "variance[np.argwhere(variance == 0)] = 1\n",
    "\n",
    "data_normalized = data.map(normalize_sample)\n",
    "\n",
    "# print(data_normalized.first())\n",
    "      \n",
    "new_means = data_normalized.map(lambda x: np.array(x.features)).reduce(add) / N\n",
    "print('means in the normalized data:')\n",
    "print(new_means)\n",
    "\n",
    "new_variance = data_normalized.map(lambda x: (np.array(x.features) - new_means) ** 2).reduce(add) / N\n",
    "print('variance in the normalized data:')\n",
    "print(new_variance)\n",
    "\n",
    "\n",
    "new_N = data_normalized.count() # number of examples\n",
    "new_D = len(data_normalized.first().features) # number of features\n",
    "new_nb_nonzero_vals = data_normalized.map(lambda x: x.features.numNonzeros()).reduce(add)\n",
    "new_density = 1. * new_nb_nonzero_vals / (new_N * new_D)\n",
    "\n",
    "print(\"number of examples (N) in the normalized data = %d\" % new_N)\n",
    "print(\"new number of features (D) in the normalized data = %d\" % new_D)\n",
    "print(\"density in the normalized data = %f\" % new_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Initialization\n",
    "\n",
    "We will set up here the variables, and the training versus testing dataset. Indeed, we will take a portion of the dataset to learn called the `learning set`, say $95$%, and we will test our predictions on the rest, the `testing set`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 3__\n",
    "\n",
    ">  Split the scaled dataset into a training and a testing set. For instance, you may use the function <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit\">`randomSplit`</a>.\n",
    "\n",
    "> Count the number of examples, and subjects in class '+1' in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_learn = 337, N_test = 14\n",
      "number of examples of class +1 in learn = %d 214\n",
      "number of examples of class +1 in test = %d 11\n"
     ]
    }
   ],
   "source": [
    "learn, test = data_normalized.randomSplit([0.95, 0.05])\n",
    "\n",
    "N_learn = learn.count()\n",
    "N_test = test.count()\n",
    "\n",
    "nb_pos_samples_learn = learn.filter(lambda x: x.label == 1).count()\n",
    "nb_pos_samples_test = test.filter(lambda x: x.label == 1).count()\n",
    "\n",
    "print('N_learn = %d, N_test = %d' %(N_learn, N_test))\n",
    "print('number of examples of class +1 in learn = %d', nb_pos_samples_learn)\n",
    "print('number of examples of class +1 in test = %d', nb_pos_samples_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Minimization of the logistic loss with the Gradient algorithm\n",
    "\n",
    "The goal of this section is to: \n",
    "1. Compute gradients of the loss functions.\n",
    "2. Implement a Gradient algorithm.\n",
    "3. Observe the prediction accuracy of the developed methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 4__\n",
    ">Define a routine computing functional loss and gradient from one example \n",
    "\n",
    "For a Labeled point <tt>example</tt> (`LabeledPoint(example.label,example.features)`) that we denoted $(b_i,a_i)$ and a regressor <tt>x</tt>, compute $f_i(x) = \\log(1+\\exp(-b_i \\langle a_i,x\\rangle) )$ and $\\nabla f_i(x)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_loss_per_example(example,x):\n",
    "    \"\"\" Computes the logistic loss for a Labeled point\n",
    "    Args:\n",
    "        example: a labeled point\n",
    "        x: regressor\n",
    "    Returns:\n",
    "        real value: l \n",
    "    \"\"\"\n",
    "    res = np.log(1 + np.exp(- example.label * np.dot(example.features, x)))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_grad_per_example(example,x):\n",
    "    \"\"\" Computes the logistic gradient for a Labeled point\n",
    "    Args:\n",
    "        example: a labeled point\n",
    "        x: regressor\n",
    "    Returns:\n",
    "        numpy array: g\n",
    "    \"\"\"\n",
    "    denom = (1 + np.exp(example.label * np.dot(example.features, x)))\n",
    "    res = - example.label * example.features.toArray() / denom\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 5__\n",
    ">Implement a gradient descent algorithm to minimize\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } f(x) := \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) = \\frac{1}{m}  \\sum_{i=1}^m f_i(x).\n",
    "\\end{align*}\n",
    ">by \n",
    "* defining a function taking a stepsize and a maximal number of iterations and returning the final point as well as the value of $f(x)$ at each iteration. \n",
    "* running `x, f_tab = grad_algo(gamma,MAX_ITE)`\n",
    "\n",
    "\n",
    "For the choice of the stepsize, we help you by provinding you an upper bound on the Lipschitz constant $L$ of $\\nabla f$:\n",
    "\n",
    "$ L \\leq L_b = \\max_i 0.25 \\|a_i\\|_2^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad_algo(trainRDD, gamma, max_iter):\n",
    "    print('start grad_algo with gamma = %f, max_iter = %d' % (gamma, max_iter))\n",
    "    f_tab = [1.]\n",
    "    N = trainRDD.count()\n",
    "    x = np.zeros(len(trainRDD.first().features)) # init values = 0\n",
    "    for i in range(max_iter): # maybe change to convergence criterion\n",
    "        sg = trainRDD.map(lambda ex: logistic_grad_per_example(ex, x)).reduce(add) / N\n",
    "        x -= gamma * sg\n",
    "        ll = trainRDD.map(lambda ex: logistic_loss_per_example(ex, x)).reduce(add) / N\n",
    "        f_tab.append(ll)\n",
    "        print('[iter %d] f(x) = %f' %(i, ll))\n",
    "    print('done')\n",
    "    return x, f_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start grad_algo with gamma = 0.691580, max_iter = 100\n",
      "[iter 0] f(x) = 0.504062\n",
      "[iter 1] f(x) = 0.432227\n",
      "[iter 2] f(x) = 0.394354\n",
      "[iter 3] f(x) = 0.369547\n",
      "[iter 4] f(x) = 0.351557\n",
      "[iter 5] f(x) = 0.337743\n",
      "[iter 6] f(x) = 0.326711\n",
      "[iter 7] f(x) = 0.317645\n",
      "[iter 8] f(x) = 0.310024\n",
      "[iter 9] f(x) = 0.303505\n",
      "[iter 10] f(x) = 0.297844\n",
      "[iter 11] f(x) = 0.292868\n",
      "[iter 12] f(x) = 0.288447\n",
      "[iter 13] f(x) = 0.284483\n",
      "[iter 14] f(x) = 0.280900\n",
      "[iter 15] f(x) = 0.277639\n",
      "[iter 16] f(x) = 0.274652\n",
      "[iter 17] f(x) = 0.271900\n",
      "[iter 18] f(x) = 0.269352\n",
      "[iter 19] f(x) = 0.266982\n",
      "[iter 20] f(x) = 0.264768\n",
      "[iter 21] f(x) = 0.262693\n",
      "[iter 22] f(x) = 0.260742\n",
      "[iter 23] f(x) = 0.258900\n",
      "[iter 24] f(x) = 0.257158\n",
      "[iter 25] f(x) = 0.255505\n",
      "[iter 26] f(x) = 0.253934\n",
      "[iter 27] f(x) = 0.252437\n",
      "[iter 28] f(x) = 0.251008\n",
      "[iter 29] f(x) = 0.249640\n",
      "[iter 30] f(x) = 0.248331\n",
      "[iter 31] f(x) = 0.247074\n",
      "[iter 32] f(x) = 0.245866\n",
      "[iter 33] f(x) = 0.244704\n",
      "[iter 34] f(x) = 0.243585\n",
      "[iter 35] f(x) = 0.242505\n",
      "[iter 36] f(x) = 0.241463\n",
      "[iter 37] f(x) = 0.240455\n",
      "[iter 38] f(x) = 0.239481\n",
      "[iter 39] f(x) = 0.238537\n",
      "[iter 40] f(x) = 0.237623\n",
      "[iter 41] f(x) = 0.236736\n",
      "[iter 42] f(x) = 0.235876\n",
      "[iter 43] f(x) = 0.235040\n",
      "[iter 44] f(x) = 0.234227\n",
      "[iter 45] f(x) = 0.233438\n",
      "[iter 46] f(x) = 0.232669\n",
      "[iter 47] f(x) = 0.231921\n",
      "[iter 48] f(x) = 0.231192\n",
      "[iter 49] f(x) = 0.230481\n",
      "[iter 50] f(x) = 0.229789\n",
      "[iter 51] f(x) = 0.229113\n",
      "[iter 52] f(x) = 0.228453\n",
      "[iter 53] f(x) = 0.227809\n",
      "[iter 54] f(x) = 0.227180\n",
      "[iter 55] f(x) = 0.226566\n",
      "[iter 56] f(x) = 0.225965\n",
      "[iter 57] f(x) = 0.225377\n",
      "[iter 58] f(x) = 0.224802\n",
      "[iter 59] f(x) = 0.224239\n",
      "[iter 60] f(x) = 0.223689\n",
      "[iter 61] f(x) = 0.223149\n",
      "[iter 62] f(x) = 0.222621\n",
      "[iter 63] f(x) = 0.222103\n",
      "[iter 64] f(x) = 0.221596\n",
      "[iter 65] f(x) = 0.221098\n",
      "[iter 66] f(x) = 0.220610\n",
      "[iter 67] f(x) = 0.220131\n",
      "[iter 68] f(x) = 0.219662\n",
      "[iter 69] f(x) = 0.219201\n",
      "[iter 70] f(x) = 0.218748\n",
      "[iter 71] f(x) = 0.218304\n",
      "[iter 72] f(x) = 0.217867\n",
      "[iter 73] f(x) = 0.217438\n",
      "[iter 74] f(x) = 0.217017\n",
      "[iter 75] f(x) = 0.216603\n",
      "[iter 76] f(x) = 0.216196\n",
      "[iter 77] f(x) = 0.215796\n",
      "[iter 78] f(x) = 0.215403\n",
      "[iter 79] f(x) = 0.215015\n",
      "[iter 80] f(x) = 0.214635\n",
      "[iter 81] f(x) = 0.214260\n",
      "[iter 82] f(x) = 0.213892\n",
      "[iter 83] f(x) = 0.213529\n",
      "[iter 84] f(x) = 0.213172\n",
      "[iter 85] f(x) = 0.212820\n",
      "[iter 86] f(x) = 0.212474\n",
      "[iter 87] f(x) = 0.212133\n",
      "[iter 88] f(x) = 0.211797\n",
      "[iter 89] f(x) = 0.211466\n",
      "[iter 90] f(x) = 0.211140\n",
      "[iter 91] f(x) = 0.210818\n",
      "[iter 92] f(x) = 0.210501\n",
      "[iter 93] f(x) = 0.210189\n",
      "[iter 94] f(x) = 0.209881\n",
      "[iter 95] f(x) = 0.209578\n",
      "[iter 96] f(x) = 0.209279\n",
      "[iter 97] f(x) = 0.208984\n",
      "[iter 98] f(x) = 0.208692\n",
      "[iter 99] f(x) = 0.208405\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "max_example_norm = learn.map(lambda x: np.sqrt(x.features.dot(x.features))).reduce(lambda x,y: x if x > y else y)\n",
    "L_b = 0.25 * max_example_norm # we take the upperbound\n",
    "gamma = 2. / L_b # works better with e.g. 8. / L_b\n",
    "max_iter = 100 # first guess\n",
    "\n",
    "(x_opt, f_tab) = grad_algo(learn, gamma, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 6__\n",
    "\n",
    "> Plot the functional value versus the iterations.\n",
    "\n",
    "> Investigate if the computations are distributed over different threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAF5CAYAAADQ2iM1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWz0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d328ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(max_iter+1), f_tab, color=\"black\", linewidth=1.0, linestyle=\"-\")\n",
    "plt.xlim(0, max_iter+1)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Functional value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized logisitic regression\n",
    "\n",
    "In addition to the loss, it is usual to add a regularization term of the form\n",
    "$$ r(x) = \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2 $$\n",
    "\n",
    "The first part promotes sparsity of the iterates while the second part prevents over-fitting. \n",
    "This kind of regularization is often called:\n",
    "- *elastic-net* when $ \\lambda_1$ and $ \\lambda_2$ are non-null\n",
    "- $\\ell_1$ when $\\lambda_2 = 0$\n",
    "- *Tikhonov* when $\\lambda_1 = 0$\n",
    "\n",
    "The full optimization problems now writes\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } g(x) =  \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) +  \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "__Question 7__\n",
    "\n",
    "> Which part of $g$ is smooth, which part is not? Write $g$ as \n",
    "$$ g(x) =  \\frac{1}{m}  \\sum_{i=1}^m s_i(x) + n(x)  $$\n",
    "where the $(s_i)$ are smooth function and $n$ is non smooth. \n",
    "\n",
    "> Define a function `regularized_logistic_grad_per_example(examples,x)` returning the gradient of the smooth part per example (i.e. $\\nabla s_i(x)$)\n",
    "\n",
    "> Define a function `n_prox(x,gamma)` returning the proximal operator of the non-smooth part (i.e. $\\mathbf{prox}_{\\gamma n}(y)$)\n",
    "\n",
    "we recall that\n",
    "$$ \\mathbf{prox}_{\\gamma n}(y) = \\arg\\min_x\\left\\{ n(x) + \\frac{1}{2\\gamma} \\|x-y\\|_2^2 \\right\\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 8__\n",
    "\n",
    "> Compute a proximal gradient algorithm for computing a solution of\n",
    "$$ \\min_x  f(x) + r(x) = \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) + \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2 $$\n",
    "\n",
    "\n",
    "Hint: An admissible stepsize can be found by taking $\\gamma = 1/L_{b2}$ with  $ L_b = \\max_i 0.25 \\|a_i\\|_2^2 + 2\\lambda_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 9__\n",
    "\n",
    "> Examine the behavior and output of your proximal gradient algorithm with different values of $\\lambda_1$, $\\lambda_2$. What do you observe in terms of sparsity of the solution and convergence rate of the algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Question 10__\n",
    "\n",
    "> Write a function that evaluates the accuracy of the classification on the training dataset.\n",
    "\n",
    "> Investigate how this accuracy change when playing with the regularization terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To go further\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerations\n",
    "\n",
    "A popular acceleration method to improve the convergence rate of proximal gradient algorithm is the addition of inertia. That is, contructing the next gradient input by a combination of the last two outputs.\n",
    "\n",
    "\n",
    "In particular, Nesterov's acceleration is the most popular form of inertia. It writes\n",
    "$$ \\left\\{ \\begin{array}{l}   y_{k+1} = \\mathbf{prox\\_grad}(x_k) \\\\ x_{k+1} = y_{k+1} + \\alpha_{k+1} (y_{k+1} - y_k)  \\end{array} \\right. $$ \n",
    "with\n",
    "* $\\mathbf{prox\\_grad}$ the proximal gradient operation\n",
    "* $(\\alpha_{k})$ the inertial sequence defined as $\\alpha_k = \\frac{t_k-1}{t_{k+1}}$ and $t_0 = 0$ and $t_{k+1} = \\frac{1+\\sqrt{1+4t_k^2}}{2}$\n",
    "\n",
    "__Question 11__\n",
    "\n",
    "> Implement a fast proximal gradient with this kind of inertia (This algorithm is often nicknamed FISTA).\n",
    "\n",
    "> Compare the convergence speed with the vanilla proximal gradient algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental methods\n",
    "\n",
    "When dimension increases, incremental algorithms are often priviledged. \n",
    "\n",
    "A possible incremental algorithm for a problem such as regularized logistic regression is MISO (see *J Mairal. Incremental Majorization-Minimization Optimization with Application to Large-Scale Machine Learning. SIAM Journal on Optimization,2015 and ICML 2014.*):\n",
    "\n",
    "* Draw randomly a sample $n$\n",
    "* Compute $x^n_{k+1} = \\mathbf{prox}_{\\gamma g} (\\bar{x}_k) - \\gamma \\nabla f_n(\\mathbf{prox}_{\\gamma g} (\\bar{x}_k) )$\n",
    "* For all $i\\neq n$, $x^i_{k+1}=x^i_k$ \n",
    "* Compute new $\\bar{x}_{k+1} = \\frac{1}{m} \\sum_{j=1}^m x^j_{k+1}$\n",
    " \n",
    "\n",
    "__Question 12__\n",
    "\n",
    "> Implement this incremental algorithm and compare with the previous algorithms in terms of convergence time and functional value versus number of passes over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
