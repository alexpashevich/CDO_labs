{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was created by Franck Iutzeler, Jerome Malick and Yann Vernaz (2016).</i></small>\n",
    "<!-- Credit (images) Jeffrey Keating Thompson. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)</h3></center>\n",
    "<hr>\n",
    "<center><h1>Convex and distributed optimization</h1></center>\n",
    "<center><h2>Part II - Classification (3h + 3h home work)</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "In this Lab, we will investigate some gradient-based and proximal algorithms on the binary classification problems with logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classification and Logistic Regression\n",
    "\n",
    "### Machine Learning as an Optimization problem\n",
    "\n",
    "We have some *data*  $\\mathcal{D}$ consisting of $m$ *examples* $\\{d_i\\}$; each example consisting of a *feature* vector $a_i\\in\\mathbb{R}^d$ and an *observation* $b_i\\in \\mathcal{O}$: $\\mathcal{D} = \\{[a_i,b_i]\\}_{i=1..m}$ .\n",
    "\n",
    "\n",
    "The goal of *supervised learning* is to construct a predictor for the observations when given feature vectors.\n",
    "\n",
    "\n",
    "A popular approach is based on *linear models* which are based on finding a *parameter* $x$ such that the real number $\\langle a_i , x \\rangle$ is used to predict the value of the observation through a *predictor function* $g:\\mathbb{R}\\to \\mathcal{O}$: $g(\\langle a_i , x \\rangle)$ is the predicted value from $a_i$.\n",
    "\n",
    "\n",
    "In order to find such a parameter, we use the available data and a *loss* $\\ell$ that penalizes the error made between the predicted $g(\\langle a_i , x \\rangle)$ and observed $b_i$ values. For each example $i$, the corresponding error function for a parameter $x$ is $f_i(x) =   \\ell( g(\\langle a_i , x \\rangle) ; b_i )$. Using the whole data, the parameter that minimizes the total error is the solution of the minimization problem\n",
    "$$ \\min_{x\\in\\mathbb{R}^d} \\frac{1}{m} \\sum_{i=1}^m f_i(x) = \\frac{1}{m} \\sum_{i=1}^m  \\ell( g(\\langle a_i , x \\rangle) ; b_i ). $$\n",
    "\n",
    "\n",
    "### Binary Classification with Logisitic Regression\n",
    "\n",
    "In our setup, the observations are binary: $\\mathcal{O} = \\{-1 , +1 \\}$, and the *Logistic loss* is used to form the following optimization problem\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } f(x) := \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ).\n",
    "\\end{align*}\n",
    "\n",
    "Under some statistical hypotheses, $x^\\star = \\arg\\min f(x)$ maximizes the likelihood of the labels knowing the features vector. Then, for a new point $d$ with features vector $a$, \n",
    "$$ p_1(a) = \\mathbb{P}[d\\in \\text{ class }  +1] = \\frac{1}{1+\\exp(-\\langle a;x^\\star \\rangle)} $$\n",
    "Thus, from $a$, if $p_1(a)$ is close to $1$, one can decide that $d$ belongs to class $1$; and the opposite decision if $p(a)$ is close to $0$. Between the two, the appreciation is left to the data scientist depending on the application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised classification datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset\n",
    "\n",
    "We will use LibSVM formatted data, meaning that each line of the file (i.e. each example) will have the form\n",
    "\n",
    "<tt>class feature_number1:feature_value1 feature_number2:feature_value2 ... feature_number$n_i$:feature_value$n_i$ </tt>\n",
    "\n",
    "You may read such a file using MLUtils's <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.util.MLUtils.loadLibSVMFile\">`loadLibSVMFile`</a> routine on the supervised classification datasets below.\n",
    "\n",
    "The elements of the produced RDD have the form of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint\">`LabeledPoints`</a> composed of a label `example.label` corresponding to the class (+1 or -1) and a feature vector `example.features` generally encoded as a <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector\">`SparseVector`</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up spark environment (using Spark local mode set to # cores on your machine)\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\")\n",
    "conf.setAppName(\"MSIAM part II - Logistic Regression\")\n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remind you that you can access this interface (Spark UI) by simply opening http://localhost:4040 in a web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to LibSVM Datasets\n",
    "#LibSVMHomeDir=\"data/LibSVM/\"\n",
    "LibName=\"ionosphere.txt\"             # a small dataset to begin with\n",
    "#LibName=\"rcv1_train.binary\"          # a bigger one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 1__\n",
    "> Form an RDD from the selected dataset.\n",
    "\n",
    "> Count the number of examples, features, the number of examples of class '+1' and the density of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples (N) = 351\n",
      "number of features (D) = 34\n",
      "number of examples of class +1 = 225\n",
      "number of examples of class -1 = 126\n",
      "density = 0.884113\n",
      "10551\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "data = MLUtils.loadLibSVMFile(sc, LibName).setName(\"LibSVM\")\n",
    "#data = sc.textFile(\"ionosphere.txt\")#.setName(\"LibSVM\")\n",
    "N = data.count() # number of examples\n",
    "D = len(data.first().features) # number of features\n",
    "nb_pos_samples = data.filter(lambda x: x.label == 1).count()\n",
    "nb_neg_samples = data.filter(lambda x: x.label == -1).count()\n",
    "nb_nonzero_vals = data.map(lambda x: x.features.numNonzeros()).reduce(lambda x, y: x + y)\n",
    "density = 1. * nb_nonzero_vals / (N * D)\n",
    "\n",
    "print(\"number of examples (N) = %d\" % N)\n",
    "print(\"number of features (D) = %d\" % D)\n",
    "print(\"number of examples of class +1 = %d\" % nb_pos_samples)\n",
    "print(\"number of examples of class -1 = %d\" % nb_neg_samples)\n",
    "print(\"density = %f\" % density)\n",
    "print(nb_nonzero_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "An important first step for learning by regression is to preprocess the dataset. This processing usually consists in:\n",
    "* Adding an intercept, that is an additional feature equal to one for all examples (statistically, this accounts for the fact that the two classes may be imbalanced).\n",
    "* For the dense datasets:\n",
    "    *  normalize to have zero-mean and unit variance for every feature (except the interecept for instance.\n",
    "* For sparse datasets:\n",
    "    * normalize so that the feature vector has unit $\\ell_2$ norm for each example.\n",
    "\n",
    "This does not really change the problem but it will ease the convergence of the applied optimization algorithms.\n",
    "\n",
    "__Question 2__\n",
    "> Form a new RDD with the scaled version of the dataset.\n",
    "\n",
    "> Check that the number of examples, features, and the density is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from operator import add\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# as variance can be zero for some features, we will remove those column and insert them back\n",
    "def normalize_sample(x):\n",
    "    new_features = (x.features.toArray() - means) / np.sqrt(variance)\n",
    "    new_features = np.append(new_features, 1)\n",
    "    features_sparse_vector = SparseVector(np.shape(means)[0] + 1,\n",
    "                                          np.nonzero(new_features)[0],\n",
    "                                          new_features[np.nonzero(new_features)])\n",
    "    return LabeledPoint(x.label, features_sparse_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.4\n",
      "(1.0,(34,[0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33],[1.0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1.0,0.0376,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.3409,0.42267,-0.54487,0.18641,-0.453]))\n",
      "(34,[0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33],[1.0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1.0,0.0376,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.3409,0.42267,-0.54487,0.18641,-0.453])\n",
      "[ 1.       0.       0.99539 -0.05889  0.85243  0.02306  0.83398 -0.37708\n",
      "  1.       0.0376   0.85243 -0.17755  0.59755 -0.44945  0.60536 -0.38223\n",
      "  0.84356 -0.38542  0.58212 -0.32192  0.56971 -0.29674  0.36946 -0.47357\n",
      "  0.56811 -0.51171  0.41078 -0.46168  0.21266 -0.3409   0.42267 -0.54487\n",
      "  0.18641 -0.453  ]\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "\n",
    "first = data.first()\n",
    "print(first)\n",
    "\n",
    "features = first.features\n",
    "print(features)\n",
    "\n",
    "np_features = features.toArray()\n",
    "print(np_features)\n",
    "# data_mapped = data.map(lambda x: x.features)\n",
    "# first_mapped = data_mapped.first()\n",
    "# print(first_mapped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means:\n",
      "[ 0.78347578  0.          0.64134185  0.04437188  0.60106789  0.115889\n",
      "  0.55009507  0.11936037  0.51184809  0.18134538  0.47618265  0.15504046\n",
      "  0.4008012   0.09341368  0.34415915  0.07113234  0.381949   -0.00361681\n",
      "  0.3593896  -0.0240247   0.33669547  0.0082959   0.3624755  -0.05740575\n",
      "  0.39613467 -0.07118687  0.5416408  -0.06953761  0.37844519 -0.02790709\n",
      "  0.35251373 -0.00379376  0.34936365  0.01448011]\n",
      "variance:\n",
      "[ 0.3861657   0.          0.24700772  0.19430949  0.26948603  0.211741\n",
      "  0.24201626  0.27040786  0.25638293  0.2334447   0.31662351  0.24414675\n",
      "  0.38601268  0.24420121  0.42496998  0.20950509  0.38086098  0.24606941\n",
      "  0.39109271  0.26867235  0.37083107  0.26773094  0.36349662  0.27741755\n",
      "  0.33365214  0.25783     0.26570809  0.30166587  0.33069932  0.25730253\n",
      "  0.32566278  0.26300717  0.27239872  0.21871485]\n",
      "(1.0,(35,[0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34],[0.348433280269,0.712372367309,-0.234257237563,0.484207686793,-0.201734977754,0.577058790699,-0.954679144632,0.964074158868,-0.297510229259,0.668654651086,-0.673107318134,0.316673202835,-1.0985425274,0.400677973145,-0.990485565808,0.747985026589,-0.769680949556,0.356155483134,-0.574714507232,0.38264403556,-0.589524243017,0.0115847114771,-0.790128635368,0.29772766393,-0.867564946095,-0.253867539692,-0.713971226166,-0.288289660581,-0.617038783086,0.122936944467,-1.05505394246,-0.312220599512,-0.999594828772,1.0]))\n",
      "means in the normalized data:\n",
      "[  1.56886217e-16   0.00000000e+00  -7.08518397e-17  -6.07301484e-17\n",
      "   5.06084570e-18  -6.07301484e-17   2.83407359e-16   1.41703679e-16\n",
      "  -3.34015816e-16  -4.04867656e-17   1.06277760e-16  -1.21460297e-16\n",
      "   1.01216914e-17   5.56693027e-17  -1.67007908e-16   1.31581988e-16\n",
      "  -2.83407359e-16  -3.03650742e-17  -2.63163976e-16  -5.06084570e-17\n",
      "  -1.77129599e-16   2.53042285e-17  -2.43236896e-16  -5.06084570e-18\n",
      "   3.98541599e-17  -1.32847200e-17   0.00000000e+00   1.01216914e-17\n",
      "   2.53042285e-17   3.92215541e-17   2.02433828e-16  -3.54259199e-17\n",
      "   3.54259199e-17  -7.59126854e-17   1.00000000e+00]\n",
      "variance in the normalized data:\n",
      "[ 1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.]\n",
      "number of examples (N) in the normalized data = 351\n",
      "new number of features (D) in the normalized data = 35\n",
      "density in the normalized data = 0.971429\n"
     ]
    }
   ],
   "source": [
    "means = data.map(lambda x: x.features.toArray()).reduce(add) / N\n",
    "print('means:')\n",
    "print(means)\n",
    "\n",
    "variance = data.map(lambda x: (x.features.toArray() - means) ** 2).reduce(add) / N\n",
    "print('variance:')\n",
    "print(variance)\n",
    "\n",
    "# as we can not divide by zero, we fix the values of variance where it is 0 (the second column which is empty)\n",
    "variance[np.argwhere(variance == 0)] = 1\n",
    "\n",
    "data_normalized = data.map(normalize_sample)\n",
    "\n",
    "print(data_normalized.first())\n",
    "      \n",
    "new_means = data_normalized.map(lambda x: x.features.toArray()).reduce(add) / N\n",
    "print('means in the normalized data:')\n",
    "print(new_means)\n",
    "\n",
    "new_variance = data_normalized.map(lambda x: (x.features.toArray() - new_means) ** 2).reduce(add) / N\n",
    "print('variance in the normalized data:')\n",
    "print(new_variance)\n",
    "\n",
    "\n",
    "new_N = data_normalized.count() # number of examples\n",
    "new_D = len(data_normalized.first().features.toArray()) # number of features\n",
    "new_nb_nonzero_vals = data_normalized.map(lambda x: x.features.numNonzeros()).reduce(add)\n",
    "new_density = 1. * new_nb_nonzero_vals / (new_N * new_D)\n",
    "\n",
    "print(\"number of examples (N) in the normalized data = %d\" % new_N)\n",
    "print(\"new number of features (D) in the normalized data = %d\" % new_D)\n",
    "print(\"density in the normalized data = %f\" % new_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Initialization\n",
    "\n",
    "We will set up here the variables, and the training versus testing dataset. Indeed, we will take a portion of the dataset to learn called the `learning set`, say $95$%, and we will test our predictions on the rest, the `testing set`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 3__\n",
    "\n",
    ">  Split the scaled dataset into a training and a testing set. For instance, you may use the function <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit\">`randomSplit`</a>.\n",
    "\n",
    "> Count the number of examples, and subjects in class '+1' in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_learn = 332, N_test = 19\n",
      "number of examples of class +1 in learn = %d 211\n",
      "number of examples of class +1 in test = %d 14\n"
     ]
    }
   ],
   "source": [
    "learn, test = data_normalized.randomSplit([0.95, 0.05])\n",
    "\n",
    "N_learn = learn.count()\n",
    "N_test = test.count()\n",
    "\n",
    "nb_pos_samples_learn = learn.filter(lambda x: x.label == 1).count()\n",
    "nb_pos_samples_test = test.filter(lambda x: x.label == 1).count()\n",
    "\n",
    "print('N_learn = %d, N_test = %d' %(N_learn, N_test))\n",
    "print('number of examples of class +1 in learn = %d', nb_pos_samples_learn)\n",
    "print('number of examples of class +1 in test = %d', nb_pos_samples_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Minimization of the logistic loss with the Gradient algorithm\n",
    "\n",
    "The goal of this section is to: \n",
    "1. Compute gradients of the loss functions.\n",
    "2. Implement a Gradient algorithm.\n",
    "3. Observe the prediction accuracy of the developed methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 4__\n",
    ">Define a routine computing functional loss and gradient from one example \n",
    "\n",
    "For a Labeled point <tt>example</tt> (`LabeledPoint(example.label,example.features)`) that we denoted $(b_i,a_i)$ and a regressor <tt>x</tt>, compute $f_i(x) = \\log(1+\\exp(-b_i \\langle a_i,x\\rangle) )$ and $\\nabla f_i(x)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_loss_per_example(example,x):\n",
    "    \"\"\" Computes the logistic loss for a Labeled point\n",
    "    Args:\n",
    "        example: a labeled point\n",
    "        x: regressor\n",
    "    Returns:\n",
    "        real value: l \n",
    "    \"\"\"\n",
    "    res = np.log(1 + np.exp(- example.label * np.dot(example.features.toArray(), x)))\n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_grad_per_example(example,x):\n",
    "    \"\"\" Computes the logistic gradient for a Labeled point\n",
    "    Args:\n",
    "        example: a labeled point\n",
    "        x: regressor\n",
    "    Returns:\n",
    "        numpy array: g \n",
    "    \"\"\"\n",
    "    denom = (1 + np.exp(example.label * np.dot(example.features.toArray(), x)))\n",
    "    res = - example.label * example.features.toArray() / denom\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 5__\n",
    ">Implement a gradient descent algorithm to minimize\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } f(x) := \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) = \\frac{1}{m}  \\sum_{i=1}^m f_i(x).\n",
    "\\end{align*}\n",
    ">by \n",
    "* defining a function taking a stepsize and a maximal number of iterations and returning the final point as well as the value of $f(x)$ at each iteration. \n",
    "* running `x, f_tab = grad_algo(gamma,MAX_ITE)`\n",
    "\n",
    "\n",
    "For the choice of the stepsize, we help you by provinding you an upper bound on the Lipschitz constant $L$ of $\\nabla f$:\n",
    "\n",
    "$ L \\leq L_b = \\max_i 0.25 \\|a_i\\|_2^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad_algo(trainRDD, gamma, max_iter):\n",
    "    print('start grad_algo with gamma = %f, max_iter = %d' % (gamma, max_iter))\n",
    "    f_tab = [1.]\n",
    "    N = trainRDD.count()\n",
    "    x = np.zeros(len(trainRDD.first().features.toArray())) # init values = 0\n",
    "    for i in range(max_iter): # maybe change to convergence criterion\n",
    "        sg = trainRDD.map(lambda ex: logistic_grad_per_example(ex, x)).reduce(add) / N\n",
    "        x -= gamma * sg\n",
    "        ll = trainRDD.map(lambda ex: logistic_loss_per_example(ex, x)).reduce(add) / N\n",
    "        f_tab.append(ll)\n",
    "        print('[iter %d] f(x) = %f' %(i, ll))\n",
    "    print('done')\n",
    "    return x, f_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start grad_algo with gamma = 0.691580, max_iter = 100\n",
      "[iter 0] f(x) = 0.502956\n",
      "[iter 1] f(x) = 0.430383\n",
      "[iter 2] f(x) = 0.392265\n",
      "[iter 3] f(x) = 0.367245\n",
      "[iter 4] f(x) = 0.349044\n",
      "[iter 5] f(x) = 0.335030\n",
      "[iter 6] f(x) = 0.323811\n",
      "[iter 7] f(x) = 0.314570\n",
      "[iter 8] f(x) = 0.306788\n",
      "[iter 9] f(x) = 0.300116\n",
      "[iter 10] f(x) = 0.294313\n",
      "[iter 11] f(x) = 0.289202\n",
      "[iter 12] f(x) = 0.284654\n",
      "[iter 13] f(x) = 0.280569\n",
      "[iter 14] f(x) = 0.276870\n",
      "[iter 15] f(x) = 0.273498\n",
      "[iter 16] f(x) = 0.270405\n",
      "[iter 17] f(x) = 0.267550\n",
      "[iter 18] f(x) = 0.264904\n",
      "[iter 19] f(x) = 0.262438\n",
      "[iter 20] f(x) = 0.260133\n",
      "[iter 21] f(x) = 0.257968\n",
      "[iter 22] f(x) = 0.255930\n",
      "[iter 23] f(x) = 0.254005\n",
      "[iter 24] f(x) = 0.252181\n",
      "[iter 25] f(x) = 0.250449\n",
      "[iter 26] f(x) = 0.248800\n",
      "[iter 27] f(x) = 0.247228\n",
      "[iter 28] f(x) = 0.245725\n",
      "[iter 29] f(x) = 0.244286\n",
      "[iter 30] f(x) = 0.242907\n",
      "[iter 31] f(x) = 0.241582\n",
      "[iter 32] f(x) = 0.240308\n",
      "[iter 33] f(x) = 0.239080\n",
      "[iter 34] f(x) = 0.237897\n",
      "[iter 35] f(x) = 0.236755\n",
      "[iter 36] f(x) = 0.235652\n",
      "[iter 37] f(x) = 0.234585\n",
      "[iter 38] f(x) = 0.233551\n",
      "[iter 39] f(x) = 0.232550\n",
      "[iter 40] f(x) = 0.231579\n",
      "[iter 41] f(x) = 0.230637\n",
      "[iter 42] f(x) = 0.229722\n",
      "[iter 43] f(x) = 0.228833\n",
      "[iter 44] f(x) = 0.227968\n",
      "[iter 45] f(x) = 0.227127\n",
      "[iter 46] f(x) = 0.226308\n",
      "[iter 47] f(x) = 0.225509\n",
      "[iter 48] f(x) = 0.224731\n",
      "[iter 49] f(x) = 0.223973\n",
      "[iter 50] f(x) = 0.223233\n",
      "[iter 51] f(x) = 0.222510\n",
      "[iter 52] f(x) = 0.221804\n",
      "[iter 53] f(x) = 0.221115\n",
      "[iter 54] f(x) = 0.220441\n",
      "[iter 55] f(x) = 0.219782\n",
      "[iter 56] f(x) = 0.219138\n",
      "[iter 57] f(x) = 0.218507\n",
      "[iter 58] f(x) = 0.217890\n",
      "[iter 59] f(x) = 0.217286\n",
      "[iter 60] f(x) = 0.216694\n",
      "[iter 61] f(x) = 0.216114\n",
      "[iter 62] f(x) = 0.215546\n",
      "[iter 63] f(x) = 0.214988\n",
      "[iter 64] f(x) = 0.214442\n",
      "[iter 65] f(x) = 0.213906\n",
      "[iter 66] f(x) = 0.213380\n",
      "[iter 67] f(x) = 0.212863\n",
      "[iter 68] f(x) = 0.212357\n",
      "[iter 69] f(x) = 0.211859\n",
      "[iter 70] f(x) = 0.211370\n",
      "[iter 71] f(x) = 0.210890\n",
      "[iter 72] f(x) = 0.210418\n",
      "[iter 73] f(x) = 0.209955\n",
      "[iter 74] f(x) = 0.209499\n",
      "[iter 75] f(x) = 0.209051\n",
      "[iter 76] f(x) = 0.208610\n",
      "[iter 77] f(x) = 0.208177\n",
      "[iter 78] f(x) = 0.207750\n",
      "[iter 79] f(x) = 0.207331\n",
      "[iter 80] f(x) = 0.206918\n",
      "[iter 81] f(x) = 0.206511\n",
      "[iter 82] f(x) = 0.206111\n",
      "[iter 83] f(x) = 0.205717\n",
      "[iter 84] f(x) = 0.205329\n",
      "[iter 85] f(x) = 0.204947\n",
      "[iter 86] f(x) = 0.204571\n",
      "[iter 87] f(x) = 0.204200\n",
      "[iter 88] f(x) = 0.203834\n",
      "[iter 89] f(x) = 0.203474\n",
      "[iter 90] f(x) = 0.203119\n",
      "[iter 91] f(x) = 0.202769\n",
      "[iter 92] f(x) = 0.202424\n",
      "[iter 93] f(x) = 0.202084\n",
      "[iter 94] f(x) = 0.201749\n",
      "[iter 95] f(x) = 0.201418\n",
      "[iter 96] f(x) = 0.201092\n",
      "[iter 97] f(x) = 0.200770\n",
      "[iter 98] f(x) = 0.200452\n",
      "[iter 99] f(x) = 0.200139\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "max_example_norm = learn.map(lambda x: np.sqrt(x.features.dot(x.features))).reduce(lambda x,y: x if x > y else y)\n",
    "L_b = 0.25 * max_example_norm # we take the upperbound\n",
    "gamma = 2. / L_b # works better with e.g. 8. / L_b\n",
    "max_iter = 100 # first guess\n",
    "\n",
    "(x_opt, f_tab) = grad_algo(learn, gamma, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 6__\n",
    "\n",
    "> Plot the functional value versus the iterations.\n",
    "\n",
    "> Investigate if the computations are distributed over different threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAF5CAYAAADQ2iM1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fee8805e2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(max_iter+1), f_tab, color=\"black\", linewidth=1.0, linestyle=\"-\")\n",
    "plt.xlim(0, max_iter+1)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Functional value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized logisitic regression\n",
    "\n",
    "In addition to the loss, it is usual to add a regularization term of the form\n",
    "$$ r(x) = \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2 $$\n",
    "\n",
    "The first part promotes sparsity of the iterates while the second part prevents over-fitting. \n",
    "This kind of regularization is often called:\n",
    "- *elastic-net* when $ \\lambda_1$ and $ \\lambda_2$ are non-null\n",
    "- $\\ell_1$ when $\\lambda_2 = 0$\n",
    "- *Tikhonov* when $\\lambda_1 = 0$\n",
    "\n",
    "The full optimization problems now writes\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } g(x) =  \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) +  \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "__Question 7__\n",
    "\n",
    "> Which part of $g$ is smooth, which part is not? Write $g$ as \n",
    "$$ g(x) =  \\frac{1}{m}  \\sum_{i=1}^m s_i(x) + n(x)  $$\n",
    "where the $(s_i)$ are smooth function and $n$ is non smooth. \n",
    "\n",
    "> Define a function `regularized_logistic_grad_per_example(examples,x)` returning the gradient of the smooth part per example (i.e. $\\nabla s_i(x)$)\n",
    "\n",
    "> Define a function `n_prox(x,gamma)` returning the proximal operator of the non-smooth part (i.e. $\\mathbf{prox}_{\\gamma n}(y)$)\n",
    "\n",
    "we recall that\n",
    "$$ \\mathbf{prox}_{\\gamma n}(y) = \\arg\\min_x\\left\\{ n(x) + \\frac{1}{2\\gamma} \\|x-y\\|_2^2 \\right\\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def regularized_logistic_loss_per_example(example,x,lambda_1,lambda_2):\n",
    "    res = np.log(1 + np.exp(- example.label * np.dot(example.features.toArray(), x))) + lambda_2*np.dot(x,x) +lambda_1*sum(abs(x))\n",
    "    return res \n",
    "\n",
    "\n",
    "def regularized_logistic_grad_per_example(example, x, lambda_2):\n",
    "    denom = (1 + np.exp(example.label * np.dot(example.features.toArray(), x)))\n",
    "    res = - example.label * example.features.toArray() / denom  + 2 * lambda_2 * x\n",
    "    return res\n",
    "\n",
    "def n_prox(y,gamma,lambda_1):\n",
    "    x = y.copy()\n",
    "    for i in range(np.size(y)):\n",
    "        if y[i] > gamma*lambda_1: \n",
    "            x[i] = y[i] - gamma*lambda_1\n",
    "        elif y[i] < -gamma*lambda_1:\n",
    "            x[i] = y[i] + gamma*lambda_1\n",
    "        else:\n",
    "            x[i] = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 8__\n",
    "\n",
    "> Compute a proximal gradient algorithm for computing a solution of\n",
    "$$ \\min_x  f(x) + r(x) = \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) + \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2 $$\n",
    "\n",
    "\n",
    "Hint: An admissible stepsize can be found by taking $\\gamma = 1/L_{b2}$ with  $ L_b = \\max_i 0.25 \\|a_i\\|_2^2 + 2\\lambda_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def proximal_gradient_algorithm(trainRDD, gamma, max_iter,lambda_1,lambda_2):\n",
    "    print('start grad_algo with gamma = %f, max_iter = %d' % (gamma, max_iter))\n",
    "    f_tab = [1.]\n",
    "    N = trainRDD.count()\n",
    "    x = np.zeros(len(trainRDD.first().features.toArray())) # init values = 0\n",
    "    for i in range(max_iter): # maybe change to convergence criterion\n",
    "        sg = trainRDD.map(lambda ex: regularized_logistic_grad_per_example(ex, x,lambda_2)).reduce(add) / N\n",
    "        x -= gamma * sg\n",
    "        x = n_prox(x,gamma,lambda_1)\n",
    "        ll = trainRDD.map(lambda ex: regularized_logistic_loss_per_example(ex, x,lambda_1,lambda_2)).reduce(add) / N\n",
    "        f_tab.append(ll)\n",
    "        print('[iter %d] f(x) = %f' %(i, ll))\n",
    "    print('done')\n",
    "    return x, f_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start grad_algo with gamma = 0.343415, max_iter = 100\n",
      "[iter 0] f(x) = 0.591747\n",
      "[iter 1] f(x) = 0.540014\n",
      "[iter 2] f(x) = 0.506212\n",
      "[iter 3] f(x) = 0.482354\n",
      "[iter 4] f(x) = 0.464713\n",
      "[iter 5] f(x) = 0.451122\n",
      "[iter 6] f(x) = 0.440330\n",
      "[iter 7] f(x) = 0.431559\n",
      "[iter 8] f(x) = 0.424301\n",
      "[iter 9] f(x) = 0.418205\n",
      "[iter 10] f(x) = 0.413019\n",
      "[iter 11] f(x) = 0.408586\n",
      "[iter 12] f(x) = 0.404808\n",
      "[iter 13] f(x) = 0.401528\n",
      "[iter 14] f(x) = 0.398654\n",
      "[iter 15] f(x) = 0.396118\n",
      "[iter 16] f(x) = 0.393868\n",
      "[iter 17] f(x) = 0.391860\n",
      "[iter 18] f(x) = 0.390061\n",
      "[iter 19] f(x) = 0.388443\n",
      "[iter 20] f(x) = 0.386980\n",
      "[iter 21] f(x) = 0.385655\n",
      "[iter 22] f(x) = 0.384450\n",
      "[iter 23] f(x) = 0.383351\n",
      "[iter 24] f(x) = 0.382347\n",
      "[iter 25] f(x) = 0.381425\n",
      "[iter 26] f(x) = 0.380579\n",
      "[iter 27] f(x) = 0.379799\n",
      "[iter 28] f(x) = 0.379079\n",
      "[iter 29] f(x) = 0.378414\n",
      "[iter 30] f(x) = 0.377797\n",
      "[iter 31] f(x) = 0.377241\n",
      "[iter 32] f(x) = 0.376734\n",
      "[iter 33] f(x) = 0.376268\n",
      "[iter 34] f(x) = 0.375835\n",
      "[iter 35] f(x) = 0.375431\n",
      "[iter 36] f(x) = 0.375056\n",
      "[iter 37] f(x) = 0.374704\n",
      "[iter 38] f(x) = 0.374376\n",
      "[iter 39] f(x) = 0.374069\n",
      "[iter 40] f(x) = 0.373780\n",
      "[iter 41] f(x) = 0.373510\n",
      "[iter 42] f(x) = 0.373255\n",
      "[iter 43] f(x) = 0.373016\n",
      "[iter 44] f(x) = 0.372790\n",
      "[iter 45] f(x) = 0.372578\n",
      "[iter 46] f(x) = 0.372377\n",
      "[iter 47] f(x) = 0.372187\n",
      "[iter 48] f(x) = 0.372008\n",
      "[iter 49] f(x) = 0.371846\n",
      "[iter 50] f(x) = 0.371697\n",
      "[iter 51] f(x) = 0.371556\n",
      "[iter 52] f(x) = 0.371424\n",
      "[iter 53] f(x) = 0.371299\n",
      "[iter 54] f(x) = 0.371181\n",
      "[iter 55] f(x) = 0.371073\n",
      "[iter 56] f(x) = 0.370973\n",
      "[iter 57] f(x) = 0.370879\n",
      "[iter 58] f(x) = 0.370789\n",
      "[iter 59] f(x) = 0.370705\n",
      "[iter 60] f(x) = 0.370625\n",
      "[iter 61] f(x) = 0.370550\n",
      "[iter 62] f(x) = 0.370478\n",
      "[iter 63] f(x) = 0.370410\n",
      "[iter 64] f(x) = 0.370346\n",
      "[iter 65] f(x) = 0.370284\n",
      "[iter 66] f(x) = 0.370226\n",
      "[iter 67] f(x) = 0.370171\n",
      "[iter 68] f(x) = 0.370118\n",
      "[iter 69] f(x) = 0.370068\n",
      "[iter 70] f(x) = 0.370021\n",
      "[iter 71] f(x) = 0.369975\n",
      "[iter 72] f(x) = 0.369932\n",
      "[iter 73] f(x) = 0.369890\n",
      "[iter 74] f(x) = 0.369851\n",
      "[iter 75] f(x) = 0.369813\n",
      "[iter 76] f(x) = 0.369777\n",
      "[iter 77] f(x) = 0.369742\n",
      "[iter 78] f(x) = 0.369709\n",
      "[iter 79] f(x) = 0.369678\n",
      "[iter 80] f(x) = 0.369648\n",
      "[iter 81] f(x) = 0.369619\n",
      "[iter 82] f(x) = 0.369591\n",
      "[iter 83] f(x) = 0.369564\n",
      "[iter 84] f(x) = 0.369539\n",
      "[iter 85] f(x) = 0.369514\n",
      "[iter 86] f(x) = 0.369491\n",
      "[iter 87] f(x) = 0.369468\n",
      "[iter 88] f(x) = 0.369447\n",
      "[iter 89] f(x) = 0.369426\n",
      "[iter 90] f(x) = 0.369406\n",
      "[iter 91] f(x) = 0.369387\n",
      "[iter 92] f(x) = 0.369368\n",
      "[iter 93] f(x) = 0.369350\n",
      "[iter 94] f(x) = 0.369333\n",
      "[iter 95] f(x) = 0.369317\n",
      "[iter 96] f(x) = 0.369301\n",
      "[iter 97] f(x) = 0.369286\n",
      "[iter 98] f(x) = 0.369271\n",
      "[iter 99] f(x) = 0.369257\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "lambda_1 = 0.01\n",
    "lambda_2 = 0.01\n",
    "max_example_norm = learn.map(lambda x: np.sqrt(x.features.dot(x.features.toArray()))).reduce(lambda x,y: x if x > y else y)\n",
    "L_b = 0.25 * max_example_norm + 2*lambda_2 # we take the upperbound\n",
    "gamma = 1./ L_b # works better with e.g. 8. / L_b\n",
    "max_iter = 100 # first guess\n",
    "\n",
    "(x_opt, f_tab) =  proximal_gradient_algorithm(learn, gamma, max_iter,lambda_1,lambda_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 9__\n",
    "\n",
    "> Examine the behavior and output of your proximal gradient algorithm with different values of $\\lambda_1$, $\\lambda_2$. What do you observe in terms of sparsity of the solution and convergence rate of the algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start grad_algo with gamma = 0.408837, max_iter = 100\n",
      "[iter 0] f(x) = 0.628312\n",
      "[iter 1] f(x) = 0.624043\n",
      "[iter 2] f(x) = 0.622500\n",
      "[iter 3] f(x) = 0.621771\n",
      "[iter 4] f(x) = 0.621441\n",
      "[iter 5] f(x) = 0.621286\n",
      "[iter 6] f(x) = 0.621215\n",
      "[iter 7] f(x) = 0.621182\n",
      "[iter 8] f(x) = 0.621167\n",
      "[iter 9] f(x) = 0.621160\n",
      "[iter 10] f(x) = 0.621156\n",
      "[iter 11] f(x) = 0.621155\n",
      "[iter 12] f(x) = 0.621154\n",
      "[iter 13] f(x) = 0.621154\n",
      "[iter 14] f(x) = 0.621154\n",
      "[iter 15] f(x) = 0.621154\n",
      "[iter 16] f(x) = 0.621154\n",
      "[iter 17] f(x) = 0.621154\n",
      "[iter 18] f(x) = 0.621154\n",
      "[iter 19] f(x) = 0.621154\n",
      "[iter 20] f(x) = 0.621154\n",
      "[iter 21] f(x) = 0.621154\n",
      "[iter 22] f(x) = 0.621154\n",
      "[iter 23] f(x) = 0.621154\n",
      "[iter 24] f(x) = 0.621154\n",
      "[iter 25] f(x) = 0.621154\n",
      "[iter 26] f(x) = 0.621154\n",
      "[iter 27] f(x) = 0.621154\n",
      "[iter 28] f(x) = 0.621154\n",
      "[iter 29] f(x) = 0.621154\n",
      "[iter 30] f(x) = 0.621154\n",
      "[iter 31] f(x) = 0.621154\n",
      "[iter 32] f(x) = 0.621154\n",
      "[iter 33] f(x) = 0.621154\n",
      "[iter 34] f(x) = 0.621154\n",
      "[iter 35] f(x) = 0.621154\n",
      "[iter 36] f(x) = 0.621154\n",
      "[iter 37] f(x) = 0.621154\n",
      "[iter 38] f(x) = 0.621154\n",
      "[iter 39] f(x) = 0.621154\n",
      "[iter 40] f(x) = 0.621154\n",
      "[iter 41] f(x) = 0.621154\n",
      "[iter 42] f(x) = 0.621154\n",
      "[iter 43] f(x) = 0.621154\n",
      "[iter 44] f(x) = 0.621154\n",
      "[iter 45] f(x) = 0.621154\n",
      "[iter 46] f(x) = 0.621154\n",
      "[iter 47] f(x) = 0.621154\n",
      "[iter 48] f(x) = 0.621154\n",
      "[iter 49] f(x) = 0.621154\n",
      "[iter 50] f(x) = 0.621154\n",
      "[iter 51] f(x) = 0.621154\n",
      "[iter 52] f(x) = 0.621154\n",
      "[iter 53] f(x) = 0.621154\n",
      "[iter 54] f(x) = 0.621154\n",
      "[iter 55] f(x) = 0.621154\n",
      "[iter 56] f(x) = 0.621154\n",
      "[iter 57] f(x) = 0.621154\n",
      "[iter 58] f(x) = 0.621154\n",
      "[iter 59] f(x) = 0.621154\n",
      "[iter 60] f(x) = 0.621154\n",
      "[iter 61] f(x) = 0.621154\n",
      "[iter 62] f(x) = 0.621154\n",
      "[iter 63] f(x) = 0.621154\n",
      "[iter 64] f(x) = 0.621154\n",
      "[iter 65] f(x) = 0.621154\n",
      "[iter 66] f(x) = 0.621154\n",
      "[iter 67] f(x) = 0.621154\n",
      "[iter 68] f(x) = 0.621154\n",
      "[iter 69] f(x) = 0.621154\n",
      "[iter 70] f(x) = 0.621154\n",
      "[iter 71] f(x) = 0.621154\n",
      "[iter 72] f(x) = 0.621154\n",
      "[iter 73] f(x) = 0.621154\n",
      "[iter 74] f(x) = 0.621154\n",
      "[iter 75] f(x) = 0.621154\n",
      "[iter 76] f(x) = 0.621154\n",
      "[iter 77] f(x) = 0.621154\n",
      "[iter 78] f(x) = 0.621154\n",
      "[iter 79] f(x) = 0.621154\n",
      "[iter 80] f(x) = 0.621154\n",
      "[iter 81] f(x) = 0.621154\n",
      "[iter 82] f(x) = 0.621154\n",
      "[iter 83] f(x) = 0.621154\n",
      "[iter 84] f(x) = 0.621154\n",
      "[iter 85] f(x) = 0.621154\n",
      "[iter 86] f(x) = 0.621154\n",
      "[iter 87] f(x) = 0.621154\n",
      "[iter 88] f(x) = 0.621154\n",
      "[iter 89] f(x) = 0.621154\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b94199cf1093>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mL_b\u001b[0m \u001b[0;31m# works better with e.g. 8. / L_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;31m# first guess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mx_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_tab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproximal_gradient_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-d0d9423d31c6>\u001b[0m in \u001b[0;36mproximal_gradient_algorithm\u001b[0;34m(trainRDD, gamma, max_iter, lambda_1, lambda_2)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# init values = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# maybe change to convergence criterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mregularized_logistic_grad_per_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_prox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \"\"\"\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2402\u001b[0m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001b[0;32m-> 2403\u001b[0;31m                                       self._jrdd_deserializer, profiler)\n\u001b[0m\u001b[1;32m   2404\u001b[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n\u001b[1;32m   2405\u001b[0m                                              self.preservesPartitioning)\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   2334\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serializer should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2335\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2336\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2337\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[1;32m   2338\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2323\u001b[0m     broadcast_vars = ListConverter().convert(\n\u001b[1;32m   2324\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jbroadcast\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pickled_broadcast_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m         sc._gateway._gateway_client)\n\u001b[0m\u001b[1;32m   2326\u001b[0m     \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pickled_broadcast_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMapConverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_collections.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, object, gateway_client)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mArrayList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJavaClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"java.util.ArrayList\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mjava_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArrayList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mjava_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1397\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1400\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1401\u001b[0m             answer, self._gateway_client, None, self._fqn)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_example_norm = learn.map(lambda x: np.sqrt(x.features.dot(x.features))).reduce(lambda x,y: x if x > y else y)\n",
    "\n",
    "for i in range(10):\n",
    "    lambda_1 = i\n",
    "    lambda_2 = 1\n",
    "    L_b = 0.25 * max_example_norm + 2*lambda_2 # we take the upperbound\n",
    "    gamma = 2. / L_b # works better with e.g. 8. / L_b\n",
    "    max_iter = 100 # first guess\n",
    "    (x_opt, f_tab) = proximal_gradient_algorithm(learn, gamma, max_iter,lambda_1,lambda_2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Question 10__\n",
    "\n",
    "> Write a function that evaluates the accuracy of the classification on the training dataset.\n",
    "\n",
    "> Investigate how this accuracy change when playing with the regularization terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-71-fd74c5210587>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-71-fd74c5210587>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    p = test.map(lambda ex: 1./(1+exp(−np.dot(ex,x_opt)))\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "def accuracy(learn, test, lambda_1, lambda_2):\n",
    "    (x_opt, f_tab) = proximal_gradient_algorithm(learn, gamma, max_iter,lambda_1,lambda_2)\n",
    "    p = test.map(lambda ex: 1./(1+exp(−np.dot(ex,x_opt)))\n",
    "    res = p.map(lambda ex: np.abs(np.sign(ex - 0.5) - test.labels()).reduce(add)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To go further\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerations\n",
    "\n",
    "A popular acceleration method to improve the convergence rate of proximal gradient algorithm is the addition of inertia. That is, contructing the next gradient input by a combination of the last two outputs.\n",
    "\n",
    "\n",
    "In particular, Nesterov's acceleration is the most popular form of inertia. It writes\n",
    "$$ \\left\\{ \\begin{array}{l}   y_{k+1} = \\mathbf{prox\\_grad}(x_k) \\\\ x_{k+1} = y_{k+1} + \\alpha_{k+1} (y_{k+1} - y_k)  \\end{array} \\right. $$ \n",
    "with\n",
    "* $\\mathbf{prox\\_grad}$ the proximal gradient operation\n",
    "* $(\\alpha_{k})$ the inertial sequence defined as $\\alpha_k = \\frac{t_k-1}{t_{k+1}}$ and $t_0 = 0$ and $t_{k+1} = \\frac{1+\\sqrt{1+4t_k^2}}{2}$\n",
    "\n",
    "__Question 11__\n",
    "\n",
    "> Implement a fast proximal gradient with this kind of inertia (This algorithm is often nicknamed FISTA).\n",
    "\n",
    "> Compare the convergence speed with the vanilla proximal gradient algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental methods\n",
    "\n",
    "When dimension increases, incremental algorithms are often priviledged. \n",
    "\n",
    "A possible incremental algorithm for a problem such as regularized logistic regression is MISO (see *J Mairal. Incremental Majorization-Minimization Optimization with Application to Large-Scale Machine Learning. SIAM Journal on Optimization,2015 and ICML 2014.*):\n",
    "\n",
    "* Draw randomly a sample $n$\n",
    "* Compute $x^n_{k+1} = \\mathbf{prox}_{\\gamma g} (\\bar{x}_k) - \\gamma \\nabla f_n(\\mathbf{prox}_{\\gamma g} (\\bar{x}_k) )$\n",
    "* For all $i\\neq n$, $x^i_{k+1}=x^i_k$ \n",
    "* Compute new $\\bar{x}_{k+1} = \\frac{1}{m} \\sum_{j=1}^m x^j_{k+1}$\n",
    " \n",
    "\n",
    "__Question 12__\n",
    "\n",
    "> Implement this incremental algorithm and compare with the previous algorithms in terms of convergence time and functional value versus number of passes over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
