{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was created by Franck Iutzeler, Jerome Malick and Yann Vernaz (2016).</i></small>\n",
    "<!-- Credit (images) Jeffrey Keating Thompson. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)</h3></center>\n",
    "<hr>\n",
    "<center><h1>Convex and distributed optimization</h1></center>\n",
    "<center><h2>Part II - Classification (3h + 3h home work)</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "In this Lab, we will investigate some gradient-based and proximal algorithms on the binary classification problems with logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classification and Logistic Regression\n",
    "\n",
    "### Machine Learning as an Optimization problem\n",
    "\n",
    "We have some *data*  $\\mathcal{D}$ consisting of $m$ *examples* $\\{d_i\\}$; each example consisting of a *feature* vector $a_i\\in\\mathbb{R}^d$ and an *observation* $b_i\\in \\mathcal{O}$: $\\mathcal{D} = \\{[a_i,b_i]\\}_{i=1..m}$ .\n",
    "\n",
    "\n",
    "The goal of *supervised learning* is to construct a predictor for the observations when given feature vectors.\n",
    "\n",
    "\n",
    "A popular approach is based on *linear models* which are based on finding a *parameter* $x$ such that the real number $\\langle a_i , x \\rangle$ is used to predict the value of the observation through a *predictor function* $g:\\mathbb{R}\\to \\mathcal{O}$: $g(\\langle a_i , x \\rangle)$ is the predicted value from $a_i$.\n",
    "\n",
    "\n",
    "In order to find such a parameter, we use the available data and a *loss* $\\ell$ that penalizes the error made between the predicted $g(\\langle a_i , x \\rangle)$ and observed $b_i$ values. For each example $i$, the corresponding error function for a parameter $x$ is $f_i(x) =   \\ell( g(\\langle a_i , x \\rangle) ; b_i )$. Using the whole data, the parameter that minimizes the total error is the solution of the minimization problem\n",
    "$$ \\min_{x\\in\\mathbb{R}^d} \\frac{1}{m} \\sum_{i=1}^m f_i(x) = \\frac{1}{m} \\sum_{i=1}^m  \\ell( g(\\langle a_i , x \\rangle) ; b_i ). $$\n",
    "\n",
    "\n",
    "### Binary Classification with Logisitic Regression\n",
    "\n",
    "In our setup, the observations are binary: $\\mathcal{O} = \\{-1 , +1 \\}$, and the *Logistic loss* is used to form the following optimization problem\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } f(x) := \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ).\n",
    "\\end{align*}\n",
    "\n",
    "Under some statistical hypotheses, $x^\\star = \\arg\\min f(x)$ maximizes the likelihood of the labels knowing the features vector. Then, for a new point $d$ with features vector $a$, \n",
    "$$ p_1(a) = \\mathbb{P}[d\\in \\text{ class }  +1] = \\frac{1}{1+\\exp(-\\langle a;x^\\star \\rangle)} $$\n",
    "Thus, from $a$, if $p_1(a)$ is close to $1$, one can decide that $d$ belongs to class $1$; and the opposite decision if $p(a)$ is close to $0$. Between the two, the appreciation is left to the data scientist depending on the application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised classification datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset\n",
    "\n",
    "We will use LibSVM formatted data, meaning that each line of the file (i.e. each example) will have the form\n",
    "\n",
    "<tt>class feature_number1:feature_value1 feature_number2:feature_value2 ... feature_number$n_i$:feature_value$n_i$ </tt>\n",
    "\n",
    "You may read such a file using MLUtils's <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.util.MLUtils.loadLibSVMFile\">`loadLibSVMFile`</a> routine on the supervised classification datasets below.\n",
    "\n",
    "The elements of the produced RDD have the form of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint\">`LabeledPoints`</a> composed of a label `example.label` corresponding to the class (+1 or -1) and a feature vector `example.features` generally encoded as a <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector\">`SparseVector`</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up spark environment (using Spark local mode set to # cores on your machine)\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\")\n",
    "conf.setAppName(\"MSIAM part II - Logistic Regression\")\n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remind you that you can access this interface (Spark UI) by simply opening http://localhost:4040 in a web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to LibSVM Datasets\n",
    "LibSVMHomeDir=\"../data/LibSVM/\"\n",
    "# LibSVMHomeDir=\"\"\n",
    "LibName=\"ionosphere.txt\"             # a small dataset to begin with\n",
    "#LibName=\"rcv1_train.binary\"          # a bigger one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 1__\n",
    "> Form an RDD from the selected dataset.\n",
    "\n",
    "> Count the number of examples, features, the number of examples of class '+1' and the density of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples (N) = 351\n",
      "number of features (D) = 34\n",
      "number of examples of class +1 = 225\n",
      "number of examples of class -1 = 126\n",
      "density = 0.884113\n",
      "10551\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "data = MLUtils.loadLibSVMFile(sc, LibSVMHomeDir + LibName).setName(\"LibSVM\")\n",
    "N = data.count() # number of examples\n",
    "D = len(data.first().features) # number of features\n",
    "nb_pos_samples = data.filter(lambda x: x.label == 1).count()\n",
    "nb_neg_samples = data.filter(lambda x: x.label == -1).count()\n",
    "nb_nonzero_vals = data.map(lambda x: x.features.numNonzeros()).reduce(lambda x, y: x + y)\n",
    "density = 1. * nb_nonzero_vals / (N * D)\n",
    "\n",
    "print(\"number of examples (N) = %d\" % N)\n",
    "print(\"number of features (D) = %d\" % D)\n",
    "print(\"number of examples of class +1 = %d\" % nb_pos_samples)\n",
    "print(\"number of examples of class -1 = %d\" % nb_neg_samples)\n",
    "print(\"density = %f\" % density)\n",
    "print(nb_nonzero_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "An important first step for learning by regression is to preprocess the dataset. This processing usually consists in:\n",
    "* Adding an intercept, that is an additional feature equal to one for all examples (statistically, this accounts for the fact that the two classes may be imbalanced).\n",
    "* For the dense datasets:\n",
    "    *  normalize to have zero-mean and unit variance for every feature (except the interecept for instance.\n",
    "* For sparse datasets:\n",
    "    * normalize so that the feature vector has unit $\\ell_2$ norm for each example.\n",
    "\n",
    "This does not really change the problem but it will ease the convergence of the applied optimization algorithms.\n",
    "\n",
    "__Question 2__\n",
    "> Form a new RDD with the scaled version of the dataset.\n",
    "\n",
    "> Check that the number of examples, features, and the density is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from operator import add\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# as variance can be zero for some features, we will remove those column and insert them back\n",
    "def normalize_sample(x):\n",
    "    new_features = (x.features.toArray() - means) / np.sqrt(variance)\n",
    "    new_features = np.append(new_features, 1)\n",
    "    features_sparse_vector = SparseVector(np.shape(means)[0] + 1,\n",
    "                                          np.nonzero(new_features)[0],\n",
    "                                          new_features[np.nonzero(new_features)])\n",
    "    return LabeledPoint(x.label, features_sparse_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0,(34,[0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33],[1.0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1.0,0.0376,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.3409,0.42267,-0.54487,0.18641,-0.453]))\n",
      "(34,[0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33],[1.0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1.0,0.0376,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.3409,0.42267,-0.54487,0.18641,-0.453])\n",
      "[ 1.       0.       0.99539 -0.05889  0.85243  0.02306  0.83398 -0.37708\n",
      "  1.       0.0376   0.85243 -0.17755  0.59755 -0.44945  0.60536 -0.38223\n",
      "  0.84356 -0.38542  0.58212 -0.32192  0.56971 -0.29674  0.36946 -0.47357\n",
      "  0.56811 -0.51171  0.41078 -0.46168  0.21266 -0.3409   0.42267 -0.54487\n",
      "  0.18641 -0.453  ]\n"
     ]
    }
   ],
   "source": [
    "first = data.first()\n",
    "print(first)\n",
    "\n",
    "features = first.features\n",
    "print(features)\n",
    "\n",
    "np_features = features.toArray()\n",
    "print(np_features)\n",
    "# data_mapped = data.map(lambda x: x.features)\n",
    "# first_mapped = data_mapped.first()\n",
    "# print(first_mapped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means:\n",
      "[ 0.78347578  0.          0.64134185  0.04437188  0.60106789  0.115889\n",
      "  0.55009507  0.11936037  0.51184809  0.18134538  0.47618265  0.15504046\n",
      "  0.4008012   0.09341368  0.34415915  0.07113234  0.381949   -0.00361681\n",
      "  0.3593896  -0.0240247   0.33669547  0.0082959   0.3624755  -0.05740575\n",
      "  0.39613467 -0.07118687  0.5416408  -0.06953761  0.37844519 -0.02790709\n",
      "  0.35251373 -0.00379376  0.34936365  0.01448011]\n",
      "variance:\n",
      "[ 0.3861657   0.          0.24700772  0.19430949  0.26948603  0.211741\n",
      "  0.24201626  0.27040786  0.25638293  0.2334447   0.31662351  0.24414675\n",
      "  0.38601268  0.24420121  0.42496998  0.20950509  0.38086098  0.24606941\n",
      "  0.39109271  0.26867235  0.37083107  0.26773094  0.36349662  0.27741755\n",
      "  0.33365214  0.25783     0.26570809  0.30166587  0.33069932  0.25730253\n",
      "  0.32566278  0.26300717  0.27239872  0.21871485]\n",
      "(1.0,(35,[0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34],[0.348433280269,0.712372367309,-0.234257237563,0.484207686793,-0.201734977754,0.577058790699,-0.954679144632,0.964074158868,-0.297510229259,0.668654651086,-0.673107318134,0.316673202835,-1.0985425274,0.400677973145,-0.990485565808,0.747985026589,-0.769680949556,0.356155483134,-0.574714507232,0.38264403556,-0.589524243017,0.0115847114771,-0.790128635368,0.29772766393,-0.867564946095,-0.253867539692,-0.713971226166,-0.288289660581,-0.617038783086,0.122936944467,-1.05505394246,-0.312220599512,-0.999594828772,1.0]))\n",
      "means in the normalized data:\n",
      "[  1.56886217e-16   0.00000000e+00  -7.08518397e-17  -6.07301484e-17\n",
      "   5.06084570e-18  -6.07301484e-17   2.83407359e-16   1.41703679e-16\n",
      "  -3.34015816e-16  -4.04867656e-17   1.06277760e-16  -1.21460297e-16\n",
      "   1.01216914e-17   5.56693027e-17  -1.67007908e-16   1.31581988e-16\n",
      "  -2.83407359e-16  -3.03650742e-17  -2.63163976e-16  -5.06084570e-17\n",
      "  -1.77129599e-16   2.53042285e-17  -2.43236896e-16  -5.06084570e-18\n",
      "   3.98541599e-17  -1.32847200e-17   0.00000000e+00   1.01216914e-17\n",
      "   2.53042285e-17   3.92215541e-17   2.02433828e-16  -3.54259199e-17\n",
      "   3.54259199e-17  -7.59126854e-17   1.00000000e+00]\n",
      "variance in the normalized data:\n",
      "[ 1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.]\n",
      "number of examples (N) in the normalized data = 351\n",
      "new number of features (D) in the normalized data = 35\n",
      "density in the normalized data = 0.971429\n"
     ]
    }
   ],
   "source": [
    "means = data.map(lambda x: x.features.toArray()).reduce(add) / N\n",
    "print('means:')\n",
    "print(means)\n",
    "\n",
    "variance = data.map(lambda x: (x.features.toArray() - means) ** 2).reduce(add) / N\n",
    "print('variance:')\n",
    "print(variance)\n",
    "\n",
    "# as we can not divide by zero, we fix the values of variance where it is 0 (the second column which is empty)\n",
    "variance[np.argwhere(variance == 0)] = 1\n",
    "\n",
    "data_normalized = data.map(normalize_sample)\n",
    "\n",
    "print(data_normalized.first())\n",
    "      \n",
    "new_means = data_normalized.map(lambda x: x.features.toArray()).reduce(add) / N\n",
    "print('means in the normalized data:')\n",
    "print(new_means)\n",
    "\n",
    "new_variance = data_normalized.map(lambda x: (x.features.toArray() - new_means) ** 2).reduce(add) / N\n",
    "print('variance in the normalized data:')\n",
    "print(new_variance)\n",
    "\n",
    "\n",
    "new_N = data_normalized.count() # number of examples\n",
    "new_D = len(data_normalized.first().features.toArray()) # number of features\n",
    "new_nb_nonzero_vals = data_normalized.map(lambda x: x.features.numNonzeros()).reduce(add)\n",
    "new_density = 1. * new_nb_nonzero_vals / (new_N * new_D)\n",
    "\n",
    "print(\"number of examples (N) in the normalized data = %d\" % new_N)\n",
    "print(\"new number of features (D) in the normalized data = %d\" % new_D)\n",
    "print(\"density in the normalized data = %f\" % new_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Initialization\n",
    "\n",
    "We will set up here the variables, and the training versus testing dataset. Indeed, we will take a portion of the dataset to learn called the `learning set`, say $95$%, and we will test our predictions on the rest, the `testing set`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 3__\n",
    "\n",
    ">  Split the scaled dataset into a training and a testing set. For instance, you may use the function <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit\">`randomSplit`</a>.\n",
    "\n",
    "> Count the number of examples, and subjects in class '+1' in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_learn = 335, N_test = 16\n",
      "number of examples of class +1 in learn = %d 213\n",
      "number of examples of class +1 in test = %d 12\n"
     ]
    }
   ],
   "source": [
    "learn, test = data_normalized.randomSplit([0.95, 0.05])\n",
    "\n",
    "N_learn = learn.count()\n",
    "N_test = test.count()\n",
    "\n",
    "nb_pos_samples_learn = learn.filter(lambda x: x.label == 1).count()\n",
    "nb_pos_samples_test = test.filter(lambda x: x.label == 1).count()\n",
    "\n",
    "print('N_learn = %d, N_test = %d' %(N_learn, N_test))\n",
    "print('number of examples of class +1 in learn = %d', nb_pos_samples_learn)\n",
    "print('number of examples of class +1 in test = %d', nb_pos_samples_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Minimization of the logistic loss with the Gradient algorithm\n",
    "\n",
    "The goal of this section is to: \n",
    "1. Compute gradients of the loss functions.\n",
    "2. Implement a Gradient algorithm.\n",
    "3. Observe the prediction accuracy of the developed methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 4__\n",
    ">Define a routine computing functional loss and gradient from one example \n",
    "\n",
    "For a Labeled point <tt>example</tt> (`LabeledPoint(example.label,example.features)`) that we denoted $(b_i,a_i)$ and a regressor <tt>x</tt>, compute $f_i(x) = \\log(1+\\exp(-b_i \\langle a_i,x\\rangle) )$ and $\\nabla f_i(x)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_loss_per_example(example,x):\n",
    "    \"\"\" Computes the logistic loss for a Labeled point\n",
    "    Args:\n",
    "        example: a labeled point\n",
    "        x: regressor\n",
    "    Returns:\n",
    "        real value: l \n",
    "    \"\"\"\n",
    "    res = np.log(1 + np.exp(- example.label * np.dot(example.features.toArray(), x)))\n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_grad_per_example(example,x):\n",
    "    \"\"\" Computes the logistic gradient for a Labeled point\n",
    "    Args:\n",
    "        example: a labeled point\n",
    "        x: regressor\n",
    "    Returns:\n",
    "        numpy array: g \n",
    "    \"\"\"\n",
    "    denom = (1 + np.exp(example.label * np.dot(example.features.toArray(), x)))\n",
    "    res = - example.label * example.features.toArray() / denom\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 5__\n",
    ">Implement a gradient descent algorithm to minimize\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } f(x) := \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) = \\frac{1}{m}  \\sum_{i=1}^m f_i(x).\n",
    "\\end{align*}\n",
    ">by \n",
    "* defining a function taking a stepsize and a maximal number of iterations and returning the final point as well as the value of $f(x)$ at each iteration. \n",
    "* running `x, f_tab = grad_algo(gamma,MAX_ITE)`\n",
    "\n",
    "\n",
    "For the choice of the stepsize, we help you by provinding you an upper bound on the Lipschitz constant $L$ of $\\nabla f$:\n",
    "\n",
    "$ L \\leq L_b = \\max_i 0.25 \\|a_i\\|_2^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad_algo(trainRDD, gamma, max_iter, verbose = True):\n",
    "    if verbose:\n",
    "        print('start grad_algo with gamma = %f, max_iter = %d' % (gamma, max_iter))\n",
    "    f_tab = [1.]\n",
    "    N = trainRDD.count()\n",
    "    x = np.zeros(len(trainRDD.first().features.toArray())) # init values = 0\n",
    "    for i in range(max_iter): # maybe change to convergence criterion\n",
    "        sg = trainRDD.map(lambda ex: logistic_grad_per_example(ex, x)).reduce(add) / N\n",
    "        x -= gamma * sg\n",
    "        ll = trainRDD.map(lambda ex: logistic_loss_per_example(ex, x)).reduce(add) / N\n",
    "        f_tab.append(ll)\n",
    "        if verbose and (i == 0 or i == (max_iter - 1)):\n",
    "            print('[iter %d] f(x) = %f' %(i, ll))\n",
    "    if verbose:\n",
    "        print('done')\n",
    "    return x, f_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start grad_algo with gamma = 0.691580, max_iter = 100\n",
      "[iter 0] f(x) = 0.507856\n",
      "[iter 99] f(x) = 0.210867\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "max_example_norm = learn.map(lambda x: np.sqrt(x.features.dot(x.features))).reduce(lambda x,y: x if x > y else y)\n",
    "L_b = 0.25 * max_example_norm # we take the upperbound\n",
    "gamma = 2. / L_b # works better with e.g. 8. / L_b\n",
    "max_iter = 100 # first guess\n",
    "\n",
    "(x_opt, f_tab) = grad_algo(learn, gamma, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 6__\n",
    "\n",
    "> Plot the functional value versus the iterations.\n",
    "\n",
    "> Investigate if the computations are distributed over different threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAF5CAYAAADQ2iM1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1085d3d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(max_iter+1), f_tab, color=\"black\", linewidth=1.0, linestyle=\"-\")\n",
    "plt.xlim(0, max_iter+1)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Functional value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized logisitic regression\n",
    "\n",
    "In addition to the loss, it is usual to add a regularization term of the form\n",
    "$$ r(x) = \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2 $$\n",
    "\n",
    "The first part promotes sparsity of the iterates while the second part prevents over-fitting. \n",
    "This kind of regularization is often called:\n",
    "- *elastic-net* when $ \\lambda_1$ and $ \\lambda_2$ are non-null\n",
    "- $\\ell_1$ when $\\lambda_2 = 0$\n",
    "- *Tikhonov* when $\\lambda_1 = 0$\n",
    "\n",
    "The full optimization problems now writes\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } g(x) =  \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) +  \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "__Question 7__\n",
    "\n",
    "> Which part of $g$ is smooth, which part is not? Write $g$ as \n",
    "$$ g(x) =  \\frac{1}{m}  \\sum_{i=1}^m s_i(x) + n(x)  $$\n",
    "where the $(s_i)$ are smooth function and $n$ is non smooth. \n",
    "\n",
    "> Define a function `regularized_logistic_grad_per_example(examples,x)` returning the gradient of the smooth part per example (i.e. $\\nabla s_i(x)$)\n",
    "\n",
    "> Define a function `n_prox(x,gamma)` returning the proximal operator of the non-smooth part (i.e. $\\mathbf{prox}_{\\gamma n}(y)$)\n",
    "\n",
    "we recall that\n",
    "$$ \\mathbf{prox}_{\\gamma n}(y) = \\arg\\min_x\\left\\{ n(x) + \\frac{1}{2\\gamma} \\|x-y\\|_2^2 \\right\\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def regularized_logistic_loss_per_example(example, x, lambda_1, lambda_2):\n",
    "    res = np.log(1 + np.exp(- example.label * np.dot(example.features.toArray(), x))) + lambda_2*np.dot(x,x) +lambda_1*sum(abs(x))\n",
    "    return res\n",
    "\n",
    "\n",
    "def regularized_logistic_grad_per_example(example, x, lambda_2):\n",
    "    denom = (1 + np.exp(example.label * np.dot(example.features.toArray(), x)))\n",
    "    res = - example.label * example.features.toArray() / denom  + 2 * lambda_2 * x\n",
    "    return res\n",
    "\n",
    "def n_prox(y,gamma,lambda_1):\n",
    "    x = y.copy()\n",
    "    for i in range(np.size(y)):\n",
    "        if y[i] > gamma*lambda_1: \n",
    "            x[i] = y[i] - gamma*lambda_1\n",
    "        elif y[i] < -gamma*lambda_1:\n",
    "            x[i] = y[i] + gamma*lambda_1\n",
    "        else:\n",
    "            x[i] = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 8__\n",
    "\n",
    "> Compute a proximal gradient algorithm for computing a solution of\n",
    "$$ \\min_x  f(x) + r(x) = \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) + \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2 $$\n",
    "\n",
    "\n",
    "Hint: An admissible stepsize can be found by taking $\\gamma = 1/L_{b2}$ with  $ L_b = \\max_i 0.25 \\|a_i\\|_2^2 + 2\\lambda_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def proximal_gradient_algorithm(trainRDD, gamma, max_iter,lambda_1,lambda_2, verbose = True):\n",
    "    if verbose:\n",
    "        print('start prox_grad_algo with gamma = %f, max_iter = %d' % (gamma, max_iter))\n",
    "    f_tab = [1.]\n",
    "    N = trainRDD.count()\n",
    "    x = np.zeros(len(trainRDD.first().features.toArray())) # init values = 0\n",
    "    for i in range(max_iter): # maybe change to convergence criterion\n",
    "        sg = trainRDD.map(lambda ex: regularized_logistic_grad_per_example(ex, x,lambda_2)).reduce(add) / N\n",
    "        x -= gamma * sg\n",
    "        x = n_prox(x,gamma,lambda_1)\n",
    "        ll = trainRDD.map(lambda ex: regularized_logistic_loss_per_example(ex, x,lambda_1,lambda_2)).reduce(add) / N\n",
    "        f_tab.append(ll)\n",
    "        if verbose and (i == 0 or i == (max_iter - 1)):\n",
    "            print('[iter %d] f(x) = %f' %(i, ll))\n",
    "    if verbose:\n",
    "        print('done')\n",
    "    return x, f_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start prox_grad_algo with gamma = 0.343415, max_iter = 100\n",
      "[iter 0] f(x) = 0.593296\n",
      "[iter 99] f(x) = 0.376040\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "lambda_1 = 0.01\n",
    "lambda_2 = 0.01\n",
    "max_example_norm = learn.map(lambda x: np.sqrt(x.features.dot(x.features.toArray()))).reduce(lambda x,y: x if x > y else y)\n",
    "L_b = 0.25 * max_example_norm + 2*lambda_2 # we take the upperbound\n",
    "gamma = 1./ L_b # works better with e.g. 8. / L_b\n",
    "max_iter = 100 # first guess\n",
    "\n",
    "(x_opt, f_tab) =  proximal_gradient_algorithm(learn, gamma, max_iter,lambda_1,lambda_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 9__\n",
    "\n",
    "> Examine the behavior and output of your proximal gradient algorithm with different values of $\\lambda_1$, $\\lambda_2$. What do you observe in terms of sparsity of the solution and convergence rate of the algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.093738 (lambda1 = 0.000000, lambda2 = 0.000000)\n",
      "loss = 0.128580 (lambda1 = 0.000000, lambda2 = 0.010000)\n",
      "loss = 0.156691 (lambda1 = 0.000000, lambda2 = 0.020000)\n",
      "loss = 0.178827 (lambda1 = 0.000000, lambda2 = 0.030000)\n",
      "loss = 0.197289 (lambda1 = 0.000000, lambda2 = 0.040000)\n",
      "loss = 0.213337 (lambda1 = 0.000000, lambda2 = 0.050000)\n",
      "loss = 0.227653 (lambda1 = 0.000000, lambda2 = 0.060000)\n",
      "loss = 0.240640 (lambda1 = 0.000000, lambda2 = 0.070000)\n",
      "loss = 0.252560 (lambda1 = 0.000000, lambda2 = 0.080000)\n",
      "loss = 0.263595 (lambda1 = 0.000000, lambda2 = 0.090000)\n",
      "loss = 0.115475 (lambda1 = 0.010000, lambda2 = 0.000000)\n",
      "loss = 0.148548 (lambda1 = 0.010000, lambda2 = 0.010000)\n",
      "loss = 0.174675 (lambda1 = 0.010000, lambda2 = 0.020000)\n",
      "loss = 0.196391 (lambda1 = 0.010000, lambda2 = 0.030000)\n",
      "loss = 0.214814 (lambda1 = 0.010000, lambda2 = 0.040000)\n",
      "loss = 0.230935 (lambda1 = 0.010000, lambda2 = 0.050000)\n",
      "loss = 0.245432 (lambda1 = 0.010000, lambda2 = 0.060000)\n",
      "loss = 0.258605 (lambda1 = 0.010000, lambda2 = 0.070000)\n",
      "loss = 0.270691 (lambda1 = 0.010000, lambda2 = 0.080000)\n",
      "loss = 0.281867 (lambda1 = 0.010000, lambda2 = 0.090000)\n",
      "loss = 0.151285 (lambda1 = 0.020000, lambda2 = 0.000000)\n",
      "loss = 0.178278 (lambda1 = 0.020000, lambda2 = 0.010000)\n",
      "loss = 0.201344 (lambda1 = 0.020000, lambda2 = 0.020000)\n",
      "loss = 0.221036 (lambda1 = 0.020000, lambda2 = 0.030000)\n",
      "loss = 0.238458 (lambda1 = 0.020000, lambda2 = 0.040000)\n",
      "loss = 0.254055 (lambda1 = 0.020000, lambda2 = 0.050000)\n",
      "loss = 0.268118 (lambda1 = 0.020000, lambda2 = 0.060000)\n",
      "loss = 0.280995 (lambda1 = 0.020000, lambda2 = 0.070000)\n",
      "loss = 0.292842 (lambda1 = 0.020000, lambda2 = 0.080000)\n",
      "loss = 0.303805 (lambda1 = 0.020000, lambda2 = 0.090000)\n",
      "loss = 0.186449 (lambda1 = 0.030000, lambda2 = 0.000000)\n",
      "loss = 0.209910 (lambda1 = 0.030000, lambda2 = 0.010000)\n",
      "loss = 0.230165 (lambda1 = 0.030000, lambda2 = 0.020000)\n",
      "loss = 0.248497 (lambda1 = 0.030000, lambda2 = 0.030000)\n",
      "loss = 0.264917 (lambda1 = 0.030000, lambda2 = 0.040000)\n",
      "loss = 0.279799 (lambda1 = 0.030000, lambda2 = 0.050000)\n",
      "loss = 0.293328 (lambda1 = 0.030000, lambda2 = 0.060000)\n",
      "loss = 0.305709 (lambda1 = 0.030000, lambda2 = 0.070000)\n",
      "loss = 0.317143 (lambda1 = 0.030000, lambda2 = 0.080000)\n",
      "loss = 0.327759 (lambda1 = 0.030000, lambda2 = 0.090000)\n",
      "loss = 0.217505 (lambda1 = 0.040000, lambda2 = 0.000000)\n",
      "loss = 0.239558 (lambda1 = 0.040000, lambda2 = 0.010000)\n",
      "loss = 0.258359 (lambda1 = 0.040000, lambda2 = 0.020000)\n",
      "loss = 0.275021 (lambda1 = 0.040000, lambda2 = 0.030000)\n",
      "loss = 0.290172 (lambda1 = 0.040000, lambda2 = 0.040000)\n",
      "loss = 0.304240 (lambda1 = 0.040000, lambda2 = 0.050000)\n",
      "loss = 0.317043 (lambda1 = 0.040000, lambda2 = 0.060000)\n",
      "loss = 0.328796 (lambda1 = 0.040000, lambda2 = 0.070000)\n",
      "loss = 0.339650 (lambda1 = 0.040000, lambda2 = 0.080000)\n",
      "loss = 0.349732 (lambda1 = 0.040000, lambda2 = 0.090000)\n",
      "loss = 0.246107 (lambda1 = 0.050000, lambda2 = 0.000000)\n",
      "loss = 0.267648 (lambda1 = 0.050000, lambda2 = 0.010000)\n",
      "loss = 0.286054 (lambda1 = 0.050000, lambda2 = 0.020000)\n"
     ]
    },
    {
     "ename": "timeout",
     "evalue": "timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-bf9e25dd5544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mL_b\u001b[0m \u001b[0;31m# works better with e.g. 8. / L_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;31m# first guess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmbd_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmbd_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mlambda_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlmbd_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmbd_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-1fc07cb3659d>\u001b[0m in \u001b[0;36maccuracy_reg\u001b[0;34m(learn, test, max_iter, lambda_1, lambda_2, verbose)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mL_b_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_example_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlambda_2\u001b[0m \u001b[0;31m# we take the upperbound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mgamma_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mL_b_reg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mx_opt_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_tab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproximal_gradient_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mloss_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlogistic_loss_per_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_opt_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-1c3354ade256>\u001b[0m in \u001b[0;36mproximal_gradient_algorithm\u001b[0;34m(trainRDD, gamma, max_iter, lambda_1, lambda_2, verbose)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_prox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mregularized_logistic_loss_per_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mf_tab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alexpashevich/MoSIG/CDO/spark-2.0.2-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alexpashevich/MoSIG/CDO/spark-2.0.2-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alexpashevich/MoSIG/CDO/spark-2.0.2-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_load_from_socket\u001b[0;34m(port, serializer)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m65536\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alexpashevich/MoSIG/CDO/spark-2.0.2-bin-hadoop2.7/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mload_stream\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_with_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alexpashevich/MoSIG/CDO/spark-2.0.2-bin-hadoop2.7/python/pyspark/serializers.py\u001b[0m in \u001b[0;36m_read_with_length\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSpecialLengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNULL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mtimeout\u001b[0m: timed out"
     ]
    }
   ],
   "source": [
    "max_example_norm = learn.map(lambda x: np.sqrt(x.features.dot(x.features))).reduce(lambda x,y: x if x > y else y)\n",
    "loss_array = []\n",
    "lambda_array = []\n",
    "for lmbd_1 in np.arange(0, 0.1, 0.01):\n",
    "    for lmbd_2 in np.arange(0, 0.1, 0.01):\n",
    "        L_b = 0.25 * max_example_norm + 2*lmbd_2 # we take the upperbound\n",
    "        gamma = 2. / L_b # works better with e.g. 8. / L_b\n",
    "        max_iter = 100 # first guess\n",
    "        ll = accuracy_reg(learn, test, max_iter, lmbd_1, lmbd_2)\n",
    "        lambda_array.append([lmbd_1, lmbd_2])\n",
    "        loss_array.append(ll)\n",
    "        print('loss = %f (lambda1 = %f, lambda2 = %f)' % (ll, lmbd_1, lmbd_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(lambda_array)\n",
    "# print(len(lambda_array))\n",
    "# zip_la = zip(*lambda_array)\n",
    "# print(type(zip_la))\n",
    "\n",
    "plot_loss_vs_lambdas(lambda_array, loss_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Question 10__\n",
    "\n",
    "> Write a function that evaluates the accuracy of the classification on the training dataset.\n",
    "\n",
    "> Investigate how this accuracy change when playing with the regularization terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_reg_vs_nonreg(learn, test, max_iter, lambda_1, lambda_2, verbose = False):\n",
    "    N_test = test.count()\n",
    "    L_b_nonreg = 0.25 * max_example_norm # we take the upperbound\n",
    "    L_b_reg = 0.25 * max_example_norm + 2*lambda_2 # we take the upperbound\n",
    "    gamma_nonreg = 2. / L_b_nonreg\n",
    "    gamma_reg = 2. / L_b_reg\n",
    "    (x_opt_nonreg, f_tab) = grad_algo(learn, gamma_nonreg, max_iter, verbose)\n",
    "    (x_opt_reg, f_tab) = proximal_gradient_algorithm(learn, gamma_reg, max_iter, lambda_1, lambda_2, verbose)\n",
    "    loss_nonreg = test.map(lambda ex: logistic_loss_per_example(ex, x_opt_nonreg)).reduce(add) / N_test\n",
    "    loss_reg = test.map(lambda ex: logistic_loss_per_example(ex, x_opt_reg)).reduce(add) / N_test\n",
    "#     res = p.map(lambda ex: np.abs(np.sign(ex - 0.5) - test.labels()).reduce(add)\n",
    "    if verbose:\n",
    "        print('loss_nonreg = %f, loss_reg = %f' % (loss_nonreg, loss_reg))\n",
    "    return (loss_nonreg, loss_reg)\n",
    "\n",
    "def accuracy_reg(learn, test, max_iter, lambda_1, lambda_2, verbose = False):\n",
    "    N_test = test.count()\n",
    "    L_b_reg = 0.25 * max_example_norm + 2*lambda_2 # we take the upperbound\n",
    "    gamma_reg = 2. / L_b_reg\n",
    "    (x_opt_reg, f_tab) = proximal_gradient_algorithm(learn, gamma_reg, max_iter, lambda_1, lambda_2, verbose)\n",
    "    loss_reg = test.map(lambda ex: logistic_loss_per_example(ex, x_opt_reg)).reduce(add) / N_test\n",
    "    if verbose:\n",
    "        print('loss on test = %f' % (loss_reg))\n",
    "    return loss_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "def plot_loss_vs_lambdas(lambda_array, loss_array):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    lambda_array_np = np.array(lambda_array)\n",
    "    ax.scatter(lambda_array_np[:,0], lambda_array_np[:,1], loss_array) #, rstride=10, cstride=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To go further\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerations\n",
    "\n",
    "A popular acceleration method to improve the convergence rate of proximal gradient algorithm is the addition of inertia. That is, contructing the next gradient input by a combination of the last two outputs.\n",
    "\n",
    "\n",
    "In particular, Nesterov's acceleration is the most popular form of inertia. It writes\n",
    "$$ \\left\\{ \\begin{array}{l}   y_{k+1} = \\mathbf{prox\\_grad}(x_k) \\\\ x_{k+1} = y_{k+1} + \\alpha_{k+1} (y_{k+1} - y_k)  \\end{array} \\right. $$ \n",
    "with\n",
    "* $\\mathbf{prox\\_grad}$ the proximal gradient operation\n",
    "* $(\\alpha_{k})$ the inertial sequence defined as $\\alpha_k = \\frac{t_k-1}{t_{k+1}}$ and $t_0 = 0$ and $t_{k+1} = \\frac{1+\\sqrt{1+4t_k^2}}{2}$\n",
    "\n",
    "__Question 11__\n",
    "\n",
    "> Implement a fast proximal gradient with this kind of inertia (This algorithm is often nicknamed FISTA).\n",
    "\n",
    "> Compare the convergence speed with the vanilla proximal gradient algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fast_proximal_gradient_algorithm(trainRDD, gamma, max_iter,lambda_1,lambda_2):\n",
    "    print('start grad_algo with gamma = %f, max_iter = %d' % (gamma, max_iter))\n",
    "    f_tab = [1.]\n",
    "    N = trainRDD.count()\n",
    "    x = np.zeros(len(trainRDD.first().features.toArray())) # init values = 0\n",
    "    y = x.copy()\n",
    "    y_last = x.copy()\n",
    "    t = 0\n",
    "    t_last = 0\n",
    "    for i in range(max_iter): # maybe change to convergence criterion\n",
    "        t = (1. + np.sqrt(1 + 4*t_last**2))/2\n",
    "        alpha = (t_last - 1)/t\n",
    "        sg = trainRDD.map(lambda ex: regularized_logistic_grad_per_example(ex, x,lambda_2)).reduce(add) / N\n",
    "        x -= gamma * sg\n",
    "        y = n_prox(x,gamma,lambda_1)\n",
    "        x = y + alpha*(y - y_last)\n",
    "        ll = trainRDD.map(lambda ex: regularized_logistic_loss_per_example(ex, x,lambda_1,lambda_2)).reduce(add) / N\n",
    "        f_tab.append(ll)\n",
    "        y_last = y\n",
    "        t_last = t\n",
    "        print('[iter %d] f(x) = %f' %(i, ll))\n",
    "    print('done')\n",
    "    return x, f_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start grad_algo with gamma = 0.343415, max_iter = 20\n",
      "[iter 0] f(x) = 0.693147\n",
      "[iter 1] f(x) = 0.589594\n",
      "[iter 2] f(x) = 0.524838\n",
      "[iter 3] f(x) = 0.479633\n",
      "[iter 4] f(x) = 0.448187\n",
      "[iter 5] f(x) = 0.426056\n",
      "[iter 6] f(x) = 0.410231\n",
      "[iter 7] f(x) = 0.398894\n",
      "[iter 8] f(x) = 0.390705\n",
      "[iter 9] f(x) = 0.384736\n",
      "[iter 10] f(x) = 0.380300\n",
      "[iter 11] f(x) = 0.377025\n",
      "[iter 12] f(x) = 0.374724\n",
      "[iter 13] f(x) = 0.373131\n",
      "[iter 14] f(x) = 0.372114\n",
      "[iter 15] f(x) = 0.371224\n",
      "[iter 16] f(x) = 0.370463\n",
      "[iter 17] f(x) = 0.369867\n",
      "[iter 18] f(x) = 0.369638\n",
      "[iter 19] f(x) = 0.369317\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "lambda_1 = 0.01\n",
    "lambda_2 = 0.01\n",
    "max_example_norm = learn.map(lambda x: np.sqrt(x.features.dot(x.features.toArray()))).reduce(lambda x,y: x if x > y else y)\n",
    "L_b = 0.25 * max_example_norm + 2*lambda_2 # we take the upperbound\n",
    "gamma = 1./ L_b # works better with e.g. 8. / L_b\n",
    "max_iter = 20 # first guess\n",
    "\n",
    "(x_opt, f_tab) =  fast_proximal_gradient_algorithm(learn, gamma, max_iter,lambda_1,lambda_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental methods\n",
    "\n",
    "When dimension increases, incremental algorithms are often priviledged. \n",
    "\n",
    "A possible incremental algorithm for a problem such as regularized logistic regression is MISO (see *J Mairal. Incremental Majorization-Minimization Optimization with Application to Large-Scale Machine Learning. SIAM Journal on Optimization,2015 and ICML 2014.*):\n",
    "\n",
    "* Draw randomly a sample $n$\n",
    "* Compute $x^n_{k+1} = \\mathbf{prox}_{\\gamma g} (\\bar{x}_k) - \\gamma \\nabla f_n(\\mathbf{prox}_{\\gamma g} (\\bar{x}_k) )$\n",
    "* For all $i\\neq n$, $x^i_{k+1}=x^i_k$ \n",
    "* Compute new $\\bar{x}_{k+1} = \\frac{1}{m} \\sum_{j=1}^m x^j_{k+1}$\n",
    " \n",
    "\n",
    "__Question 12__\n",
    "\n",
    "> Implement this incremental algorithm and compare with the previous algorithms in terms of convergence time and functional value versus number of passes over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  \n",
    "def MISO_proximal_gradient_algorithm(trainRDD, gamma, max_iter,lambda_1,lambda_2):\n",
    "    print('start grad_algo with gamma = %f, max_iter = %d' % (gamma, max_iter))\n",
    "    f_tab = [1.]\n",
    "    N = trainRDD.count()\n",
    "    x = np.zeros(len(trainRDD.first().features.toArray())) # init values = 0\n",
    "    y = x.copy()\n",
    "    y_last = x.copy()\n",
    "    t = 0\n",
    "    t_last = 0\n",
    "    for i in range(max_iter): # maybe change to convergence criterion\n",
    "        n = np.rand(np.size(x))\n",
    "        x = n_prox(x,gamma,lambda_1)\n",
    "        sg_n = trainRDD.map(lambda ex: regularized_logistic_grad_per_example(ex, x,lambda_2)).reduce(add) / N\n",
    "        y = n_prox(x,gamma,lambda_1) - gamma*sg_n\n",
    "        \n",
    "        ll = trainRDD.map(lambda ex: regularized_logistic_loss_per_example(ex, x,lambda_1,lambda_2)).reduce(add) / N\n",
    "        f_tab.append(ll)\n",
    "        y_last = y\n",
    "        t_last = t\n",
    "        print('[iter %d] f(x) = %f' %(i, ll))\n",
    "    print('done')\n",
    "    return x, f_tab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
