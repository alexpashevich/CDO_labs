{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was created by Franck Iutzeler, Jerome Malick and Yann Vernaz (2016).</i></small>\n",
    "<!-- Credit (images) Jeffrey Keating Thompson. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)</h3></center>\n",
    "<hr>\n",
    "<center><h1>Convex and distributed optimization</h1></center>\n",
    "<center><h2>Part II - Classification (3h + 3h home work)</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "In this Lab, we will investigate some gradient-based and proximal algorithms on the binary classification problems with logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classification and Logistic Regression\n",
    "\n",
    "### Machine Learning as an Optimization problem\n",
    "\n",
    "We have some *data*  $\\mathcal{D}$ consisting of $m$ *examples* $\\{d_i\\}$; each example consisting of a *feature* vector $a_i\\in\\mathbb{R}^d$ and an *observation* $b_i\\in \\mathcal{O}$: $\\mathcal{D} = \\{[a_i,b_i]\\}_{i=1..m}$ .\n",
    "\n",
    "\n",
    "The goal of *supervised learning* is to construct a predictor for the observations when given feature vectors.\n",
    "\n",
    "\n",
    "A popular approach is based on *linear models* which are based on finding a *parameter* $x$ such that the real number $\\langle a_i , x \\rangle$ is used to predict the value of the observation through a *predictor function* $g:\\mathbb{R}\\to \\mathcal{O}$: $g(\\langle a_i , x \\rangle)$ is the predicted value from $a_i$.\n",
    "\n",
    "\n",
    "In order to find such a parameter, we use the available data and a *loss* $\\ell$ that penalizes the error made between the predicted $g(\\langle a_i , x \\rangle)$ and observed $b_i$ values. For each example $i$, the corresponding error function for a parameter $x$ is $f_i(x) =   \\ell( g(\\langle a_i , x \\rangle) ; b_i )$. Using the whole data, the parameter that minimizes the total error is the solution of the minimization problem\n",
    "$$ \\min_{x\\in\\mathbb{R}^d} \\frac{1}{m} \\sum_{i=1}^m f_i(x) = \\frac{1}{m} \\sum_{i=1}^m  \\ell( g(\\langle a_i , x \\rangle) ; b_i ). $$\n",
    "\n",
    "\n",
    "### Binary Classification with Logisitic Regression\n",
    "\n",
    "In our setup, the observations are binary: $\\mathcal{O} = \\{-1 , +1 \\}$, and the *Logistic loss* is used to form the following optimization problem\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } f(x) := \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ).\n",
    "\\end{align*}\n",
    "\n",
    "Under some statistical hypotheses, $x^\\star = \\arg\\min f(x)$ maximizes the likelihood of the labels knowing the features vector. Then, for a new point $d$ with features vector $a$, \n",
    "$$ p_1(a) = \\mathbb{P}[d\\in \\text{ class }  +1] = \\frac{1}{1+\\exp(-\\langle a;x^\\star \\rangle)} $$\n",
    "Thus, from $a$, if $p_1(a)$ is close to $1$, one can decide that $d$ belongs to class $1$; and the opposite decision if $p(a)$ is close to $0$. Between the two, the appreciation is left to the data scientist depending on the application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised classification datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset\n",
    "\n",
    "We will use LibSVM formatted data, meaning that each line of the file (i.e. each example) will have the form\n",
    "\n",
    "<tt>class feature_number1:feature_value1 feature_number2:feature_value2 ... feature_number$n_i$:feature_value$n_i$ </tt>\n",
    "\n",
    "You may read such a file using MLUtils's <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.util.MLUtils.loadLibSVMFile\">`loadLibSVMFile`</a> routine on the supervised classification datasets below.\n",
    "\n",
    "The elements of the produced RDD have the form of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint\">`LabeledPoints`</a> composed of a label `example.label` corresponding to the class (+1 or -1) and a feature vector `example.features` generally encoded as a <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector\">`SparseVector`</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=MSIAM part II - Logistic Regression, master=local[*]) created by __init__ at <ipython-input-1-15540157dd7d>:11 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-15540157dd7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSIAM part II - Logistic Regression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/alexpashevich/MoSIG/CDO/spark-2.0.2-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/Users/alexpashevich/MoSIG/CDO/spark-2.0.2-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    257\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 259\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    260\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=MSIAM part II - Logistic Regression, master=local[*]) created by __init__ at <ipython-input-1-15540157dd7d>:11 "
     ]
    }
   ],
   "source": [
    "# set up spark environment (using Spark local mode set to # cores on your machine)\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\")\n",
    "conf.setAppName(\"MSIAM part II - Logistic Regression\")\n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remind you that you can access this interface (Spark UI) by simply opening http://localhost:4040 in a web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to LibSVM Datasets\n",
    "LibSVMHomeDir=\"../data/LibSVM/\"\n",
    "LibName=\"ionosphere.txt\"             # a small dataset to begin with\n",
    "#LibName=\"rcv1_train.binary\"          # a bigger one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 1__\n",
    "> Form an RDD from the selected dataset.\n",
    "\n",
    "> Count the number of examples, features, the number of examples of class '+1' and the density of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "data = MLUtils.loadLibSVMFile(sc, LibSVMHomeDir + LibName).setName(\"LibSVM\")\n",
    "N = data.count() # number of examples\n",
    "D = len(data.first().features) # number of features\n",
    "nb_pos_samples = data.filter(lambda x: x.label == 1).count()\n",
    "nb_neg_samples = data.filter(lambda x: x.label == -1).count()\n",
    "nb_nonzero_vals = data.map(lambda x: x.features.numNonzeros()).reduce(lambda x, y: x + y)\n",
    "density = 1. * nb_nonzero_vals / (N * D)\n",
    "\n",
    "print(\"number of examples (N) = %d\" % N)\n",
    "print(\"number of features (D) = %d\" % D)\n",
    "print(\"number of examples of class +1 = %d\" % nb_pos_samples)\n",
    "print(\"number of examples of class -1 = %d\" % nb_neg_samples)\n",
    "print(\"density = %f\" % density)\n",
    "print(nb_nonzero_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "An important first step for learning by regression is to preprocess the dataset. This processing usually consists in:\n",
    "* Adding an intercept, that is an additional feature equal to one for all examples (statistically, this accounts for the fact that the two classes may be imbalanced).\n",
    "* For the dense datasets:\n",
    "    *  normalize to have zero-mean and unit variance for every feature (except the interecept for instance.\n",
    "* For sparse datasets:\n",
    "    * normalize so that the feature vector has unit $\\ell_2$ norm for each example.\n",
    "\n",
    "This does not really change the problem but it will ease the convergence of the applied optimization algorithms.\n",
    "\n",
    "__Question 2__\n",
    "> Form a new RDD with the scaled version of the dataset.\n",
    "\n",
    "> Check that the number of examples, features, and the density is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from operator import add\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# as variance can be zero for some features, we will remove those column and insert them back\n",
    "def normalize_sample(x):\n",
    "    new_features = (np.array(x.features) - means) / np.sqrt(variance)\n",
    "    new_features = np.append(new_features, 1)\n",
    "    features_sparse_vector = SparseVector(np.shape(means)[0] + 1,\n",
    "                                          np.nonzero(new_features)[0],\n",
    "                                          new_features[np.nonzero(new_features)])\n",
    "    return LabeledPoint(x.label, features_sparse_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means:\n",
      "[ 0.78347578  0.          0.64134185  0.04437188  0.60106789  0.115889\n",
      "  0.55009507  0.11936037  0.51184809  0.18134538  0.47618265  0.15504046\n",
      "  0.4008012   0.09341368  0.34415915  0.07113234  0.381949   -0.00361681\n",
      "  0.3593896  -0.0240247   0.33669547  0.0082959   0.3624755  -0.05740575\n",
      "  0.39613467 -0.07118687  0.5416408  -0.06953761  0.37844519 -0.02790709\n",
      "  0.35251373 -0.00379376  0.34936365  0.01448011]\n",
      "variance:\n",
      "[ 0.3861657   0.          0.24700772  0.19430949  0.26948603  0.211741\n",
      "  0.24201626  0.27040786  0.25638293  0.2334447   0.31662351  0.24414675\n",
      "  0.38601268  0.24420121  0.42496998  0.20950509  0.38086098  0.24606941\n",
      "  0.39109271  0.26867235  0.37083107  0.26773094  0.36349662  0.27741755\n",
      "  0.33365214  0.25783     0.26570809  0.30166587  0.33069932  0.25730253\n",
      "  0.32566278  0.26300717  0.27239872  0.21871485]\n",
      "means in the normalized data:\n",
      "[  1.56886217e-16   0.00000000e+00  -7.08518397e-17  -6.07301484e-17\n",
      "   5.06084570e-18  -6.07301484e-17   2.83407359e-16   1.41703679e-16\n",
      "  -3.34015816e-16  -4.04867656e-17   1.06277760e-16  -1.21460297e-16\n",
      "   1.01216914e-17   5.56693027e-17  -1.67007908e-16   1.31581988e-16\n",
      "  -2.83407359e-16  -3.03650742e-17  -2.63163976e-16  -5.06084570e-17\n",
      "  -1.77129599e-16   2.53042285e-17  -2.43236896e-16  -5.06084570e-18\n",
      "   3.98541599e-17  -1.32847200e-17   0.00000000e+00   1.01216914e-17\n",
      "   2.53042285e-17   3.92215541e-17   2.02433828e-16  -3.54259199e-17\n",
      "   3.54259199e-17  -7.59126854e-17   1.00000000e+00]\n",
      "numerical precision = 0.000000\n",
      "variance in the normalized data:\n",
      "[ 1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.]\n",
      "number of examples (N) in the normalized data = 351\n",
      "new number of features (D) in the normalized data = 35\n",
      "density in the normalized data = 0.971429\n"
     ]
    }
   ],
   "source": [
    "means = data.map(lambda x: np.array(x.features)).reduce(add) / N\n",
    "print('means:')\n",
    "print(means)\n",
    "\n",
    "variance = data.map(lambda x: (np.array(x.features) - means) ** 2).reduce(add) / N\n",
    "print('variance:')\n",
    "print(variance)\n",
    "\n",
    "# as we can not divide by zero, we fix the values of varience where it is 0 (the second column which is empty)\n",
    "variance[np.argwhere(variance == 0)] = 1\n",
    "\n",
    "data_normalized = data.map(normalize_sample)\n",
    "\n",
    "# print(data_normalized.first())\n",
    "      \n",
    "new_means = data_normalized.map(lambda x: np.array(x.features)).reduce(add) / N\n",
    "print('means in the normalized data:')\n",
    "print(new_means)\n",
    "\n",
    "new_variance = data_normalized.map(lambda x: (np.array(x.features) - new_means) ** 2).reduce(add) / N\n",
    "print('variance in the normalized data:')\n",
    "print(new_variance)\n",
    "\n",
    "\n",
    "new_N = data_normalized.count() # number of examples\n",
    "new_D = len(data_normalized.first().features) # number of features\n",
    "new_nb_nonzero_vals = data_normalized.map(lambda x: x.features.numNonzeros()).reduce(add)\n",
    "new_density = 1. * new_nb_nonzero_vals / (new_N * new_D)\n",
    "\n",
    "print(\"number of examples (N) in the normalized data = %d\" % new_N)\n",
    "print(\"new number of features (D) in the normalized data = %d\" % new_D)\n",
    "print(\"density in the normalized data = %f\" % new_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Initialization\n",
    "\n",
    "We will set up here the variables, and the training versus testing dataset. Indeed, we will take a portion of the dataset to learn called the `learning set`, say $95$%, and we will test our predictions on the rest, the `testing set`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 3__\n",
    "\n",
    ">  Split the scaled dataset into a training and a testing set. For instance, you may use the function <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit\">`randomSplit`</a>.\n",
    "\n",
    "> Count the number of examples, and subjects in class '+1' in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_learn = 333, N_test = 18\n",
      "number of examples of class +1 in learn = %d 214\n",
      "number of examples of class +1 in test = %d 11\n"
     ]
    }
   ],
   "source": [
    "learn, test = data_normalized.randomSplit([0.95, 0.05])\n",
    "\n",
    "N_learn = learn.count()\n",
    "N_test = test.count()\n",
    "\n",
    "nb_pos_samples_learn = learn.filter(lambda x: x.label == 1).count()\n",
    "nb_pos_samples_test = test.filter(lambda x: x.label == 1).count()\n",
    "\n",
    "print('N_learn = %d, N_test = %d' %(N_learn, N_test))\n",
    "print('number of examples of class +1 in learn = %d', nb_pos_samples_learn)\n",
    "print('number of examples of class +1 in test = %d', nb_pos_samples_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Minimization of the logistic loss with the Gradient algorithm\n",
    "\n",
    "The goal of this section is to: \n",
    "1. Compute gradients of the loss functions.\n",
    "2. Implement a Gradient algorithm.\n",
    "3. Observe the prediction accuracy of the developed methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 4__\n",
    ">Define a routine computing functional loss and gradient from one example \n",
    "\n",
    "For a Labeled point <tt>example</tt> (`LabeledPoint(example.label,example.features)`) that we denoted $(b_i,a_i)$ and a regressor <tt>x</tt>, compute $f_i(x) = \\log(1+\\exp(-b_i \\langle a_i,x\\rangle) )$ and $\\nabla f_i(x)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_loss_per_example(example,x):\n",
    "    \"\"\" Computes the logistic loss for a Labeled point\n",
    "    Args:\n",
    "        example: a labeled point\n",
    "        x: regressor\n",
    "    Returns:\n",
    "        real value: l \n",
    "    \"\"\"\n",
    "    res = np.log(1 + np.exp(example.label * np.dot(example.features, x)))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_grad_per_example(example,x):\n",
    "    \"\"\" Computes the logistic gradient for a Labeled point\n",
    "    Args:\n",
    "        example: a labeled point\n",
    "        x: regressor\n",
    "    Returns:\n",
    "        numpy array: g\n",
    "    \"\"\"\n",
    "    denom = 1 + np.exp(example.label * np.dot(example.features, x)) # works with - inside the exponent\n",
    "#     res = -1 * example.label * np.array(example.features) / denom\n",
    "    res = -1 * example.label / denom\n",
    "#     print(res)\n",
    "#     print('*')\n",
    "#     print(example.features)\n",
    "#     print('=')\n",
    "    res *= np.array(example.features)\n",
    "#     print(res)\n",
    "    # the last one is array so the output should be an array\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 5__\n",
    ">Implement a gradient descent algorithm to minimize\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } f(x) := \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) = \\frac{1}{m}  \\sum_{i=1}^m f_i(x).\n",
    "\\end{align*}\n",
    ">by \n",
    "* defining a function taking a stepsize and a maximal number of iterations and returning the final point as well as the value of $f(x)$ at each iteration. \n",
    "* running `x, f_tab = grad_algo(gamma,MAX_ITE)`\n",
    "\n",
    "\n",
    "For the choice of the stepsize, we help you by provinding you an upper bound on the Lipschitz constant $L$ of $\\nabla f$:\n",
    "\n",
    "$ L \\leq L_b = \\max_i 0.25 \\|a_i\\|_2^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5\n",
      "*\n",
      "(35,[0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34],[0.348433280269,0.712372367309,-0.234257237563,0.484207686793,-0.201734977754,0.577058790699,-0.954679144632,0.964074158868,-0.297510229259,0.668654651086,-0.673107318134,0.316673202835,-1.0985425274,0.400677973145,-0.990485565808,0.747985026589,-0.769680949556,0.356155483134,-0.574714507232,0.38264403556,-0.589524243017,0.0115847114771,-0.790128635368,0.29772766393,-0.867564946095,-0.253867539692,-0.713971226166,-0.288289660581,-0.617038783086,0.122936944467,-1.05505394246,-0.312220599512,-0.999594828772,1.0])\n",
      "=\n",
      "[-0.17421664 -0.         -0.35618618  0.11712862 -0.24210384  0.10086749\n",
      " -0.2885294   0.47733957 -0.48203708  0.14875511 -0.33432733  0.33655366\n",
      " -0.1583366   0.54927126 -0.20033899  0.49524278 -0.37399251  0.38484047\n",
      " -0.17807774  0.28735725 -0.19132202  0.29476212 -0.00579236  0.39506432\n",
      " -0.14886383  0.43378247  0.12693377  0.35698561  0.14414483  0.30851939\n",
      " -0.06146847  0.52752697  0.1561103   0.49979741 -0.5       ]\n"
     ]
    }
   ],
   "source": [
    "max_example_norm = learn.map(lambda x: np.sqrt(x.features.dot(x.features))).reduce(lambda x,y: x if x > y else y)\n",
    "L_b = 0.25 * max_example_norm # we take the upperbound\n",
    "gamma = 2. / L_b\n",
    "max_iter = 10 # first guess\n",
    "\n",
    "(x_opt, f_tab) = grad_algo(learn, gamma, max_iter)\n",
    "\n",
    "# x = np.zeros(len(learn.first().features))\n",
    "# ex = learn.first()\n",
    "# r = logistic_grad_per_example(ex, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad_algo(trainRDD, gamma, max_iter):\n",
    "    print('start grad_algo with gamma = %f, max_iter = %d' % (gamma, max_iter))\n",
    "    f_tab = [1.]\n",
    "    N = trainRDD.count()\n",
    "    x = np.zeros(len(trainRDD.first().features)) # init values = 0\n",
    "    for i in range(max_iter): # maybe change to convergence criterion\n",
    "        sg = trainRDD.map(lambda ex: logistic_grad_per_example(ex, x)).reduce(add) / N\n",
    "        x -= gamma * sg # works with + here\n",
    "        ll = trainRDD.map(lambda ex: logistic_loss_per_example(ex, x)).reduce(add) / N\n",
    "        f_tab.append(ll)\n",
    "#         print('grad')\n",
    "#         print(sg)\n",
    "#         print('weights')\n",
    "#         print(x)\n",
    "        if i % 5 == 0:\n",
    "            print('[iter %d] f(x) = %f' %(i, ll))\n",
    "    print('done')\n",
    "    return x, f_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 6__\n",
    "\n",
    "> Plot the functional value versus the iterations.\n",
    "\n",
    "> Investigate if the computations are distributed over different threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAF5CAYAAADQ2iM1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwL3TzKxGIPAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106f3a7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(max_iter+1), f_tab, color=\"black\", linewidth=1.0, linestyle=\"-\")\n",
    "plt.xlim(0, max_iter+1)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Functional value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized logisitic regression\n",
    "\n",
    "In addition to the loss, it is usual to add a regularization term of the form\n",
    "$$ r(x) = \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2 $$\n",
    "\n",
    "The first part promotes sparsity of the iterates while the second part prevents over-fitting. \n",
    "This kind of regularization is often called:\n",
    "- *elastic-net* when $ \\lambda_1$ and $ \\lambda_2$ are non-null\n",
    "- $\\ell_1$ when $\\lambda_2 = 0$\n",
    "- *Tikhonov* when $\\lambda_1 = 0$\n",
    "\n",
    "The full optimization problems now writes\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } g(x) =  \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) +  \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "__Question 7__\n",
    "\n",
    "> Which part of $g$ is smooth, which part is not? Write $g$ as \n",
    "$$ g(x) =  \\frac{1}{m}  \\sum_{i=1}^m s_i(x) + n(x)  $$\n",
    "where the $(s_i)$ are smooth function and $n$ is non smooth. \n",
    "\n",
    "> Define a function `regularized_logistic_grad_per_example(examples,x)` returning the gradient of the smooth part per example (i.e. $\\nabla s_i(x)$)\n",
    "\n",
    "> Define a function `n_prox(x,gamma)` returning the proximal operator of the non-smooth part (i.e. $\\mathbf{prox}_{\\gamma n}(y)$)\n",
    "\n",
    "we recall that\n",
    "$$ \\mathbf{prox}_{\\gamma n}(y) = \\arg\\min_x\\left\\{ n(x) + \\frac{1}{2\\gamma} \\|x-y\\|_2^2 \\right\\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 8__\n",
    "\n",
    "> Compute a proximal gradient algorithm for computing a solution of\n",
    "$$ \\min_x  f(x) + r(x) = \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) + \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2 $$\n",
    "\n",
    "\n",
    "Hint: An admissible stepsize can be found by taking $\\gamma = 1/L_{b2}$ with  $ L_b = \\max_i 0.25 \\|a_i\\|_2^2 + 2\\lambda_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 9__\n",
    "\n",
    "> Examine the behavior and output of your proximal gradient algorithm with different values of $\\lambda_1$, $\\lambda_2$. What do you observe in terms of sparsity of the solution and convergence rate of the algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Question 10__\n",
    "\n",
    "> Write a function that evaluates the accuracy of the classification on the training dataset.\n",
    "\n",
    "> Investigate how this accuracy change when playing with the regularization terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To go further\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerations\n",
    "\n",
    "A popular acceleration method to improve the convergence rate of proximal gradient algorithm is the addition of inertia. That is, contructing the next gradient input by a combination of the last two outputs.\n",
    "\n",
    "\n",
    "In particular, Nesterov's acceleration is the most popular form of inertia. It writes\n",
    "$$ \\left\\{ \\begin{array}{l}   y_{k+1} = \\mathbf{prox\\_grad}(x_k) \\\\ x_{k+1} = y_{k+1} + \\alpha_{k+1} (y_{k+1} - y_k)  \\end{array} \\right. $$ \n",
    "with\n",
    "* $\\mathbf{prox\\_grad}$ the proximal gradient operation\n",
    "* $(\\alpha_{k})$ the inertial sequence defined as $\\alpha_k = \\frac{t_k-1}{t_{k+1}}$ and $t_0 = 0$ and $t_{k+1} = \\frac{1+\\sqrt{1+4t_k^2}}{2}$\n",
    "\n",
    "__Question 11__\n",
    "\n",
    "> Implement a fast proximal gradient with this kind of inertia (This algorithm is often nicknamed FISTA).\n",
    "\n",
    "> Compare the convergence speed with the vanilla proximal gradient algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental methods\n",
    "\n",
    "When dimension increases, incremental algorithms are often priviledged. \n",
    "\n",
    "A possible incremental algorithm for a problem such as regularized logistic regression is MISO (see *J Mairal. Incremental Majorization-Minimization Optimization with Application to Large-Scale Machine Learning. SIAM Journal on Optimization,2015 and ICML 2014.*):\n",
    "\n",
    "* Draw randomly a sample $n$\n",
    "* Compute $x^n_{k+1} = \\mathbf{prox}_{\\gamma g} (\\bar{x}_k) - \\gamma \\nabla f_n(\\mathbf{prox}_{\\gamma g} (\\bar{x}_k) )$\n",
    "* For all $i\\neq n$, $x^i_{k+1}=x^i_k$ \n",
    "* Compute new $\\bar{x}_{k+1} = \\frac{1}{m} \\sum_{j=1}^m x^j_{k+1}$\n",
    " \n",
    "\n",
    "__Question 12__\n",
    "\n",
    "> Implement this incremental algorithm and compare with the previous algorithms in terms of convergence time and functional value versus number of passes over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
