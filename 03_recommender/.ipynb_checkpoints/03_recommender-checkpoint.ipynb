{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was create by Franck Iutzeler, Jerome Malick and Yann Vernaz (2016).</i></small>\n",
    "<!-- Credit (images) Jeffrey Keating Thompson. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)</h3></center>\n",
    "<hr>\n",
    "<center><h1>Convex and distributed optimization</h1></center>\n",
    "<center><h2>Part III - Recommender Systems (3h + 3h home work)</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "In this Lab, we will investigate some gradient-based algorithms on the very well known matrix factorization problem which is the most prominent approach for build a _Recommender Systems_.\n",
    "\n",
    "Our goal is to implement Large-Scale Matrix Factorization with Distributed Stochastic Gradient Descent in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Formulation\n",
    "\n",
    "The problem of matrix factorization for collaborative filtering captured much attention, especially after the [Netflix prize](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf). The premise behind this approach is to approximate a large rating matrix $R$ with the multiplication of two low-dimensional factor matrices $P$ and $Q$, i.e. $R \\approx \\hat{R} = P^TQ$, that model respectively users and items in some latent space. For instance, matrix $R$ has dimension $m \\times  n$ where $m$ and $n$ are restrictively the number of users and items, both large; while $P$ has size $m \\times  k$ and contains user information in a latent space of size $k<<m,n$, $Q$ has size $n\\times k$ and contains item information in the same latent space of size $k << m,n$. Typical values for $m, n$ are $10^6$ while $k$ is in the tens.\n",
    "\n",
    "For a pair of user and item $(u_i,i_j)$ for which a rating $r_{ij}$ exists, a common approach approach is based on the minimization of the $\\ell_2$-regularized quadratic error:\n",
    "$$  \\ell_{u_i,i_j}(P,Q)= \\left(r_{ij} - p_{i}^{\\top}q_{j}\\right)^2 + \\lambda(|| p_{i} ||^{2} + || q_{j} ||^2 )  $$\n",
    "where $p_i$ is the column vector composed of the $i$-th line of $P$ and  $\\lambda\\geq 0$ is a regularization parameter. The whole matrix factorization problem thus writes\n",
    "$$ \\min_{P,Q} \\sum_{i,j : r_{ij} \\text{exists}}  \\ell_{u_i,i_j}(P,Q). $$\n",
    "Note that the error $ \\ell_{u_i,i_j}(P,Q)$ depends only on $P$ and $Q$ through $p_{i}$ and $q_{j}$; however, item $i_j$ may also be rated by user $u_{i'}$ so that the optimal factor $q_{j}$ depends on both $p_{i}$ and $p_{i'}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up spark environment (Using Spark Local Mode)\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().set(\"spark.executor.heartbeatInterval\", \"100000s\")\n",
    "# conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\")\n",
    "conf.setAppName(\"MSIAM part III - Matrix Factorization\")\n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remind you that you can access this interface by simply opening http://localhost:4040 in a web browser.\n",
    "\n",
    "We will capitalize on the first lab and take the MovieLens dataset, and thus the RDD routines we already have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1000209 ratings from 6040 users on 3706 movies.\n",
      "\n",
      "We have 6040 users, 3952 movies and the rating matrix has 0.000004 percent of non-zero value.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def parseRating(line):\n",
    "    fields = line.split('::')\n",
    "    return int(fields[0]), int(fields[1]), float(fields[2])\n",
    "\n",
    "# path to MovieLens dataset\n",
    "movieLensHomeDir=\"../data/movielens/medium/\"\n",
    "\n",
    "# ratings is an RDD of (userID, movieID, rating)\n",
    "ratingsRDD = sc.textFile(movieLensHomeDir + \"ratings.dat\").map(parseRating).setName(\"ratings\").cache()\n",
    "\n",
    "numRatings = ratingsRDD.count()\n",
    "numUsers = ratingsRDD.map(lambda r: r[0]).distinct().count()\n",
    "numMovies = ratingsRDD.map(lambda r: r[1]).distinct().count()\n",
    "print(\"We have %d ratings from %d users on %d movies.\\n\" % (numRatings, numUsers, numMovies))\n",
    "\n",
    "M = ratingsRDD.map(lambda r: r[0]).max()\n",
    "N = ratingsRDD.map(lambda r: r[1]).max()\n",
    "matrixSparsity = float(1)/float(M*N)\n",
    "print(\"We have %d users, %d movies and the rating matrix has %f percent of non-zero value.\\n\" % \\\n",
    "                                          (M, N, 100*matrixSparsity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Gradient Descent Algorithms\n",
    "\n",
    "The goal here is to \n",
    "1. Compute gradients of the loss functions.\n",
    "2. Implement gradient algorithms.\n",
    "3. Observe the prediction accuracy of the developed methods.\n",
    "\n",
    "__Question 1__\n",
    "\n",
    "> Split (ramdomly) the dataset into training versus testing sample. We learn over 70% (for example) of the users, we test over the rest.\n",
    "\n",
    "> Define a routine that returns the predicted rating from factor matrices. Form a RDD with the following elements `(i,j,true rating,predicted rating)`. \n",
    "\n",
    "> Define a routine that returns the Mean Square Error (MSE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "import numpy as np\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def split_dataset(dataset, train_fraction):\n",
    "    learn, test = dataset.randomSplit([train_fraction, 1 - train_fraction])\n",
    "    return (learn, test)\n",
    "\n",
    "def predict_ratings(true_ratings, P, Q):\n",
    "    return true_ratings.map(lambda x: (x[2], np.dot(P[x[0] - 1], Q[x[1] - 1])))\n",
    "\n",
    "def calculate_MSE(rdd):\n",
    "    mse = rdd.map(lambda x: (x[0] - x[1]) ** 2).reduce(add) / rdd.count()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2__\n",
    "\n",
    "> Derive the update rules for gradient descent. \n",
    "\n",
    "> Implement a (full) gradient algorithm in `Python` on the training set.  Take a step size (learning rate) $\\gamma=0.001$ and stop after a specified number of iterations. Investigate the latent space size (e.g. $K=2,5,10,50$).\n",
    "\n",
    "> Provide plots and explanations for your experiments. \n",
    "\n",
    "> Try to parrallelize it so that the code can be run using `PySpark`. What do you conclude?\n",
    "\n",
    "Stochastic Gradient Descent (SGD) simply does away with the expectation in the update and computes the gradient of the parameters using only a single or a few training examples. In SGD the learning rate $\\gamma$ is typically much smaller than a corresponding learning rate in batch gradient descent because there is much more variance in the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sqr_loss(y_ij, p_i, q_j, lambda_1, lambda_2):\n",
    "    se = (y_ij - np.dot(p_i, q_j)) ** 2\n",
    "    res = se + lambda_1 / 2 * np.linalg.norm(p_i) ** 2 + lambda_2 / 2 * np.linalg.norm(q_j) ** 2\n",
    "    return res\n",
    "\n",
    "def grad_p_i(y_ij, p_i, q_j, lambda_1):\n",
    "    res = - 2 * q_j * (y_ij - np.dot(p_i, q_j)) + lambda_1 * p_i\n",
    "    return res\n",
    "\n",
    "def grad_q_j(y_ij, p_i, q_j, lambda_2):\n",
    "    res = - 2 * p_i * (y_ij - np.dot(p_i, q_j)) + lambda_2 * q_j\n",
    "    return res\n",
    "\n",
    "def do_grad_p(grad_p):\n",
    "    global P\n",
    "    i = grad_p[0] - 1\n",
    "    grad_val = grad_p[1]\n",
    "    print('before P[%d] = %s' %(i, P[i]))\n",
    "    P[i] -= gamma * grad_val\n",
    "    print('after P[%d] = %s' %(i, P[i]))\n",
    "    return 0\n",
    "\n",
    "def do_grad_q(grad_q):\n",
    "    global Q\n",
    "    j = grad_q[0] - 1\n",
    "    grad_val = grad_q[1]\n",
    "    print('before Q[%d] = %s' %(j, Q[j]))\n",
    "    Q[j] -= gamma * grad_val\n",
    "    print('after Q[%d] = %s' %(j, Q[j]))\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "def grad_algo(trainRDD, testRDD, N, M, K, gamma, max_iter, lambda_1, lambda_2, verbose = True):\n",
    "    \"\"\" Run the gradient algorithm and returs P and Q such that rating matrix X = P^T * Q\n",
    "    Args:\n",
    "        trainRDD: RDD with ratings\n",
    "        N: number of movies\n",
    "        M: number of users\n",
    "        K: latent feature space size\n",
    "        gamma: gradient step size\n",
    "        max_iter: number of gradient iterations before stop\n",
    "        lambda_1, lambda_2: regularization constants\n",
    "        verbose: if info messages should be printed\n",
    "    Returns:\n",
    "        (P, Q): matrix factorizations with sizes M x K and N x K correspondingly\n",
    "        ll_tab: vector of likelihoods of every train iteration\n",
    "    \"\"\"\n",
    "    P = np.ones((M, K))\n",
    "    Q = np.ones((N, K))\n",
    "    if verbose:\n",
    "        print('start grad_algo with gamma = %f, max_iter = %d' % (gamma, max_iter))\n",
    "    ll_tab = [1.]\n",
    "    train_ll_tab = [1.]\n",
    "    test_ll_tab = [1.]\n",
    "    \n",
    "    for it in range(max_iter): # maybe change to convergence criterion\n",
    "        grads = trainRDD.map(lambda ex: (ex[0], # i\n",
    "                                         ex[1], # j\n",
    "                                         grad_p_i(ex[2], P[ex[0] - 1], Q[ex[1] - 1], lambda_1),\n",
    "                                         grad_q_j(ex[2], P[ex[0] - 1], Q[ex[1] - 1], lambda_2)))\n",
    "        grads_p = grads.map(lambda x: (x[0], x[2])).groupBy(lambda x: x[0])\\\n",
    "                       .mapValues(lambda x: np.sum(np.array([list(el)[1] for el in x]), 0) / len(x))\n",
    "        grads_q = grads.map(lambda x: (x[1], x[3])).groupBy(lambda x: x[0])\\\n",
    "                       .mapValues(lambda x: np.sum(np.array([list(el)[1] for el in x]), 0) / len(x))\n",
    "        \n",
    "        \n",
    "        for grad_p in grads_p.collect():\n",
    "            i = grad_p[0] - 1\n",
    "            grad_val = grad_p[1]\n",
    "            P[i] -= gamma * grad_val\n",
    "\n",
    "        for grad_q in grads_q.collect():\n",
    "            j = grad_q[0] - 1\n",
    "            grad_val = grad_q[1]\n",
    "            Q[j] -= gamma * grad_val\n",
    "            \n",
    "\n",
    "        ll_sum = trainRDD.map(lambda ex: sqr_loss(ex[2], P[ex[0] - 1], Q[ex[1] - 1], lambda_1, lambda_2)).reduce(add)\n",
    "        ll = ll_sum / trainRDD.count()\n",
    "        ll_tab.append(ll)\n",
    "        \n",
    "        train_ll = calculate_MSE(predict_ratings(train, P, Q))\n",
    "        test_ll = calculate_MSE(predict_ratings(test, P, Q))\n",
    "        \n",
    "        train_ll_tab.append(train_ll)\n",
    "        test_ll_tab.append(test_ll)\n",
    "        \n",
    "#         if verbose and (i == 0 or i == (max_iter - 1)):\n",
    "        print('iter[%d] ll_reg = %f, train_ll = %f, test_ll = %f' % (it, ll, train_ll, test_ll))\n",
    "    if verbose:\n",
    "        print('done')\n",
    "    return P, Q, train_ll_tab, test_ll_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start grad_algo with gamma = 0.001000, max_iter = 100\n",
      "iter[0] ll_reg = 3.726736, train_ll = 3.706609, test_ll = 3.703103\n",
      "iter[1] ll_reg = 3.683504, train_ll = 3.663251, test_ll = 3.659842\n",
      "iter[2] ll_reg = 3.640649, train_ll = 3.620270, test_ll = 3.616959\n",
      "iter[3] ll_reg = 3.598175, train_ll = 3.577670, test_ll = 3.574458\n",
      "iter[4] ll_reg = 3.556085, train_ll = 3.535455, test_ll = 3.532340\n",
      "iter[5] ll_reg = 3.514381, train_ll = 3.493626, test_ll = 3.490610\n",
      "iter[6] ll_reg = 3.473067, train_ll = 3.452186, test_ll = 3.449271\n",
      "iter[7] ll_reg = 3.432145, train_ll = 3.411139, test_ll = 3.408324\n",
      "iter[8] ll_reg = 3.391618, train_ll = 3.370487, test_ll = 3.367772\n",
      "iter[9] ll_reg = 3.351487, train_ll = 3.330232, test_ll = 3.327617\n",
      "iter[10] ll_reg = 3.311755, train_ll = 3.290375, test_ll = 3.287862\n",
      "iter[11] ll_reg = 3.272424, train_ll = 3.250920, test_ll = 3.248508\n",
      "iter[12] ll_reg = 3.233496, train_ll = 3.211868, test_ll = 3.209558\n",
      "iter[13] ll_reg = 3.194972, train_ll = 3.173221, test_ll = 3.171013\n",
      "iter[14] ll_reg = 3.156854, train_ll = 3.134980, test_ll = 3.132875\n",
      "iter[15] ll_reg = 3.119144, train_ll = 3.097147, test_ll = 3.095144\n",
      "iter[16] ll_reg = 3.081842, train_ll = 3.059722, test_ll = 3.057822\n",
      "iter[17] ll_reg = 3.044949, train_ll = 3.022707, test_ll = 3.020911\n",
      "iter[18] ll_reg = 3.008467, train_ll = 2.986104, test_ll = 2.984411\n",
      "iter[19] ll_reg = 2.972397, train_ll = 2.949911, test_ll = 2.948322\n",
      "iter[20] ll_reg = 2.936738, train_ll = 2.914132, test_ll = 2.912647\n",
      "iter[21] ll_reg = 2.901492, train_ll = 2.878765, test_ll = 2.877384\n",
      "iter[22] ll_reg = 2.866658, train_ll = 2.843811, test_ll = 2.842534\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "(train, test) = split_dataset(ratingsRDD, 0.7)\n",
    "\n",
    "gamma = 0.001\n",
    "max_iter = 100\n",
    "K = 2 # 5, 10, 50\n",
    "lambda_1, lambda_2 = 0.01, 0.01\n",
    "\n",
    "(P, Q, train_tab, test_tab) = grad_algo(train, test, N, M, K, gamma, max_iter, lambda_1, lambda_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(max_iter+1), train_tab, color=\"blue\", linewidth=1.0, linestyle=\"-\")\n",
    "plt.plot(range(max_iter+1), test_tab, color=\"green\", linewidth=1.0, linestyle=\"-\")\n",
    "plt.xlim(0, max_iter+1)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Functional value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 3__\n",
    "> Implement stochastic gradient descent algorithm for Matrix Factorization.\n",
    "\n",
    "> Provide plots and explanations for your experiments.\n",
    "\n",
    "> Compare and discuss the results with the (full) gradient algorithm in terms of MSE versus full data passes.\n",
    "\n",
    "> Discuss the stepsize choice of SGD (e.g. constant v.s. 1/`nb_iter`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic_grad_algo(trainRDD, testRDD, N, M, K, gamma, max_iter, lambda_1, lambda_2, verbose = True):\n",
    "    \"\"\" Run the gradient algorithm and returs P and Q such that rating matrix X = P^T * Q\n",
    "    Args:\n",
    "        trainRDD: RDD with ratings\n",
    "        N: number of movies\n",
    "        M: number of users\n",
    "        K: latent feature space size\n",
    "        gamma: gradient step size\n",
    "        max_iter: number of gradient iterations before stop\n",
    "        lambda_1, lambda_2: regularization constants\n",
    "        verbose: if info messages should be printed\n",
    "    Returns:\n",
    "        (P, Q): matrix factorizations with sizes M x K and N x K correspondingly\n",
    "        ll_tab: vector of likelihoods of every train iteration\n",
    "    \"\"\"\n",
    "    P = np.ones((M, K))\n",
    "    Q = np.ones((N, K))\n",
    "    nb_ratings = trainRDD.count()\n",
    "    if verbose:\n",
    "        print('start stochastic_grad_algo with gamma = %f, max_iter = %d' % (gamma, max_iter))\n",
    "    ll_tab = [1.]\n",
    "    train_ll_tab = [1.]\n",
    "    test_ll_tab = [1.]\n",
    "    \n",
    "    for it in range(max_iter):\n",
    "        for s_it in range(nb_ratings):\n",
    "            sample = trainRDD.takeSample(False, 1)\n",
    "            i = sample[0] - 1\n",
    "            j = sample[1] - 1\n",
    "            grad_p_val = grad_p_i(sample[2], P[i], Q[j], lambda_1)\n",
    "            grad_q_val = grad_q_j(sample[2], P[i], Q[j], lambda_2)\n",
    "            P[i] -= gamma * grad_p_val\n",
    "            Q[j] -= gamma * grad_q_val\n",
    "\n",
    "        ll_sum = trainRDD.map(lambda ex: sqr_loss(ex[2], P[ex[0] - 1], Q[ex[1] - 1], lambda_1, lambda_2)).reduce(add)\n",
    "        ll = ll_sum / trainRDD.count()\n",
    "        ll_tab.append(ll)\n",
    "        \n",
    "        train_ll = calculate_MSE(predict_ratings(train, P, Q))\n",
    "        test_ll = calculate_MSE(predict_ratings(test, P, Q))\n",
    "        \n",
    "        train_ll_tab.append(train_ll)\n",
    "        test_ll_tab.append(test_ll)\n",
    "        \n",
    "#         if verbose and (i == 0 or i == (max_iter - 1)):\n",
    "        print('iter[%d] ll_reg = %f, train_ll = %f, test_ll = %f' % (it, ll, train_ll, test_ll))\n",
    "    if verbose:\n",
    "        print('done')\n",
    "    return P, Q, train_ll_tab, test_ll_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement Large-Scale Matrix Factorization with Distributed Stochastic Gradient Descent (DSGD) in Spark. \n",
    "The algorithm is described in the following article: <br \\><br \\>\n",
    "_Gemulla, R., Nijkamp, E., Haas, P. J., & Sismanis, Y. (2011). Large-scale matrix factorization with distributed stochastic gradient descent. New York, USA._<br \\><br \\>\n",
    "The paper sets forth a solution for matrix factorization using minimization of sum of local losses.  The solution involves dividing the matrix into strata for each iteration and performing sequential stochastic gradient descent within each stratum in parallel.  DSGD is a fully distributed algorithm, i.e. both the data matrix $R$ and factor matrices $P$ and $Q$ can be carefully split and distributed to multiple workers for parallel computation without communication costs between the workers. Hence, it is a good match for implementation in a distributed in-memory data processing system like Spark. \n",
    "\n",
    "__Question 4__\n",
    "\n",
    "> Implement a `PySpark` version of DSGD.\n",
    "\n",
    "> Test on different number of cores on a local machine (1 core, 2 cores, 4 cores). Ran the ALS method already implemented in MLlib as a reference for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
